---
id: moonshot-interview
sidebar_position: 4
title: 月之暗面大模型推理工程师面试指导
---

# 月之暗面大模型推理工程师面试指导

## 职位名称推测

根据提供的岗位职责和要求，这个职位很可能是大模型平台/基础设施工程师或大规模模型推理系统工程师。职责强调了超大规模线上推理集群的效率与稳定、模型应用到产品的架构设计，以及工作流自动化优化。这些特征对应于需要构建大模型服务平台、部署推理系统的工程类岗位，而非纯算法研究岗。因此，可能的职位名称包括「大模型推理系统架构师」、「AI 基础设施工程师(大模型方向)」等。

## 公司与技术背景

月之暗面是一家专注于大模型(LLM)研发和落地的前沿企业。根据公开信息，该公司团队背景强大，创始人和核心工程师多来自顶尖科研机构和大厂。月之暗面在2024年初获得了阿里巴巴等投资约10亿美元，显示其在国内大模型领域地位突出。公司的战略侧重于C端产品和应用，注重将大模型技术转化为实际产品。

技术上，他们既追求模型能力(如研发自研优化器 Muon、混合专家大模型 Moonlight)，也高度重视大规模集群的训练与推理基础设施。据QCon大会分享，月之暗面构建了稳定高效的 LLM 基础设施，在训练-推理混部集群上有丰富实践，包括快速定位隔离故障、提高资源利用、潮汐调度等创新。这意味着该公司的技术栈涵盖了分布式训练、分布式推理、Kubernetes容器编排、高性能通信等领域，是一个既看重前沿算法又要求深厚工程能力的团队。

已有面经表明，月之暗面的面试广度和深度并重，题目专业且贴合大模型方向。例如，有候选人反映一面由业界大牛主导，既考察基础的大模型知识(Transformer架构等)，又深入提问框架层面的分布式训练划分、设备间通信原理(如 ring-allreduce 算法)。可见，公司非常看重候选人对大模型原理与系统实现的全面理解，以及解决实际问题的能力。

## 面试流程与侧重

据求职者分享和行业情况推测，月之暗面的面试流程一般包含多轮技术面试和系统设计面试，可能还会有业务面或HR面。具体可能流程: 一到两轮深入的技术面，之后一轮系统设计/架构面，最后可能有与团队负责人或HR的沟通。

### 技术面试

技术面试侧重编程能力和计算机基础以及大模型相关技术。面试官可能通过现场Coding(算法题)考察代码能力，以及通过问答考察操作系统、网络、数据结构算法等基础知识掌握程度。更重要的是，他们会深入探讨大模型和分布式系统相关的知识细节。例如，有面试被问到Transformer与MoE架构差异、Multi-Head Attention机制、以及分布式训练中的并行切分策略和通信算法等。可以预期技术面非常专业且全面。

### 系统设计面试

系统设计面试主要考察候选人在大规模系统架构上的思考和设计能力。鉴于岗位职责涉及"超大规模线上推理集群"和"模型部署"，系统设计面可能让你设计一个高并发低延迟的大模型推理服务架构，或者描述如何在资源有限环境下高效部署并管理LLM集群。也可能探讨模型从研发到上线的工作流设计，如如何快速迭代模型、自动化CI/CD部署、A/B测试等。

总之，设计题会围绕大规模、高性能、可扩展的AI服务展开，如设计一个支持上千GPU的模型推理集群，确保既满足实时请求的潮汐式负载，又能充分利用闲时资源进行训练任务。

### 面试风格

面试风格上，公司面试官喜欢追问细节。当你给出方案或回答后，他们可能层层深入，比如针对通信算法问到底层实现(如"All-Reduce底层如何实现")、针对Kubernetes部署问具体优化手段。因此在作答时既要宏观架构清晰，也要准备好在细节上展示掌握深度。

## 技术面准备

### 1. 编程和算法

务必练习编码能力，熟悉至少一种主要开发语言(Python/Go/Rust)。面试中可能会有LeetCode难度的算法题甚至较难的现场编程。关注数据结构与算法基础(数组、哈希、树链表、排序搜索等)，以及中高级题(动态规划、并发算法)。由于岗位需要解决高性能问题，也可能涉及一些算法优化场景，如如何优化计算瓶颈、减少内存占用等。

### 2. 计算机基础

巩固操作系统和网络的核心知识。操作系统方面，了解线程进程模型、CPU调度、内存管理(分页/缓存)、I/O 和文件系统等基础。网络方面，熟悉TCP/IP原理、HTTP/gRPC协议，以及常见分布式系统概念(一致性、分区容错、CAP理论等)。这些基础知识可能以问答形式出现，也可能融入系统设计讨论中。例如，如果讨论高并发服务架构，面试官可能追问底层网络通信瓶颈，如何通过协议优化或连接复用来提升性能。

### 3. 分布式系统与并行计算

这是重头戏。重点准备以下内容: 

#### 并行训练与推理的架构
理解数据并行、模型并行(张量并行)、流水线并行以及 MoE 专家并行等多种并行技术，掌握它们的原理和优缺点。能够讨论在大模型训练/推理中何时采用哪种并行方案，以及可能遇到的问题(如通信开销、内存开销)。

:::info 大模型并行技术全解(训练 & 推理)

下文用统一框架讲清四种主流并行: **数据并行(DP)**、**张量并行/模型并行(TP)**、**流水线并行(PP)**、**MoE 专家并行(EP)**。每种都包含: 原理 → 通信模式 → 优缺点 → 适用场景/踩坑。最后给出 **训练/推理的选型策略与组合**。

### DP/TP/PP(SP)
**1) 数据并行 DP(最常用的第一层扩展手段)**

**原理**
每块 GPU 放一份完整模型，不同 GPU 处理不同样本子批(mini-batch)，**反向** 将各卡梯度做 **AllReduce** 求和，然后各自更新相同参数。

**通信模式**

* 反向结束进行 **Gradients AllReduce**(环形/树形等)，通信量 $$\approx$$ 与 **参数规模** 同阶。
* ZeRO/FSDP 变体: 把 **优化器状态/梯度/参数** 分片，使用 **Reduce-Scatter + All-Gather** 降通信与显存。

**优缺点**

* ✅ 简单稳定，收敛行为与单卡接近；吞吐扩展性最好。
* ❌ 模型必须 **能放进单卡显存**(不分片时)；大模型时 **优化器/梯度显存** 压力大；大批次可能影响收敛。

**适用**

* 训练: **首先** 考虑 DP(配合 ZeRO-2/3 或 FSDP 做分片 + 激活检查点)。
* 推理: 做 **多副本** 扩吞吐/HA；与连续批处理(continuous batching)配合。

**常见坑**

* 大 batch 需调 LR/调度器；跨机网络不足导致 AllReduce 拖慢；混合精度通信/重叠算通(overlap)要正确配置。

**2) 张量并行 TP(模型内并行，按算子/维度切分)**

**原理**
把某些大矩阵的维度切分到多卡: 

* **列并行(Column-Parallel Linear)**: 按输出维切分，每卡算一部分输出；后续若需要完整向量则 **All-Gather**。
* **行并行(Row-Parallel Linear)**: 按输入维切分，每卡算部分点积，末端需要 **All-Reduce** 求和。
* 注意 Transformer 中 QKV、FFN 两层常配对使用，精心安排可减少一次聚合。
* **Sequence Parallel (SP)**: 把激活按 **序列维** 分片，显著减激活显存。

**通信模式**

* 层内 **频繁小粒度** 的 **All-Gather / All-Reduce**；每层前后都可能发生。
* 最佳放在 **同节点 NVLink/NVSwitch** 域内；跨节点 TP 高延迟代价大。

**优缺点**

* ✅ 让 **单层** 能落地于多卡，总显存线性扩；单卡放不下时的 **刚需**。
* ❌ **层内高频集合通信**，对互联拓扑极其敏感；隐藏维等需能被并行度整除；实现复杂度高。

**适用**

* 训练/推理: **当某层矩阵太大** 单卡放不下或吞吐需更宽算子时。
* 实践建议: TP 尽量 **限定在单机**(8×H100 等)，跨机更倾向 PP/DP。

**常见坑**

* 维度不整除、跨机 TP 慢、内核融合/Flash-Attention 与并行划分冲突；日志常见 NCCL "hang/timeout"。

**3) 流水线并行 PP(按层/Block 切分)**

**原理**
把模型按深度切成多个阶段(stage)，不同 GPU 放相邻若干层。用 **微批**(micro-batch)把一个大 batch 切成 M 份，在 **前向/反向流水** 中穿梭: 

* **GPipe**: 先全部前向，再反向(气泡大，显存高)。
* **1F1B (PipeDream-Flush)**: 交替 1 前向/1 反向，降低气泡与激活显存。
* **交错 1F1B**: 进一步减少气泡。

**通信模式**

* 仅在 **相邻阶段** 传激活/梯度，点对点 send/recv，通信量与 **激活大小 × 微批数** 相关。
* 跨节点友好(比 TP 的频繁 AllReduce 好)。

**优缺点**

* ✅ 参数显存 **按阶段分摊**；跨机扩展好；通信路径简单。
* ❌ **气泡(bubble)** 导致利用率不满；阶段负载不均影响吞吐/延迟；需要调 **微批数与调度**。

**适用**

* 模型 **很深**(上百层)或跨机放置；与 TP 组合成 2D/3D 并行。
* 推理: PP 会增加 **端到端延迟**(多跳)，对 **交互式低延迟** 不友好；离线批推更可接受。

**常见坑**

* 阶段切分不均；激活显存暴涨(需激活检查点/recompute)；微批过小导致核效率差。

**4) MoE 专家并行 EP(稀疏激活)**

**原理**
MoE 把 FFN 层替换为 **多专家** 集合，路由器(gating)把 **每个 token** 分配到 **Top-k** 专家(通常 k=1/2)。每层只激活少数专家: 

* **Expert Parallel**: 不同专家分布在不同 GPU 上。
* **Token Dispatch**: 把属于同一专家的 token **All-to-All** 发到对应 GPU，计算后再 All-to-All 回来重排。
* **容量/负载均衡**: capacity factor、aux loss、随机噪声/门控温度避免专家塌缩。

**通信模式**

* **All-to-All**(负载不均会更糟)；Top-k>1 时通信/重复计算更大。
* 常与 DP/TP/PP 组合(形成 3D/4D 拓扑)。

**优缺点**

* ✅ **在相似 FLOPs 下显著增大"参数容量"**，训练/推理算力更聚焦于被选中的专家；参数可自然分布在多机。
* ❌ 实现复杂、**All-to-All 成本高**，对拓扑敏感；路由稳定性、容量溢出、冷专家问题；推理调度更难。

**适用**

* 目标是 **更高容量/质量** 而不线性增加算力成本；数据多样性大。
* 推理: 需要 **专家就近放置**、批内按专家重排(expert-aware batching)、并对 **路由抖动** 和缓存策略做工程化处理。

**常见坑**

* 专家负载倾斜(需强力正则/温度/抖动)；All-to-All 在 IB 低拓扑下退化；Top-2 延迟激增。

**6) 训练/推理 —— 何时用哪种？**

**快速决策表**

| 场景          | 首选                             | 备注                   |
| ----------- | ------------------------------ | -------------------- |
| 模型能放单卡      | **DP (+FSDP/ZeRO)**            | 最简单稳妥；吞吐/可扩展性好       |
| 单层放不下/超宽    | **TP(同机内)**                    | 避免跨机 TP；配合 SP 降激活显存  |
| 模型极深 & 多机   | **PP**(+TP/DP)                 | 阶段均衡 + 1F1B/交错调度     |
| 追求容量/质量/性价比 | **MoE(EP)**(+DP/TP/PP)         | All-to-All 成本 + 负载均衡 |
| 推理高吞吐       | **多副本 DP**                     | 连续批处理与 KV Cache 复用   |
| 推理低延迟 & 大模型 | **TP(同机) + 少量 DP**             | 避免 PP 多跳；量化 + KV 分片  |
| 超长上下文       | **PP(少阶段) 或 Context Parallel** | 注意 KV Cache 分片与跨阶段传输 |
| 跨机受限(带宽一般)  | **PP/DP 优先**                   | 减少层内集合通信；避开跨机 TP/EP  |

**关键优化与权衡**

* **通信**: 
  * DP: Grad AllReduce(或 ZeRO 的 Reduce-Scatter/All-Gather)；重叠通信/计算。
  * TP: 层内 All-Gather/All-Reduce 高频，**尽量同机**；合并通信、核融合。
  * PP: 相邻阶段点对点传激活；调微批填满流水减少气泡。
  * MoE: All-to-All，需拓扑感知(同机放常用专家/副本)，批内按专家重排。
* **显存**: FSDP/ZeRO(参数/优化器/梯度分片)、**激活检查点**、Flash-Attention/Sequence Parallel、KV Cache 量化/分页(paged)/分片。
* **稳定性/收敛**: 大批次需要学习率/预热/正则调整；MoE 加强负载均衡与门控正则。
* **拓扑映射**: TP/EP 落在 NVLink/NVSwitch，PP/DP 扩到跨机；进程组与亲和性(NUMA/PCIe)一致。

**7) 典型配置示例(思路模板)**

* **70B 级密集模型(训练，多机)**: 
  * 单机 8×H100: **TP=8(同机)**；跨 8 机: **PP=8**；再用 **DP/FSDP** 扩吞吐。总并行＝ DP×PP×TP。
  * 打开激活检查点、ZeRO-2/3，调微批使 1F1B 饱和，确保每阶段算力相近。
* **MoE 规模化(训练)**: 
  * 每机放若干专家，**EP 与 TP 同层** 并存(专家内也可 TP)，批内做 expert-aware 重排；调 capacity factor 与 aux loss。
* **交互式推理(服务)**: 
  * **DP 多副本 + 同机 TP(尽量不跨机)**；**少或不用 PP**；KV Cache 分片(与 TP 维一致)、连续批处理 + 量化；MoE 时把"热专家"与路由就近。

**8) 常见问题与对策**

* **NCCL hang/timeout**: 尺寸/对齐不一致、进程组错误、跨机 TP/EP 网络不足 → 统一张量形状；按拓扑建组；尝试分层通信(分桶)与分阶段 barrier。
* **显存爆**: 未启用 FSDP/ZeRO、激活检查点；KV Cache 未分片/未分页；TP 未开 Sequence Parallel。
* **PP 气泡大**: 微批不足或切分不均 → 提高 micro-batch、交错 1F1B、重划阶段。
* **MoE 负载倾斜/塌缩**: 调门控温度、噪声、aux/importance loss；限制 Top-k；capacity factor 合理。
* **推理延迟高**: 跨机 TP/EP；PP 多跳；未做连续批；KV 未热驻或未量化 → 亲和部署、参数/权重预热、分层 cache。

**9) 面试/实战回答模板(60 秒版)**

> **策略**: 优先用 **DP(+FSDP/ZeRO)** 扩吞吐；当 **单层放不下** 时在 **同机做 TP**；当 **跨机扩展** 时用 **PP** 把深度切段；若想 **在相似 FLOPs 下提高模型容量**，引入 **MoE(EP)**，并确保 All-to-All 在强互联域。
> **优化点**: 通信-计算重叠、激活检查点、Sequence/Context Parallel、KV Cache 管理、拓扑感知映射。
> **权衡**: TP/EP 延迟敏感且依赖互联；PP 有流水气泡；DP 需解决大批次收敛与分片策略。推理上尽量 **TP=同机、PP=最少、DP=多副本**，MoE 做专家亲和与负载均衡。

:::

#### 通信原理与优化
熟悉分布式训练中设备间通信的基本算法，如 AllReduce 的 ring 算法和 tree 算法，以及NCCL如何高效实现通信。月之暗面一面曾深入追问不同设备间通信如何做、底层算法有哪些(如 ring-reduce)并要求理解其实现。因此要理解 ring-allreduce 的过程(节点环形传递分块梯度求和)以及为什么它能优化通信带宽利用。

:::info 分布式通信原理深度解析

下面是面向技术面/系统设计面的"硬核版"梳理: 把常见集合通信的原理—尤其是 ring-AllReduce、tree-AllReduce、ReduceScatter、AllGather—以及 NCCL 的高效实现机制、网络瓶颈与 RDMA/IB 优化策略系统地讲清楚。

**一、通信性能模型(先立尺)**

* 采用经典 α–β 模型: 
  $$T \approx n\_{\text{steps}} \cdot \alpha + \frac{\text{bytes}}{BW}$$($$\alpha$$: 启动/往返时延，$$BW$$: 有效带宽)
  大消息(bytes 大)→ 带宽受限；小消息/步数多 → 时延主导。
* 衡量指标: 每设备发送/接收的总字节数、总步数(n_steps)、是否能流水化与链路并行度。

### **AllReduce 的两种主流算法**

**1) Ring-AllReduce(带宽最优，步数 O(p))**

设 p 个设备(GPU)，每个有 N 字节的向量。将向量切成 p 份，每份 N/p。分两相: 
**(a) Scatter-Reduce**(p−1 步): 环上每步发送一个分块到下家，同时接收上家的分块并做逐块规约(sum)。
**(b) AllGather**(p−1 步): 把每个设备手上"本块"的最终和，继续环形传播，直到所有设备都收齐 p 个块。

* **每设备总通信量**: 发送 + 接收 = **2·(p−1)/p · N**(接近 2N，当 p 大时趋近 2N)。
* **步数**: 2·(p−1)(但全程强流水化；每步收发并行)。
* **为何带宽利用高**: 
  * 每条链路在大多数时间都在"满管"收发(没有树状回放/停顿)。
  * 分块+流水化: 发送、接收、规约(GPU 核)并行叠加，隐藏一部分 α。
* **何时适合**: 大消息(梯度/激活大、N 大)、网络带宽成为主要瓶颈时；NVLink/NVSwitch/IB 的高带宽场景。

**小例: p=4 的 ring-AllReduce(N 切四块 A0,A1,A2,A3)**

```
GPU0: A0 A1 A2 A3
GPU1: B0 B1 B2 B3
GPU2: C0 C1 C2 C3
GPU3: D0 D1 D2 D3
Scatter-Reduce 阶段（3步，环向右）：
step1: 每GPU送出块k给右邻，同时收左邻的块并累加
step2,3: 重复直到每块都被累加到其"归属"GPU
AllGather 阶段（3步）：把最终和块沿环广播，全员收齐 p 块
```

**时间估算(带宽主导)**: 
$$T \approx \frac{2(p-1)}{p} \cdot \frac{N}{BW} + 2(p-1) \cdot \alpha$$(有流水化时 $$\alpha$$ 影响较小)

**2) Tree-AllReduce(时延更优，步数 O(log p))**

* **Reduce(up)**: 双叉/二项树向上规约到根(log p 步)
* **Broadcast(down)**: 根向下广播结果(log p 步)
* **总步数** $$\approx 2 \cdot \log_2(p)$$(明显少于 ring 的 $$2(p-1)$$)，**更适合小消息/高 $$\alpha$$ 场景**。
* **不足**: 单链路瞬时利用率不如 ring；大消息时带宽利用略逊。

> 工业里常见"双二叉树 / double binary tree"可同时在两棵互补树上传不同分块，进一步提升并行度与链路利用。

**AllReduce 与 ReduceScatter / AllGather 的关系**

* **AllReduce ≡ ReduceScatter + AllGather**。
  先把"和"的不同分块散到各自归属(ReduceScatter)，再拼回全量(AllGather)。
* Ring-AllReduce 的实现本质就是上面两步，因此 **ReduceScatter/AllGather** 也常被单独调用(例如 ZeRO、分片优化器、词表并行)。

**三、NCCL 为何快: 实现机制要点(面试常问)**

1. **拓扑感知与多通道(channels)**
   * 探测 NVLink / NVSwitch / PCIe / IB/RoCE 拓扑，构建一条或多条环/树；
   * **多通道** 并行传输不同分块，跑满多条物理链路。

2. **流水化分块与协议选择**
   * 大消息用 "Simple" 协议(大分块，吞吐优先)，小消息用 "LL/LL128"(低时延)；
   * **分块 + pipeline**: GPU 内核一边收/发一边对到达分块做规约，隐藏 α。

3. **GPU 驱动通信、避开 CPU 瓶颈**
   * 启动 **持久化 CUDA kernel** 直接驱动 DMA / NIC；减少 CPU 参与与上下文切换；
   * **GPUDirect P2P**(同节点)与 **GPUDirect RDMA**(跨节点): NIC 直达 GPU 显存，**零拷贝** 绕过主存。

4. **分层/层次化(hierarchical)**
   * **节点内** 先用 NVLink/NVSwitch 做规约，再 **节点间** 用 IB/RoCE 规约，最后回播；
   * 可结合树/环混合: 节点内走环，跨节点走树(降低步数)。

5. **可调参/可观测性(常用环境变量)**
   * 诊断: `NCCL_DEBUG=INFO|TRACE`, `NCCL_TOPO_DUMP=1`
   * 算法/协议: `NCCL_ALGO=Ring|Tree|Collnet`，`NCCL_PROTO=Simple|LL|LL128`
   * 通道与缓冲: `NCCL_MIN_NCHANNELS`, `NCCL_MAX_NCHANNELS`, `NCCL_BUFFSIZE`
   * IB/RoCE: `NCCL_IB_HCA`, `NCCL_IB_GID_INDEX`, `NCCL_IB_QPS_PER_CONNECTION`, `NCCL_IB_GDR_LEVEL`
   * 套接字: `NCCL_SOCKET_IFNAME`(多网卡环境明确网段)

> 面试要点: **如何在不同消息大小/拓扑下选 Ring vs Tree、如何定位网络/拓扑未被跑满**。

**四、其他集合通信原语(实现/选型心智图)**

* **ReduceScatter**: 把按块规约的结果按"属主"分发到各设备(每人拿一块"和")。
  * **通信量**: 每设备 $$\frac{(p-1)}{p} \cdot N$$，步数 $$\sim (p-1)$$(ring)或 $$\log p$$(tree)。
  * **用途**: ZeRO-2/3、张量并行的分片参数/梯度流、MoE 专家负载统计等。

* **AllGather**: 每设备把本地块广播出去，所有人收齐所有块。
  * **通信量**: 同上 $$\frac{(p-1)}{p} \cdot N$$
  * **用途**: 词表并行的 logits 汇总、激活拼接、pipeline 边界拼合。

* **All-to-All**(MoE 核心): token/expert 之间的双向重排(每人给每人发)。
  * **极其吃网络**；常做 **分层 all-to-all**(先节点内、再跨节点)，并配合 **分组 GEMM、路由局部性、容量因子** 控制放大。

**五、网络与互联: 为什么 10GbE 会"卡死"多机 Decode**

**1) 直觉与算例**

* 10GbE **理论极限带宽 ~ 1.25 GB/s**，实际能到 0.9–1.0 GB/s 就不错。
* 推理 **Decode 阶段** 每步(每个 token)都要做跨设备通信(取决于并行策略): 
  * **张量并行(TP)** 跨机器: 通常需要对中间张量做 **AllReduce/AllGather**(例如 logits 聚合)。
  * 例: vocab=50k，batch=8，FP16 logits ≈ 50k×8×2B ≈ **0.76 MB/步**；
    若 TP=4 且跨节点，做 AllGather/AllReduce 的 **等效传输** ≈ 2·(p−1)/p·bytes ≈ **1.14 MB/步**。
    **在 10GbE 上仅带宽项就 ~ 1.1–1.3 ms/步**(未含 α、堆栈开销、拥塞/排队)。
    但真实系统还有 KV 片段同步、控制报文、队列等待、内核处理等，**多模型/多租户时会显著放大**。
* 训练更夸张: 一次反传需要对 **全量梯度**(~模型大小)做 AllReduce；10GbE 几乎不可用。

**2) 互联选择与层次化**

* **单机内**: NVLink/NVSwitch(带宽数百 GB/s，延迟低)，应把 **TP/序列并行** 尽量限定在 **单机/单底板**。
* **跨机**: InfiniBand NDR/HDR(200–400 Gbps)、RoCEv2(100–400 Gbps)，才能支撑大规模训练/高 QPS 推理。
* **层次化并行**: **TP/Seq-Parallel 节点内**，**PP/DP 跨节点**；MoE 的 **all-to-all 优先节点内**。

**六、RDMA / InfiniBand / RoCE 的价值与要点**

**1) RDMA 的本质与价值**

RDMA(Remote Direct Memory Access)
* **原理**: 网卡(NIC)直接通过 DMA 引擎访问远端节点内存 → 不再需要 CPU 参与协议栈。
* **在 NCCL/Horovod 等分布式框架里**: 可直接做到 GPU 显存到 GPU 显存(GPUDirect RDMA)，绕过 CPU 和主机内存。
* **关键价值**: 
  * **低时延**: 从几微秒降到亚微秒级别。
  * **低抖动**: 避免操作系统调度带来的尾延迟。
  * **低 CPU 占用**: CPU 不再搬运数据，可以专心做别的。

> 简单说: RDMA 是多机大规模训练/推理的"基础设施"，没有它，AllReduce/AllGather 等高频通信会被 CPU 和协议栈拖死。

**2) InfiniBand(IB)**

* **原生支持 RDMA**(硬件设计就是为 RDMA 服务的)。
* **专用交换机与链路协议**，不是"跑在以太网上的附加层"。
* **可靠性和拥塞控制极其成熟**(例如 信用机制 + 自带拥塞控制)。
* **低延迟 & 稳定 tail latency** → 在大规模训练集群(数百/数千 GPU)里，IB 仍然是黄金标准。
* **带宽**: HDR = 200 Gbps，NDR = 400 Gbps，XDR = 800 Gbps。

> 价值点: 如果要构建像 GPT-4、Gemini 这种 **超大规模万卡集群**，IB 是首选。

**3) RoCEv2(RDMA over Converged Ethernet v2)**

* **在 以太网 上模拟 RDMA**(跑在 UDP/IP 之上)。
* **好处**: 可以用已有的以太网基础设施(和云上环境兼容)。
* **问题**: 以太网默认是 best effort，会丢包；而 RDMA 要求 lossless。
* **需要依赖**: 
  * **PFC**(Priority Flow Control): 优先级流控制
  * **ECN**(Explicit Congestion Notification): 显式拥塞通知
  * 等机制，保证无丢包和拥塞管理。
* **性能**: 网络调优到位时，可以做到接近 IB，但尾延迟控制比 IB 难。
* **云厂商常用方案**，因为以太网设备更通用、成本低。

> 价值点: 更便宜、可复用以太网环境；适合 **企业级集群 / 公有云**，只要运维能搞定网络调优。

**4) EFA(Elastic Fabric Adapter, AWS 专用)**

* **AWS 自研的类 RDMA NIC**，走的是 以太网+自研协议。
* **和 NCCL 有专门的 EFA 插件配合**，可以支持 GPUDirect RDMA-like 能力。
* **性能**: 在 AWS 上做 AllReduce 时，能接近 RoCE/IB 的水平。
* **限制**: 仅限 AWS，跨云不可移植。

> 价值点: 在 AWS 上训练/推理大模型，这是唯一能提供接近 IB 级别延迟和吞吐的选项。

**5) 交换机内计算(In-Network Computing, 如 Mellanox SHARP)**

* **思想**: 很多分布式训练通信是 **规约(reduce)操作**(比如 AllReduce)。
* **传统做法**: 所有数据先传到 GPU，GPU 才做规约。
* **交换机内计算**: 直接在交换机 ASIC 上完成部分规约，减少数据回传。
* **代表技术**: SHARP (Scalable Hierarchical Aggregation and Reduction Protocol)
  * 比如做 AllReduce，可以在交换机层先合并一部分结果，再往下传。
  * 减少端到端传输的字节数和步数。
  * NCCL 已支持通过插件或 CollNet 调用交换机内计算。

> 价值点: 进一步降低 AllReduce 的时延和网络压力，对 **数千 GPU 的大规模分布式训练** 非常关键。

**6) 总结对比表**

| 技术 | 基础 | 优势 | 劣势 | 典型应用 |
|------|------|------|------|----------|
| RDMA | NIC 直访远端内存 | 低延迟、低抖动、低 CPU | 需要 RDMA-capable NIC/网络 | 分布式系统基石 |
| InfiniBand | 专用协议栈 | 稳定、性能最佳、尾延迟优 | 成本高，需专用硬件 | 超大规模 AI 训练(DGX SuperPOD) |
| RoCEv2 | 以太网上跑 RDMA | 成本低，兼容以太网 | 需复杂调优，尾延迟差于 IB | 企业集群，公有云 |
| EFA | AWS 定制 RDMA | 云内性能好，支持 NCCL | 仅限 AWS，锁定厂商 | AWS 上训练/推理 |
| 交换机内计算 | 在交换机做规约 | 节省传输字节和步数 | 依赖特定交换机/插件 | 超大规模 AllReduce |

**一句话总结**: 
* **RDMA 是基石**(绕过 CPU，直通 GPU 内存)。
* **IB 最稳**(低延迟，尾延迟小，大规模集群首选)。
* **RoCEv2 更便宜**(以太网 + 调优，性能可接近 IB)。
* **EFA 是 AWS 定制方案**(云上跑大模型时必须用)。
* **交换机内计算进一步加速 AllReduce**，适合千卡以上规模。

**七、面试速答模版(可直接背)**

**Q1: Ring vs Tree 何时选？**
* 大消息/带宽瓶颈: **Ring**；小消息/高延迟/设备多: **Tree** 或 **双树**。
* 多机: 节点内 ring(NVLink/NVSwitch)，跨节点 tree(IB/RoCE)做 **分层**。

**Q2: NCCL 为何快？**
* **拓扑感知 + 多环/多通道**；**分块+流水化**；**GPU-driven + GPUDirect (P2P/RDMA)**；**分层混合算法**；按大小自适应 **PROTO/ALGO**。

**Q3: 10GbE 下多机 decode 很慢怎么办？**
* 限制 TP 在节点内；跨机改 PP/DP；词表并行+本地 Top-k；speculative decoding；升级到 **IB/RoCE**(RDMA)，并正确调优。

**Q4: ReduceScatter/AllGather 在训练里的用法？**
* ZeRO-2/3: 用 **ReduceScatter** 把梯度分片落到各卡，减少显存与通信峰值；更新后再 **AllGather** 参数/优化器状态。

**八、常用公式&对比(记住即可)**

* **Ring-AllReduce**(带宽主导近似): 
  $$T \approx \frac{2(p-1)}{p} \cdot \frac{N}{BW} + 2(p-1) \cdot \alpha$$
* **Tree-AllReduce**: 
$$T \approx 2 \cdot \log_2(p) \cdot \alpha + c \cdot \frac{N}{BW}$$($$c$$ 与实现/并行度相关，通常 $$> \frac{(p-1)}{p}$$)
* **AllReduce ↔ ReduceScatter + AllGather**: 两者通信量相等、步数可按 ring 或 tree 选择。
* **10GbE**: $$BW \approx 1.25$$ GB/s(理想值)；粗算 decode 每步通信延迟 = 有效字节 / $$BW$$(再叠加 $$\alpha$$ 与系统开销)。

:::

也了解ReduceScatter、AllGather等操作。在网络层面，还应了解高速互联对分布式的影响——例如10Gb以太网环境下，多机并行的 Decode 阶段会因网络带宽成为瓶颈。可以准备如何利用 RDMA、InfiniBand 等技术改善带宽/延迟。

### 分布式一致性与可靠性
虽然偏底层，但可能涉及如分布式一致性算法(Paxos/Raft)在参数服务器或集群管理中的应用，容错机制(检查点与恢复)。特别地，岗位强调过"解决过生产系统性能问题优先"，因此可能被问到如何定位分布式系统中的瓶颈或不稳定因素。要熟悉监控和调优方法，例如使用指标追踪延迟、吞吐，定位慢节点并隔离。

:::info 推理系统容错与恢复设计手册

从目标与预算 → 三大对象(权重/分区、KV Cache、会话与请求)→ 具体实现要点 → 监控与演练的完整容错设计指南。

**总体目标与时间预算**

* **目标**: 节点异常重启到可对外服务 **≤30–90s**；大规模抖动时 **小于1% 请求丢失**、**小于3% SLO 违约**。
* **时间构成(参考 32B–70B 密集模型，单机 8×H100)**
  * 容器启动 + runtime 初始化: 5–15s
  * 通信栈初始化(NCCL/UCX、TP 组装): 3–8s
  * **权重装载(含量化元数据)**: 15–60s(强依赖 **本地 NVMe 命中率** 与并发加载)
  * CUDA Graph 录制/热身: 3–10s
  * 就绪探针稳定窗: 3–10s
    ⇒ **命中本地权重缓存** 是达标与否的分水岭。

**A. 权重 / 分区检查点(Artifacts)**

**A.1 形态与版本**

* **不可变快照**(内容寻址，强一致): 
  * `manifest.json`: 模型 id、架构、**TP/EP 分片映射**、每张权重文件的 **sha256/blake3**、**量化元数据**(dtype、group-size、scales/zero-points、per-channel/per-tensor)。
  * `weights/part-*.bin`: 分片化权重(按 TP shard / expert 切分)。
  * `index/*.idx`: 形状/偏移索引，MMAP 友好(64B/4KB 对齐)。
* **版本号=内容哈希**(manifest hash)，**杜绝"错版本"**。任何文件变更都会引起哈希变化。

**A.2 多级分发与回退**

1. **本地 NVMe 预热(warm cache)**
   * 目录示例: `/nvme/weights/<model_id>/<manifest_hash>/...`(hash 分桶)。
   * 定时 **预取器**(sidecar/job)根据 **近7天访问频率 + 预排班** 下发；落地后校验 hash。

2. **机架内 P2P 拉取**(优先同 AZ、同机架)
   * 简单做法: gRPC + range fetch；高级做法: 基于 Bittorrent/Dragonfly 的 **内容寻址 P2P**。

3. **对象存储兜底**(S3/GCS/OSS)(并发 range GET，分片校验)。

4. **失败策略**: 分层回退 + 限流；任何来源校验失败即丢弃，**永不服务"半坏快照"**。

**A.3 并行加载与流水化就绪**

* **分片并发 + 逐层流水化**: 
  * 先装 **embedding/首末层** → 可开启 **低并发就绪**；后台继续加载其余层(对大模型/EP 尤其有效)。
* **解压/反量化并行**(CPU 核绑定)，I/O 与解码 overlap。
* **NCCL/TP 初始化前置**: 并发创建 communicator，缩短权重就位到可服务的间隔。
* **就绪策略**: 
  * `readiness=degraded`: 只接 **小 batch / 短上下文**(快速接单)；
  * 权重全部就绪后提升为 `ready=full`(放开并发/长上下文)。

> vLLM/SGLang 实操要点
>
> * `--download-dir /nvme/weights`、`--tensor-parallel-size 2|4`
> * 允许 **分片并发加载**(engine 参数)
> * 禁用 `enforce-eager`，**启用 CUDA Graph**(decode 阶段收益最大)
> * 大文件 **mmap + readahead**(`MADV_WILLNEED`/`posix_fadvise`)

**B. KV Cache / 页式缓存的容错**

**B.1 基本原则**

* **不做分布式 checkpoint**(太重且一致性复杂)。**节点丢 KV 就"重算"**。
* 通过 **会话粘性 + 一致性哈希** 最大化 KV 命中，减少重算概率与范围。

**B.2 三层缓存形态(示例)**

* **GPU 层**: Paged KV(固定大小 page/slot，Buddy/Bitmap 管理，L2 命中关键)。
* **CPU 层(可选)**: Pinned/Host KV(回落层，prefill 前可预取到 GPU)。
* **NVMe 层(软持久)**: 
  * **对象**: 最近热 **前缀**(prompt)或 **subsequence chunk**(LMCache 思路)，以 `{model, tokenizer, tokens_hash, quant_hash}` 作为 key。
  * **数据**: `index.db`(SQLite/RocksDB)+ `pages/*.kv`(append-only，块校验 CRC32C/xxHash)。
  * **写入窗口**: 仅在 **prefill** 结束/间隙落盘(避免 decode 热路径 I/O 干扰)。
  * **淘汰**: TTL + GDSF/TinyLFU；按字节热度与命中收益驱动。

**B.3 重启"软恢复"**

* **启动**: 加载 `index.db` → 快速重建"存在但未校验"的目录(几十~几百毫秒)。
* **按需校验**: 首次命中时才校验对应页的校验和；失败立刻逐页淘汰并重算。
* **预热策略**: 路由器基于历史 **会话/租户** 热点，在节点就绪后的前 5–15s 下发 **前缀预取**。

> 关键点: **从 NVMe 回灌到 GPU 必须在 prefill 前完成**；decode 阶段回盘取数会拉高 tail。

**C. 会话与请求级容错**

**C.1 幂等与重放**

* **幂等请求 ID(`request_id`)**: 网关/路由层去重(Redis/KV 带 TTL)，重复提交仅回放同一结果。
* **确定性采样**: 传入 `seed`(或从 `request_id` 派生)，保证重试 **生成一致**；流式中断可用 `offset` 续传。
* **部分结果持久化**(可选): 对关键租户，路由层保存已发送 token 数(轻量断点续传)。

**C.2 排队与回放(队列持久化)**

* **路由层持久化队列**: Kafka/NATS JetStream/Redis Streams，保存 **未分发/进行中** 请求的最小描述(prompt 哈希 + 采样参数 + 账户/配额)。
* **节点软下线(drain)**: 
  * 置 `readiness=0` → 停止新流量
  * 等 **进行中请求** 收尾(或设定最长期限)
  * 未完成的把 **上下文/参数** 打包回队列 → 由一致性哈希的下一个节点接管(**KV 会 miss，但能恢复服务**)。

**C.3 限流与降级**

* **触发条件**: 排队时长↑、错误率↑、GPU 内存碎片↑、NVMe 命中率↓、网络抖动↑。
* **降级菜单(从轻到重)**: 
  1. 降低并发阈值 / batch 上限
  2. 关闭高成本特性(speculative/重排序/多路 reranking/高维 logprobs)
  3. 缩短 `max_tokens`、限制上下文
  4. 采样退回到 **贪心/较小 top-p**
  5. 路由层启用 **hedged 请求**(幂等前提下)；极端时 **拒绝低优先级租户**

**D. 观测与演练(SRE 视角)**

**关键指标**

* **权重侧**: `weight_cache_hit_rate`、对象存储下载时延/吞吐、分片校验失败率、启动到 degraded/ready 的时间曲线。
* **KV 侧**: `kv_gpu_hit`/`kv_cpu_hit`/`kv_nvme_hit`、从 NVMe 回灌耗时(prefill/总)、页损坏率、写放大。
* **请求侧**: p50/p95/p99、`queue_time`、丢弃率、重试率、幂等命中率、降级触发次数。
* **通信侧**: NCCL 初始化时长、AllReduce/AllToAll 占比、网络丢包/ECN 标记。

**定期演练**

* **单节点杀戮/重启**: 验证 30–90s 可用阈；观察 SLO 与降级效果。
* **权重损坏/错版本注入**: 核验哈希链路，确保"坏快照不可服务"。
* **路由器切换/分区**: 确认排队持久化与回放正确。
* **NVMe 冷盘**: 测完全从对象存储冷启动时延(确定最大恢复时间)。

**结语(如何取舍)**

* **小规模/对恢复时间不敏感**: 只做 **本地权重缓存 + 幂等 ID** 即可。
* **中规模(多租户)**: 加上 **P2P 拉取、持久化队列、降级策略**。
* **大规模/严苛 SLO**: 进一步上 **NVMe 软持久 KV、预热调度、in-place 热身与分阶段就绪**。

把"**权重快照可验证且近端命中** + **KV 可失效但可快速重建** + **请求端严格幂等可回放**"三件事做好，你的推理系统就具备了训练级的容错韧性，但成本和复杂度远低于训练时那套全量 checkpoint。

:::

### 4. 大模型原理与优化

面试可能深挖你对Transformer等模型结构的理解，以及大模型训练与推理中的常见问题和解决方案: 

#### Transformer 架构细节
熟悉Attention机制，尤其多头注意力(MHA)的原理、时间/空间复杂度，以及常见优化如Flash Attention、KV Cache使用等。能手写简化版多头注意力代码或说明其实现原理，以及Layer Norm等结构在训练中的作用。

:::info Transformer 架构深度解析

从结构→多头注意力→时间/空间复杂度→关键优化→LayerNorm 机理→简化实现示例→面试要点清单的完整技术讲解。

**1) Transformer 基本结构(以 Pre-LN 为主流)**

一个标准的 **Transformer block(Pre-LN)**: 

```
x ──LN──► MHA ──► +（残差） ──► y ──LN──► FFN（MLP） ──► +（残差） ──► out
```

* **Pre-LN(LayerNorm 在子层之前)**: 梯度更稳定、深层训练更容易收敛(现代 LLM 基本都用)。
* **FFN**: 两层全连接，中间激活(GELU/SiLU/Swish 等)，维度常为 3–4×hidden_size。
* **位置编码**: 旋转位置编码(**RoPE**)在 Q/K 上施加复旋转，相比绝对/相对位置嵌入更易外推长上下文。

**2) 多头注意力(MHA)原理与实现要点**

**(a) 张量形状与投影**

设隐藏维 **H**、头数 **h**、每头维 $$d = H/h$$、序列长 **L**、batch **B**。
输入 $$X \in \mathbb{R}^{B \times L \times H}$$，线性投影得到: 

* $$Q = XW_Q$$，$$K = XW_K$$，$$V = XW_V$$，其中 $$W_Q, W_K, W_V \in \mathbb{R}^{H \times H}$$(工程里常把三者拼成一层)。
* 重排为 $$Q, K, V \in \mathbb{R}^{B \times h \times L \times d}$$。

**(b) 缩放点积注意力(Scaled Dot-Product Attention)**

* 打分: $$S = QK^T / \sqrt{d}$$，$$S \in \mathbb{R}^{B \times h \times L \times L}$$
* 掩码: **因果掩码**(上三角 $$-\infty$$，防看未来)+ **padding 掩码**
* 归一化: $$P = \text{softmax}(S_{\text{masked}})$$
* 聚合: $$O = PV$$(仍是 $$B \times h \times L \times d$$)，再 **按头拼接** → $$B \times L \times H$$，经 $$W_O \in \mathbb{R}^{H \times H}$$ 投影。

**(c) 设计细节**

* **$$1/\sqrt{d}$$ 缩放**: 控制点积分布方差，避免 softmax 饱和。
* **数值稳定**: softmax 前做行内 max 减法；混合精度时常用 FP16/BF16 + FP32 累加。
* **参数量**: MHA 约 $$4H^2$$(QKV=$$3H^2$$ + O=$$H^2$$，不计 bias)。
* **Dropout**(训练): 注意力概率 dropout；推理关闭。

**3) 时间/空间复杂度(Prefill vs Decode)**

设每头维 $$d$$ 与 $$H$$ 同阶，忽略常数: 

* **FLOPs(一次前向)**
  * $$QK^T$$: $$O(B \cdot h \cdot L^2 \cdot d)$$
  * $$P \cdot V$$: $$O(B \cdot h \cdot L^2 \cdot d)$$
  * 合计约 $$\sim 2 B h L^2 d \approx O(B \cdot L^2 \cdot H)$$
* **内存(激活/中间产物)**: 显式存 $$L \times L$$ 注意力矩阵 → $$O(B \cdot h \cdot L^2)$$，成为长上下文的大头。

**两种阶段**: 

* **Prefill(一次性编码长上下文)**: 计算与显存均为 **$$O(L^2)$$**，易受显存/带宽瓶颈。
* **Decode(自回归逐 token)**: 若 **缓存 K/V**，每步只与过去 $$L$$ 交互，**FLOPs ~ $$O(L \cdot H)$$**，大幅低于 $$O(L^2)$$；关键瓶颈变成 **读 KV 的带宽** 与 **kernel 启动开销**(因此要 CUDA Graph)。

**4) 常见优化: FlashAttention / KV Cache / MQA-GQA / 融合核**

**(a) FlashAttention(IO-aware Attention)**

目标: **不显式物化 $$L \times L$$ 注意力矩阵**，改为 **分块流式** 计算，驻留在 **SRAM/寄存器** 中，避免 $$O(L^2)$$ 的 HBM 读写。

* **核心**: 对每一行做 **在线 softmax** 的块归约(max-trick 保数稳)，块间维护 `(m_i, l_i, o_i)`: 
  * `m_new = max(m_old, max_j S_ij)`
  * `l_new = exp(m_old - m_new)*l_old + Σ_j exp(S_ij - m_new)`
  * `o_new = (exp(m_old - m_new)*l_old*o_old + Σ_j exp(S_ij - m_new)*V_j) / l_new`
* **复杂度**: FLOPs 仍 $$O(L^2 H)$$，**显存占用与 HBM 流量降为 $$O(L \cdot H)$$**；训练/推理显著提速，长序列更明显。
* **工程要点**: 块大小匹配 **SM 寄存器/共享内存**；融合 **QK^T→softmax→PV**；配合 **RoPE**、mask、dropout 的内核内融合。

> 解码端的 **FlashDecoding** 进一步优化对单步 KV 的访存与并行，缩小常数项。

**(b) KV Cache(推理的生命线)**

* **作用**: 保存历史 `K,V`，避免每步重算过去 token 的投影。
* **大小公式(很重要)**: 
  **每 token、每层的 KV 元素数 = $$2 \times H$$**(与头数无关)，所以
  $$\text{KV\_bytes} = \text{(layers)} \times \text{(tokens)} \times (2H) \times \text{(bytes\_per\_elem)}$$
  例: $$L=48, H=8192, T=8k, \text{FP16}(2B)$$ → $$48 \times 8000 \times 2 \times 8192 \times 2 \approx 12 \text{ GB}$$
* **Paged KV**: 按页/块管理(buddy/bitmap)，减少碎片与拷贝。
* **前缀缓存**: 对共享 prompt 的多请求复用 KV(企业多租户、RAG 前缀很常见)。
* **KV 量化**: FP8/INT8 KV(分组量化+标尺/零点)，以少量精度损失换巨大容量/带宽收益。

**(c) MQA/GQA(多查询/分组查询注意力)**

* 传统 MHA: Q/K/V 都有 $$h$$ 个头 → KV Cache 成本与带宽按 $$h$$ 线性增长。
* **MQA**: $$K,V$$ 只保留 **1 个头**，所有 $$Q$$ 共享；**GQA**: $$K,V$$ 保留 $$g(\ll h)$$ 个组。
* **内存与带宽**: $$K/V$$ 规模缩小至 **$$g/h$$** 倍(MQA 是 $$1/h$$)，**KV cache 与解码读带宽显著下降**，对大模型推理尤为关键。
* **效果**: 几乎不损失质量(取决于模型/任务)，但需要训练端或蒸馏适配。

**(d) 融合与图化**

* **QKV 合并线性层**、**AddBias + RoPE 融合**、**mask + softmax 融合**、**残差+dropout+LN 融合** → 降低 kernel 启动与 HBM 往返。
* **CUDA Graph**: 把 decode 的小 kernel 序列"录制成图"一次提交，显著降低启动开销与尾延迟。

**5) LayerNorm 在训练中的作用与变体**

**(a) 标准 LayerNorm(LN)**

对每个 token 的隐藏向量 $$h \in \mathbb{R}^H$$: 

$$\mu = \text{mean}(h); \quad \sigma = \sqrt{\text{var}(h)+\varepsilon}$$
$$\text{LN}(h) = \gamma \odot \frac{h-\mu}{\sigma} + \beta \quad \text{($$\gamma, \beta$$ 为可学习仿射参数)}$$

* **作用**: 稳定激活分布，**缓解梯度爆炸/消失**，加速收敛。
* **Pre-LN 的好处**: 梯度可直接穿过残差主干，深层(>48/80 层)更易训练。

**(b) RMSNorm(很多 LLM 采用)**

$$\text{RMSNorm}(h) = \gamma \odot \frac{h}{\sqrt{\text{mean}(h^2)+\varepsilon}} \quad \text{(无均值减法)}$$

* **更省算**、数值更稳定(经验上)，搭配 Pre-LN 常见于 LLaMA 系列。

**(c) 细节**

* **ε 的选择** 会影响数稳(1e-5~1e-6 常见)。
* **残差缩放**(如 $$1/\sqrt{2}$$、或深度相关的可学习缩放)可进一步稳定深层训练。

**6) 可手写的简化版 MHA 前向(PyTorch 风格，教学用)**

```python
import torch
import torch.nn.functional as F

def mha_simple(x, W_qkv, W_o, n_heads, mask=None, rope=None):
    """
    x: (B, L, H)
    W_qkv: (H, 3H)   -- 合并的 QKV 权重
    W_o:   (H, H)
    n_heads = h
    """
    B, L, H = x.shape
    d = H // n_heads

    # 1) 线性投影并拆分 Q/K/V
    qkv = x @ W_qkv                        # (B, L, 3H)
    q, k, v = torch.chunk(qkv, 3, dim=-1)  # 各 (B, L, H)

    # 2) 重排为多头
    def reshape_heads(t):
        return t.view(B, L, n_heads, d).transpose(1, 2)  # (B, h, L, d)
    q, k, v = map(reshape_heads, (q, k, v))

    # 3) 可选 RoPE：对 (q,k) 施加旋转位置编码
    if rope is not None:
        q, k = rope(q, k)  # 需保证形状匹配

    # 4) 注意力：缩放点积 + 掩码 + softmax
    scale = 1.0 / (d ** 0.5)
    scores = torch.matmul(q, k.transpose(-2, -1)) * scale  # (B, h, L, L)

    if mask is not None:
        # mask: (1, 1, L, L)，被屏蔽位置填入 -inf
        scores = scores.masked_fill(mask == 0, float('-inf'))

    probs = F.softmax(scores, dim=-1)                       # (B, h, L, L)

    # 5) 聚合
    ctx = torch.matmul(probs, v)                            # (B, h, L, d)

    # 6) 合并头并输出投影
    ctx = ctx.transpose(1, 2).contiguous().view(B, L, H)    # (B, L, H)
    out = ctx @ W_o                                         # (B, L, H)
    return out
```

> 说明: 
>
> * 教学用"朴素实现": 会显式构造 `L×L` 注意力矩阵，**不适合长上下文/生产环境**。
> * 生产中使用 **FlashAttention** 或等价 fused kernel，避免 `O(L^2)` 中间张量。
> * 推理时开启 **KV Cache**，将 `k,v` 分步累积到缓存，decode 阶段每步只计算新 token 的 Q 与与缓存的 K/V 交互。

**7) 面试/落地要点清单(可速记)**

* **复杂度**: MHA FLOPs ~ $$O(B \cdot L^2 \cdot H)$$；Prefill $$O(L^2)$$，Decode $$O(L)$$(有 KV)。
* **KV Cache 大小**: 每 token 每层 **$$2H$$ 元素**(与头数无关)；可用 **GQA/MQA** 将 KV 按 $$g/h$$ 缩减。
* **FlashAttention**: IO-aware、在线 softmax、减少 HBM 流量到 $$O(L \cdot H)$$；FLOPs 不变、吞吐显著升。
* **CUDA Graph**: 把 decode 的小 kernel 链条一次提交，降 tail。
* **LayerNorm**: 稳定训练(Pre-LN 更易深层收敛)；RMSNorm 更省算。
* **RoPE**: 在 Q/K 上施加复旋转，利于长程外推。
* **工程实践**: 融合 QKV、RoPE、softmax kernel；Paged KV；GQA；FP8/W4 权重量化 +(可选)KV 量化。

:::

#### MoE (Mixture-of-Experts)
明确Transformer和MoE架构差别，MoE在前馈层引入多个专家网络及路由器选择子集专家，从而在参数规模很大但推理时只激活部分专家。知道MoE的挑战(专家不均衡利用、路由训练难度)和解决方法(如噪声路由、限制每专家处理token数等)。因为有前人面试被直接问到Transformer vs MoE区别，所以对这种大模型变体要有所准备。


:::info Transformer vs MoE 架构详解

### Transformer和MoE

![Transformer vs MoE架构对比](../llm-infra/img/trans_vs_moe.gif)

1. 结构层面: 稀疏的是 FFN，不是注意力

* **Dense Transformer**: 每层通常是 `Self-Attn → FFN`；所有 token 都经过 **同一个** 前馈网络(FFN)。
* **MoE Transformer**: 仅把 **FFN** 替换为"专家集合"(E 个专家，每个专家是一套独立 FFN)。

一层 MoE 的数据流: 
`Self-Attn → Router(线性+Softmax) → 选 Top-k 专家 → 专家前向 → 按门控权重加权合并 → 下一层`

注意力仍是 **稠密** 的；**稀疏发生在 FFN 阶段**。

2. 计算与参数规模: 用参数换计算(per-token)

设隐藏维 `d_model`，FFN 宽度 `d_ff`。

* **Dense FFN FLOPs**(单 token，忽略激活函数)$$\approx 2 \times d\_{\text{model}} \times d\_{\text{ff}}$$。
* **MoE FFN FLOPs**(Top-k 路由): $$\approx k \times (2 \times d\_{\text{model}} \times d\_{\text{ff}})$$(每命中 1 个专家就做一次 FFN)。

典型设置: 
* **Top-1(Switch 类)**: per-token FFN 计算 $$\approx$$ Dense 持平，但参数变为 **E 倍**(因为有 E 个专家)。
* **Top-2**: per-token FFN 计算 $$\approx$$ Dense 的 2×，但参数仍是 **E 倍**。

**参数量**: MoE 主要增加的是 **权重(专家)数量**，不是每 token 的计算量。

举例: $$d\_{\text{model}}=2048, d\_{\text{ff}}=8192$$ 时，单个 FFN 参数约 $$2 \times 2048 \times 8192 \approx 33.6\text{M}$$。
若一层设 **E=64** 个专家、Top-1: 
* **参数** $$\approx 64 \times 33.6\text{M} \approx 2.15\text{B}$$(仅 FFN 部分)
* **FLOPs/token(FFN)** $$\approx$$ 和 Dense 持平

这就是"**参数巨大、计算稀疏**"的核心优势: **提升容量而不线性增加每 token 计算**。

3. 质量与收敛直觉: 专长分工 vs. 通才网络

* **Dense**: 单一 FFN 学"通才"表示。
* **MoE**: 不同专家会 **自发"专业化"**(语言/领域/语法结构等)。在同等或更少的 per-token 计算下，**更高的模型容量** 通常带来更好的样本效率与效果上界。

4. 路由与负载均衡: MoE 的关键难点

* **Router**: 常用线性投影到 `E` 维后 softmax，选 Top-k(k=1/2 常见)，并用门控权重加权合并输出。
* **负载均衡问题**: 如果不约束，少数热门专家会"爆满"。常见做法: 
  * **Auxiliary load-balancing loss**(惩罚专家使用分布的偏斜)
  * **Noisy Top-k**(加噪声打散)/ **温度** / **随机路由** / **Sinkhorn 匹配** 等
* **容量(capacity factor)**: 每专家对一个 batch 可接收的 token 上限。
  * **dropless**: 不丢 token，超过容量用 **padding** 补齐(算力稳定、通信规整)
  * **dropping**: 溢出 token 丢弃或回退次选专家(节省算力但会影响梯度一致性/质量)
* **路由稳定性**: 可加 **z-loss** 限制路由 logits 过大，降低"专家塌缩"。

5. 并行与系统实现: All-to-All 是瓶颈

* **并行维度组合**: MoE 常与 **DP/TP/PP** 叠加，引入 **EP(Expert Parallelism)**: 
  * **EP**: 把 E 个专家分布到不同 GPU；路由后需要把 token 表征 **分发到对应 GPU**，做完专家 FFN 再 **回传**。
  * 这引入两次 **All-to-All**(FWD/BWD 各一次)，常成为 **性能瓶颈**。
* **通信体量估算**(单层前向): 近似 ≈ `2 * Tokens * d_model * bytes * k`(一次发、一次数)，反传再乘一遍。
* **工程优化要点**: 
  * **核融合/打包**: 高效 pack/unpack、排序聚合，减少小消息
  * **分组 All-to-All**、跨层重用通信通道、通信-计算重叠
  * **更细粒度专家**(fine-grained)减少单专家容量、平衡负载
  * **低精度激活传输**(FP8/FP16)降带宽
  * **调容量系数** 与 **Top-k**(Top-1 明显更友好)
  * 框架: DeepSpeed-MoE、Tutel、Megablocks、FasterMoE、vLLM/SGLang 的 MoE 路由器/dispatcher 实现等

6. 训练稳定性与超参

* **常见超参**: `k∈{1,2}`，`capacity_factor≈1.0–1.5`，负载均衡 loss 系数(如 1e-2 量级起步)，路由温度/噪声幅度，dropless vs dropping。
* **优化器与正则**: 与 Dense 相同(如 AdamW / Muon / Adafactor)，但 MoE 中 **梯度方差** 更敏感；适度 **weight decay**、**稳定化技巧**(路由 z-loss、clip-grad)常见。
* **收敛现象**: 专家逐步分工，但早期可能"抢样本"；需靠均衡 loss/噪声引导。

7. 推理(Serving)差异: 吞吐、延迟与调度

* **Dense**: 所有 token 走同一路 FFN，批处理与内存访问模式稳定。
* **MoE**: Token-wise 路由导致 **批次在专家间分裂**: 
  * **优点**: Top-1 时单 token FFN 算力≈Dense，但模型总容量远大；高 QPS 下，能通过"多专家并行"提升 **集群吞吐**(前提是路由均衡、通信高效)。
  * **挑战**: 
    1. **动态批合并** 更复杂(同一 batch token 被分发到不同 GPU)
    2. **All-to-All 延迟** 对小 batch 或低 QPS 更敏感
    3. **容量/过载** 处理(高峰时热门专家排队)
    4. **缓存**: KV cache 仍由注意力决定(MoE 不改变注意力稠密性)，但专家侧的激活缓存/权重放置策略会影响命中与冷启动
* **部署建议**: 
  * **Top-1 gating** 优先，简化通信与算力预算
  * **专家-GPU 映射** 固定、避免跨 NUMA/节点频繁跳转
  * 高峰期对"热门专家"设 **专用副本/配额**，配合 admission control、排队与弹性扩缩容
  * **小请求** 场景可倾向 Dense 或低 E、低 k 的 MoE；**大吞吐** 场景 MoE 优势更显著

8. 何时选 Dense，何时选 MoE？

* **优先 Dense**: 低延迟极敏感、小流量、端侧/单卡推理、工程复杂度需极简。
* **优先 MoE**: 大规模训练/高吞吐在线服务、希望在 **不线性增大 per-token FLOPs** 前提下大幅提升模型容量与表现；能投入工程优化(All-to-All、路由均衡)的团队。

9. 对比总结

| 维度           | Dense Transformer | MoE Transformer                  |
| ------------ | ----------------- | -------------------------------- |
| 稀疏位置         | 无(全层稠密)           | FFN 稀疏(注意力仍稠密)                   |
| per-token 计算 | 固定                | 约为 `k × FFN_cost`(常见 k=1/2)      |
| 参数规模         | 随宽度线性增长           | FFN 参数乘以 E，但 per-token 计算≈与 k 相关 |
| 质量/样本效率      | 通才表示              | 专家分工，容量大、效果上界更高                  |
| 训练稳定性        | 常规                | 需路由均衡、容量与 z-loss 等稳定化            |
| 并行与通信        | 常规 DP/TP/PP       | 需 EP + 两次 All-to-All(瓶颈)         |
| 推理行为         | 批处理简单、稳定          | 路由拆批、通信敏感，吞吐潜力大                  |
| 适用场景         | 小模型/低延迟/端侧        | 大规模训练&高吞吐线上服务                    |

:::


#### 训练问题排查
掌握大模型训练中的典型问题及对策，如 loss 爆炸(可能因为学习率不当、初始化问题等，可通过降低学习率、梯度裁剪等解决)、模式崩溃、梯度不稳定等。能够回答如果训练出现 loss spike 怎么办，以及大batch训练如何调整超参。

#### 主流模型及最新进展
关注GPT、BERT、GLM等主流模型的差异(自回归vs自编码器、位置编码类型、预训练目标等)。也关注最近业界的新模型/新论文，因为"跟踪最新模型进展"是职责之一，面试官可能聊到近期的大模型热点(例如Transformer变体、结构优化、推理加速的新成果)。

确保对例如ChatGPT/GPT-4、Llama2、Baichuan等模型有所了解，至少能谈论参数规模、能力特点，以及开源社区在做什么。月之暗面属于技术信仰派，公司非常前沿，面试官可能会考察你对前沿论文的学习能力和见解。

### 5. 主流大模型部署框架

岗位要求熟悉主流大模型推理框架，因此准备好回答业界常用的模型部署方案: 

- **HuggingFace Transformers & Text Generation Inference (TGI)**: Python生态主流方案，了解其基本用法和性能特点。
- **DeepSpeed-Inference**: 深入了解微软DeepSpeed的推理优化(如张量并行、ZeRO-Offload等)以及它如何提高大模型推理吞吐/降低显存占用。
- **vLLM**: 这是近期很火的高性能推理引擎，利用PagedAttention和高效内存管理，实现高吞吐的多并发生成。需要理解vLLM如何将KV Cache管理和批处理优化提升到新水平，比如其连续批处理 (Continuous batching)理念。
- **TensorRT / FasterTransformer**: NVIDIA的高性能推理库，通过低水平优化(融合kernel、INT8量化等)加速推理。
- **Ray Serve / Distributed Inference**: 有一些基于Ray或其他框架的分布式推理方案。
- **容器化部署工具**: 如TorchServe、TF Serving或KServe等也可以略微熟悉。

总之，能列举一两个框架并讲出它的优劣，会让面试官觉得你对业界生态有全面了解。

### 6. 代码工程与DevOps

公司强调"良好的代码习惯"和DevOps经验，因此准备一些代码规范、工程实践相关的话题。例如: 

- 编码风格、单元测试、Code Review 重要性。
- 持续集成/持续部署(CI/CD)流程，在模型开发中的应用(比如如何自动化测试新模型性能，如何一键部署)。
- 容器化Docker基础: 如何写高质量的Dockerfile，减小镜像体积，合理使用基础镜像等。
- Kubernetes 基础: Pod/Deployment概念、Service和Ingress、ConfigMap/Secret等。

准备过程中，可以结合自己简历中的项目，多想想实现细节和性能调优。比如如果做过模型部署项目，复盘当时如何解决了某个瓶颈(GPU利用率不高？响应延迟过长？用了什么优化手段)。能够举出亲身经历的问题解决案例将是加分项。

## 系统设计面准备

在系统设计面试中，你需要展现设计大规模分布式AI服务的能力。以下是一些关键方向和知识点，可用于构思和回答系统设计题目: 

### 1. 大规模模型推理服务架构设计

假设面试要求设计一个支持超大规模并发的LLM在线推理平台，你可以从以下方面组织答案: 

#### 整体架构
通常采用前端网关 + 后端推理服务的分层架构。前端Gateway负责接收用户请求、认证、路由和负载均衡，后端是多个模型实例(可能分布在许多GPU节点上)提供推理服务。说明如何利用Kubernetes部署这些组件: 如Gateway可用Deployment+Service实现，模型实例作为有状态服务(StatefulSet)或者Deployment+GPU调度。强调架构需水平扩展: 可以增加实例来应对更高QPS。

#### 负载均衡策略
和传统无状态服务不同，LLM推理服务有状态(主要是KV Cache)。设计负载均衡时要考虑请求的粘性: 让相同会话或有相同前缀的请求尽量命中同一实例，从而复用缓存提升速度。可以介绍一种"Prefix-Aware"的路由策略: 网关维护每个实例的缓存前缀信息，将具有相似前缀(如相同会话上下文或历史)的请求路由给已有该前缀缓存的实例。这样能最大化缓存命中，减少重复计算。

#### 状态管理 (KV Cache)
介绍如何处理大模型推理的状态。一般方案是在单实例内维护该实例的KV缓存，不对外暴露。更先进的方案可能探讨跨实例共享缓存，但那会引入复杂的同步成本。因此，大多数实现选择将这部分逻辑隐藏在推理引擎内部，而通过前面的智能路由策略尽可能提高缓存利用率。

#### 扩展与弹性
大规模集群需要自动扩缩容策略。阐述如何用 K8s HPA (Horizontal Pod Autoscaler)根据流量调节实例数，以及Cluster Autoscaler(或Karpenter)动态增加节点。举例说明: 当请求量激增，HPA感知到每个实例CPU/GPU利用率上升，会扩容更多实例Pod；Karpenter检测到集群无空闲GPU，则自动申请新节点。

#### 性能优化
系统设计需考虑延迟和吞吐优化。可以引入关键指标如 TTFT (Time to First Token) 和 TPOT (Time Per Output Token)来讨论。TTFT主要衡量模型Prefill阶段性能(一次性算完用户Prompt得到首个回复的延迟)，TPOT衡量Decode阶段每生成一个token的平均耗时。设计中可提出优化TTFT的手段如批处理请求(将多个请求的Prompt打包一起prefill，提高GPU利用率)，优化TPOT的手段如并行解码等。

#### 容错与稳定性
说明如何保证集群稳定。可从监控和故障恢复讲起: 利用K8s提供的探针(liveness/readiness probe)检测服务健康，失效则自动重启容器；使用分布式Tracing和Logging监控各请求延迟，快速发现性能异常的节点。提到检查点机制: 推理服务虽然无状态(除了缓存)可以快速重启，但如果混合同步训练任务，则训练任务需要定期保存checkpoint，节点故障时从最近checkpoint恢复。

#### 资源利用与调度
如果面试涉及训推混部或多任务调度，可讨论调度算法。比如如何在同一集群上同时跑推理服务和模型训练/微调任务，实现资源最大化利用而互不影响。可以提出"潮汐调度"思想: 在白天高峰时优先保证线上推理服务，夜深流量低谷时释放部分GPU用于训练作业。

### 2. 模型迭代部署的工作流设计

另一类设计题可能让你构思如何加速模型从开发到上线的迭代流程。回答时可以包含: 

#### 模型开发流程
数据收集标注、模型训练、评估的周期。设计自动化Pipeline，比如使用Git+CI触发训练作业，训练完自动跑一系列评测(包括离线指标和模拟真实请求的效果)。

#### 模型登记和版本管理
建立模型仓库(Model Registry)，每个训练产出模型打版本标签，保存指标和元数据。团队可以方便查询哪个版本用于线上。

#### 部署流程
利用容器化将模型封装为镜像。设计CI/CD将通过评审的模型自动部署: 比如用Helm或Kustomize管理不同环境(staging, prod)部署配置。一旦新模型通过测试，在K8s集群上灰度发布: 可以采用蓝绿部署或金丝雀发布策略——同时运行旧版和新版模型服务，按一定比例流量给新版，监控其性能与反馈，然后逐步全量替换。确保出现问题能快速回滚到旧模型。

#### 工作流编排
提到可以使用Kubeflow Pipelines、Airflow等来串联上述步骤，或者公司内部自研脚本。重点是阐述自动化和流水线思想，用工具减少人工介入和等待时间，提高迭代效率。

### 3. 其他系统设计考点

根据要求和可能的侧重，还可准备: 

- **API 设计与QoS**: 假设要设计模型服务的API，可以谈REST vs gRPC，如何设计接口协议以支持流式返回(边生成边返回token)，如何处理超时和取消。以及怎样实现多租户的配额和限流，确保某些用户不会独占资源导致服务退化。
- **安全与权限**: 如果产品to C，可能提及如何防滥用(比如对每IP每分钟请求数限制)，如何确保数据安全(敏感信息脱敏、访问控制)。大模型还有内容安全问题，可提部署监控/过滤模块等。
- **成本考虑**: 大模型服务成本高昂，设计时可以提到一些成本优化措施，如按需启动实例(结合前面弹性伸缩)、充分利用现有GPU算力(混部训练思路)、采用更低精度模型做蒸馏以降低线上开销等。展示你有产品化落地的意识。

系统设计回答时，建议层次清晰: 先总体架构→模块划分→关键机制→具体技术点，并结合实际权衡取舍。比如你可以讨论: "为降低延迟，我选择在单机内多卡加载模型而不拆分到多机，因为多机会引入网络延迟瓶颈；只有模型大到单机放不下时，才考虑多机张量并行部署，那就需要高速网络支持。" 这种体现 trade-off 的论述很有深度。

## 关键知识点详解

最后，总结一些与你岗位强相关的知识点细节，可作为复习提纲: 

### Kubernetes 大规模部署要点

理解K8s如何管理GPU等资源以及在大规模集群上的优化技巧。例如: 

#### 设备资源调度
知道通过 Device Plugin 将GPU纳管进K8s。熟悉节点亲和/污点(把GPU节点打标签，只让GPU工作负载调度上去)以及如何避免GPU碎片浪费(合理设置每Pod的GPU请求资源，GPU共享技术如果有支持)。

#### 自动扩容缩容
熟练掌握 HPA 和 Cluster Autoscaler/Karpenter 的配置与原理，实现根据流量自动伸缩Pod、副本，并动态增减集群节点。

#### 容器化优化
理解容器启动的开销，如何通过镜像预热、本地镜像缓存来加速大镜像部署；利用Init Container做模型下载缓存，以避免每个Pod重复从远端拉取模型。

#### 稳定性保障
合理设置探针检测服务健康，配置重启策略。在高并发时关注K8s Ingress/Service性能，可能需要考虑云厂商SLB配置或Linkerd等服务网格做更智能的流量管理。还要注意 etcd 在超大规模集群下的性能，控制K8s对象数量和变更频率。

### LLM 分布式推理架构

掌握大型模型分布式推理的工作机制和挑战: 

#### 推理流程
分为 Prefill 阶段(并行处理输入，全量计算Attention获取KV缓存)和 Decode阶段(自回归逐token生成，串行依赖缓存)。理解前者计算密集、后者访存/带宽密集的特点，以便优化时对症下药。

#### 性能指标
知道 TTFT 和 TPOT 两大指标及其意义(前者衡量Prompt处理延迟，后者衡量生成速度)，以及端到端延迟包含哪些部分。这有助于在面试中讨论优化时更精确。

#### 批量并发
掌握连续批处理(Continuous batching)思想，即不断收集一定时间窗口内到达的多个请求一起推理，以提高GPU利用率。特别是prefill阶段，可以并行处理多个输入，这种批处理策略显著提升吞吐。但批处理过大会增加延迟，因此需要平衡，并可能动态调整批大小。

#### 缓存与长上下文
大模型可能要处理长上下文，了解 KV Cache作用: 存储过去生成token的key/value避免重复计算。当多个请求串行交互生成时，缓存命中对性能影响巨大。因此知道如何通过前缀路由增进缓存复用(见前述Prefix-Aware策略)。另外，如果设计支持流式输出，需要考虑一边生成一边返回、缓存如何同步刷新等机制。

#### 多机部署
明白何时需要多机推理。如果模型太大单机显存放不下，才采用张量并行跨机。这时网络带宽成为关键瓶颈，如10Gb以太网上AllReduce同步权重会拖慢Decode速度。如果可以，尽量使用同机多GPU或NVLink机架内部通信。如果必须跨机，提到需要高速网络(Infiniband/NVLink Switch)以及可能用模型压缩(如ZeRO Stage-3在推理时只加载部分权重)等减轻通信量。

#### 推理引擎与编排协同
了解目前业界在尝试将推理的一些逻辑从引擎层提升到编排层。也就是让Kubernetes等上层来处理多实例负载均衡、缓存协调，而引擎专注单机优化。这显示了未来演进方向，也提醒我们工程师需要紧跟前沿，将新思想融入架构。

### 大模型训练与部署并行策略

对数据并行、模型并行、流水线并行的原理和区别非常熟悉。能够解释例如: 

- 数据并行如何通过每卡处理不同batch同步梯度来扩展训练，适合计算密集但会受制于梯度聚合通信。
- 模型张量并行将一个模型算层切分到多卡同步计算，解决单卡内存不足但带来更多通信。
- 流水线并行将不同层分给不同设备，微批次流水线，但需要解决bubble和batch splitter。
- MoE并行(专家并行)通过路由不同token到不同专家，有条件地利用多机多专家，注意负载均衡。

知道这些并行方法在推理中有何应用: 推理主要用模型并行(分模型权重)、或专家并行服务不同请求等等。可以提及DeepSpeed ZerO技术在训练时如何分片优化内存，以及部分思路在推理时也可用 (比如ZeRO-Inference)。总之，这些都是面试高频考点。

### 实践经验与问题解决

结合岗位偏好，突出你对线上系统优化的理解: 

#### 性能分析工具
提到曾用过的性能分析工具: 如profilers分析GPU利用率、tracing跟踪延迟来源等。举例说明如何发现某次服务延迟飙升是因为某个GPU出现ECC错误或某依赖出现锁等，然后采取措施。月之暗面很看重解决实际问题的能力，因此最好准备一两个自己排查性能或稳定性问题的故事，体现"独立思考和解决问题"的能力。

#### Kubernetes故障处理
比如Pod挂了如何自动重新调度，有没有遇到过OOM或CrashLoop，如何调优内存/重试策略。对限流和过载保护的理解: 高并发下如何避免击垮服务，是否采用了排队或降级策略。

#### 团队协作与沟通
虽然是软性方面，但可以在系统设计回答中穿插，例如设计一个系统时会如何与算法团队确定模型接口，与DevOps合作实现CI/CD等等，体现你的合作意识。良好的团队沟通也是岗位要求。

以上知识点需要融会贯通地运用在回答中。在准备过程中，多阅读大模型系统方面的最新文章和案例(例如QCon大会分享、和鲸科技的技术专栏等)，将其中的专业术语和实践数据引用到你的答案里，会让面试官眼前一亮，觉得你对业界动态非常关注且有深度理解。

## 总结

面对月之暗面的技术与系统设计面试，既要展示宽广的知识面(从算法到系统，从模型到容器)，又要体现深入的专业度(细节原理和优化手段了然于胸)。通过结合上述指导要点进行充分准备，你将能够自信应对「月之暗面」的面试挑战，在讨论超大规模模型推理系统时拿出令人信服的见解和方案。祝你面试顺利，拿到心仪的Offer！

## 参考资料

- [我的大模型岗位面试总结: 共24家，9个offer - 哔哩哔哩](https://www.bilibili.com/read/cv27115509/)
- [朱啸虎点评月之暗面杨植麟团队 - 新浪财经](https://finance.sina.cn/tech/2024-03-06/detail-inamkitr0115447.d.html?from=wap)
- [大模型"四虎"出山，亮相4月QCon北京站 - InfoQ](https://www.infoq.cn/article/bwui7bk09beqyvi9v6g2)
- [面试月之暗面大模型面试题: Transformer和MoE的差别在哪里？ - CSDN博客](https://blog.csdn.net/m0_59596990/article/details/146691633)
- ["大模型"技术专栏 | 浅谈基于 Kubernetes 的 LLM 分布式推理框架架构: 概览 - 和鲸科技](https://ai.heywhale.com/article/833.html)
- [大模型(LLM)基础Kubernetes环境构建 - CSDN博客](https://blog.csdn.net/weixin_39403185/article/details/147285173)
- [在Kubernetes 中部署LLM 的实践 - 知乎专栏](https://zhuanlan.zhihu.com/p/16078952166)