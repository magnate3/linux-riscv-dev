{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d483501-8b24-40d1-a111-a2bb81037b21",
   "metadata": {},
   "source": [
    "- [llama.cpp源码解读--ggml框架学习](https://zhuanlan.zhihu.com/p/19968327329)\n",
    "- [llama.cpp源码解读--推理流程总览](https://zhuanlan.zhihu.com/p/25774381094)\n",
    "- [llama.cpp源码解读--cgraph计算图与sched后端调度机制详解](https://zhuanlan.zhihu.com/p/1893801096918585567)\n",
    "- [llama.cpp中的量化方法简介](https://zhuanlan.zhihu.com/p/12729759086)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10009b7d-823c-46ae-8355-4e580c686bf3",
   "metadata": {},
   "source": [
    "```c++\n",
    "common_params params;\n",
    "--> common_params_parse;\n",
    "common_init();\n",
    "llama_backend_init();\n",
    "llama_numa_init(params.numa);\n",
    "// load the model and apply lora adapter, if any\n",
    "--> common_init_from_params;\n",
    "llama_model * model = llama_init.model.get();\n",
    "llama_context * ctx = llama_init.context.get();\n",
    "struct results_perplexity results;\n",
    "--> perplexity;\n",
    "llama_perf_context_print(ctx);\n",
    "--> llama_backend_free --> ggml_quantize_free;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019f8f4-e92b-49a1-97c0-7da9b098ca40",
   "metadata": {},
   "source": [
    "# [`common_params_parse`](https://github.com/ggml-org/llama.cpp/blob/master/common/arg.cpp#L1179)\n",
    "\n",
    "```c++\n",
    "common_params_parse(argc, argv, params, LLAMA_EXAMPLE_PERPLEXITY);\n",
    "----------\n",
    "bool common_params_parse(int argc, char ** argv, common_params & params, llama_example ex, void(*print_usage)(int, char **)) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd208c65",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct common_params_context</summary>\n",
    "\n",
    "```c++\n",
    "struct common_params_context {\n",
    "    enum llama_example ex = LLAMA_EXAMPLE_COMMON;\n",
    "    common_params & params;\n",
    "    std::vector<common_arg> options;\n",
    "    void(*print_usage)(int, char **) = nullptr;\n",
    "    common_params_context(common_params & params) : params(params) {}\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "--> common_params_parser_init --> ggml_backend_load_all --> ggml_backend_load_all_from_path --> common_params_context ctx_arg;\n",
    "const common_params params_org = ctx_arg.params; // the example can modify the default params\n",
    "--> common_params_parse_ex;\n",
    "--> common_params_print_usage;\n",
    "--> common_params_print_completion;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1936a7",
   "metadata": {},
   "source": [
    "## [`ggml_backend_load_all_from_path`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend-reg.cpp#L567)\n",
    "\n",
    "```c++\n",
    "auto ctx_arg = common_params_parser_init(params, ex, print_usage);\n",
    "----------\n",
    "common_params_context common_params_parser_init(common_params & params, llama_example ex, void(*print_usage)(int, char **)) {\n",
    "    // load dynamic backends\n",
    "    --> ggml_backend_load_all();\n",
    "    /**\n",
    "     * filter options by example\n",
    "     * rules:\n",
    "     * - all examples inherit options from LLAMA_EXAMPLE_COMMON\n",
    "     * - if LLAMA_EXAMPLE_* is set (other than COMMON), we only show the option in the corresponding example\n",
    "     * - if both {LLAMA_EXAMPLE_COMMON, LLAMA_EXAMPLE_*,} are set, we will prioritize the LLAMA_EXAMPLE_* matching current example\n",
    "     */\n",
    "    auto add_opt = [&](common_arg arg) {\n",
    "        if ((arg.in_example(ex) || arg.in_example(LLAMA_EXAMPLE_COMMON)) && !arg.is_exclude(ex)) {\n",
    "            ctx_arg.options.push_back(std::move(arg));\n",
    "        }\n",
    "    };\n",
    "    common_arg & common_arg::set_examples(std::initializer_list<enum llama_example> examples) {\n",
    "        this->examples = std::move(examples);\n",
    "        return *this;\n",
    "    }\n",
    "    common_arg & common_arg::set_excludes(std::initializer_list<enum llama_example> excludes) {\n",
    "        this->excludes = std::move(excludes);\n",
    "        return *this;\n",
    "    }\n",
    "    common_arg & common_arg::set_env(const char * env) {\n",
    "        help = help + \"\\n(env: \" + env + \")\";\n",
    "        this->env = env;\n",
    "        return *this;\n",
    "    }\n",
    "    common_arg & common_arg::set_sparam() {\n",
    "        is_sparam = true;\n",
    "        return *this;\n",
    "    }\n",
    "}\n",
    "\n",
    "void ggml_backend_load_all() {\n",
    "    --> ggml_backend_load_all_from_path(nullptr);\n",
    "}\n",
    "\n",
    "void ggml_backend_load_all_from_path(const char * dir_path) {\n",
    "    --> ggml_backend_load_best(\"cuda\", silent, dir_path);\n",
    "    --> ggml_backend_load_best(\"cpu\", silent, dir_path);\n",
    "    // check the environment variable GGML_BACKEND_PATH to load an out-of-tree backend\n",
    "    const char * backend_path = std::getenv(\"GGML_BACKEND_PATH\");\n",
    "    ggml_backend_load(backend_path); //if (backend_path)\n",
    "        --> return get_reg().load_backend(path, false);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6765b55",
   "metadata": {},
   "source": [
    "[`ggml_backend_load_best`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend-reg.cpp#L493)\n",
    "\n",
    "```c++\n",
    "static ggml_backend_reg_t ggml_backend_load_best(const char * name, bool silent, const char * user_search_path) {\n",
    "    // enumerate all the files that match [lib]ggml-name-*.[so|dll] in the search paths\n",
    "    const fs::path name_path = fs::u8path(name); //filesystem::path \"cuda\"\n",
    "    const fs::path file_prefix = backend_filename_prefix().native() + name_path.native() + fs::u8path(\"-\").native(); // filesystem::path \"libggml-cuda-\"\n",
    "    const fs::path file_extension = backend_filename_extension(); // filesystem::path \".so\"\n",
    "    std::vector<fs::path> search_paths;\n",
    "    // default search paths: executable directory, current directory\n",
    "    search_paths.push_back(get_executable_path());\n",
    "    //[0] = filesystem::path \"/mnt/nfsdir_client/zhouyingkun/llama.cpp/cuda_debug/bin/\"\n",
    "    search_paths.push_back(fs::current_path());\n",
    "    fs::path filename = backend_filename_prefix().native() + name_path.native() + backend_filename_extension().native();\n",
    "    //filesystem::path \"libggml-cuda.so\"\n",
    "    fs::path path = search_path / filename;\n",
    "    --> return get_reg().load_backend(path, silent);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde4573",
   "metadata": {},
   "source": [
    "[`ggml_backend_registry::ggml_backend_registry`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend-reg.cpp#L167)\n",
    "\n",
    "```c++\n",
    "get_reg();\n",
    "----------\n",
    "static ggml_backend_registry & get_reg() {\n",
    "    --> static ggml_backend_registry reg;\n",
    "    return reg;\n",
    "}\n",
    "\n",
    "ggml_backend_registry::ggml_backend_registry() {\n",
    "#ifdef GGML_USE_CUDA\n",
    "        register_backend(ggml_backend_cuda_reg());\n",
    "#endif\n",
    "#ifdef GGML_USE_CPU\n",
    "        register_backend(ggml_backend_cpu_reg());\n",
    "#endif\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c63cb",
   "metadata": {},
   "source": [
    "### [`ggml_backend_cuda_reg`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L3504)\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_backend_reg/ggml_backend_reg_t</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_reg {\n",
    "    int api_version; // initialize to GGML_BACKEND_API_VERSION\n",
    "    struct ggml_backend_reg_i iface;\n",
    "    void * context;\n",
    "};\n",
    "\n",
    "typedef struct ggml_backend_reg * ggml_backend_reg_t;\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_backend_device/ggml_backend_dev_t</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_device {\n",
    "    struct ggml_backend_device_i iface;\n",
    "    ggml_backend_reg_t reg;\n",
    "    void * context;\n",
    "};\n",
    "\n",
    "typedef struct ggml_backend_device * ggml_backend_dev_t;\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_backend_cuda_reg_context</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_cuda_reg_context {\n",
    "    std::vector<ggml_backend_dev_t> devices;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_backend_cuda_device_context</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_cuda_device_context {\n",
    "    int device;\n",
    "    std::string name;\n",
    "    std::string description;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "static ggml_backend_reg reg;\n",
    "static bool initialized = false;\n",
    "{\n",
    "    static std::mutex mutex;\n",
    "    std::lock_guard<std::mutex> lock(mutex);\n",
    "    if (!initialized) {\n",
    "        ggml_backend_cuda_reg_context * ctx = new ggml_backend_cuda_reg_context;\n",
    "\n",
    "        for (int i = 0; i < ggml_cuda_info().device_count; i++) {\n",
    "                ---> return static ggml_cuda_device_info info = ggml_cuda_init();\n",
    "            ggml_backend_cuda_device_context * dev_ctx = new ggml_backend_cuda_device_context;\n",
    "            dev_ctx->device = i;\n",
    "            // CUDA0\n",
    "            dev_ctx->name = GGML_CUDA_NAME + std::to_string(i); //#define GGML_CUDA_NAME \"CUDA\"\n",
    "\n",
    "            ggml_cuda_set_device(i);\n",
    "            cudaDeviceProp prop;\n",
    "            CUDA_CHECK(cudaGetDeviceProperties(&prop, i));\n",
    "            dev_ctx->description = prop.name; //\n",
    "\n",
    "            ggml_backend_dev_t dev = new ggml_backend_device {\n",
    "                /* .iface   = */ ggml_backend_cuda_device_interface,\n",
    "                /* .reg     = */ &reg,\n",
    "                /* .context = */ dev_ctx\n",
    "            };\n",
    "            ctx->devices.push_back(dev);\n",
    "        }\n",
    "\n",
    "        reg = ggml_backend_reg {\n",
    "            /* .api_version = */ GGML_BACKEND_API_VERSION,\n",
    "            /* .iface       = */ ggml_backend_cuda_reg_interface,\n",
    "            /* .context     = */ ctx\n",
    "        };\n",
    "    }\n",
    "\n",
    "    initialized = true;\n",
    "}\n",
    "return &reg;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac034ff",
   "metadata": {},
   "source": [
    "### [`ggml_backend_cpu_reg`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.cpp#L658)\n",
    "\n",
    "```c++\n",
    "// init CPU feature detection\n",
    "--> ggml_cpu_init();\n",
    "\n",
    "static struct ggml_backend_reg ggml_backend_cpu_reg = {\n",
    "    /* .api_version = */ GGML_BACKEND_API_VERSION,\n",
    "    /* .iface       = */ ggml_backend_cpu_reg_i,\n",
    "    /* .context     = */ NULL,\n",
    "};\n",
    "\n",
    "return &ggml_backend_cpu_reg;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0901307",
   "metadata": {},
   "source": [
    "#### [`ggml_cpu_init`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.c#L3240)\n",
    "\n",
    "<details>\n",
    "<summary>FP32_TO_FP16/FP16_TO_FP32</summary>\n",
    "\n",
    "```c++\n",
    "// On ARM NEON, it's quicker to directly convert x -> x instead of calling into ggml_lookup_fp16_to_fp32,\n",
    "// so we define GGML_FP16_TO_FP32 and GGML_FP32_TO_FP16 elsewhere for NEON.\n",
    "// This is also true for POWER9.\n",
    "#if !defined(GGML_FP16_TO_FP32)\n",
    "inline static float ggml_lookup_fp16_to_fp32(ggml_fp16_t f) {\n",
    "    uint16_t s;\n",
    "    memcpy(&s, &f, sizeof(uint16_t));\n",
    "    return ggml_table_f32_f16[s];\n",
    "}\n",
    "\n",
    "#define GGML_FP16_TO_FP32(x) ggml_lookup_fp16_to_fp32(x)\n",
    "#endif\n",
    "\n",
    "#if !defined(GGML_FP32_TO_FP16)\n",
    "#define GGML_FP32_TO_FP16(x) GGML_COMPUTE_FP32_TO_FP16(x)\n",
    "#endif\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "// needed to initialize f16 tables\n",
    "{\n",
    "    struct ggml_init_params params = { 0, NULL, false };\n",
    "    struct ggml_context * ctx = ggml_init(params);\n",
    "    ---> {\n",
    "        static bool is_first_call = true;\n",
    "        ggml_critical_section_start();\n",
    "        if (is_first_call) {\n",
    "            // initialize time system (required on Windows)\n",
    "            ggml_time_init();\n",
    "\n",
    "            for (int i = 0; i < (1 << 16); ++i) {\n",
    "                union {\n",
    "                    uint16_t u16;\n",
    "                    ggml_fp16_t fp16;\n",
    "                } u = {i};\n",
    "                ggml_table_f32_f16[i] = GGML_COMPUTE_FP16_TO_FP32(u.fp16);\n",
    "            }\n",
    "\n",
    "            is_first_call = false;\n",
    "        }\n",
    "        ggml_critical_section_end();\n",
    "    }\n",
    "    ggml_free(ctx);\n",
    "}\n",
    "ggml_critical_section_start();\n",
    "static bool is_first_call = true;\n",
    "if (is_first_call) {\n",
    "    // initialize GELU, Quick GELU, SILU and EXP F32 tables\n",
    "    {\n",
    "        const uint64_t t_start = ggml_time_us(); UNUSED(t_start);\n",
    "        for (int i = 0; i < (1 << 16); ++i) {\n",
    "            union {\n",
    "                uint16_t u16;\n",
    "                ggml_fp16_t fp16;\n",
    "            } u = {i};\n",
    "            float f = GGML_FP16_TO_FP32(u.fp16);\n",
    "            ggml_table_gelu_f16[i] = GGML_FP32_TO_FP16(ggml_gelu_f32(f));\n",
    "            ggml_table_gelu_quick_f16[i] = GGML_FP32_TO_FP16(ggml_gelu_quick_f32(f));\n",
    "        }\n",
    "        const uint64_t t_end = ggml_time_us(); UNUSED(t_end);\n",
    "        GGML_PRINT_DEBUG(\"%s: GELU, Quick GELU, SILU and EXP tables initialized in %f ms\\n\", __func__, (t_end - t_start)/1000.0);\n",
    "#ifdef GGML_USE_OPENMP\n",
    "        //if (!getenv(\"OMP_WAIT_POLICY\")) {\n",
    "        //    // set the wait policy to active, so that OpenMP threads don't sleep\n",
    "        //    putenv(\"OMP_WAIT_POLICY=active\");\n",
    "        //}\n",
    "        // 有意思\n",
    "        if (!getenv(\"KMP_BLOCKTIME\")) {\n",
    "            // set the time to wait before sleeping a thread\n",
    "            // this is less aggressive than setting the wait policy to active, but should achieve similar results in most cases\n",
    "            putenv(\"KMP_BLOCKTIME=200\"); // 200ms\n",
    "        }\n",
    "#endif\n",
    "    }\n",
    "\n",
    "#if defined(__ARM_ARCH)\n",
    "    ggml_init_arm_arch_features();\n",
    "#endif\n",
    "\n",
    "    is_first_call = false;\n",
    "}\n",
    "ggml_critical_section_end();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c45c2f6",
   "metadata": {},
   "source": [
    "[`ggml_backend_cpu_reg_get_device`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.cpp#L488)\n",
    "\n",
    "```c++\n",
    "static ggml_backend_dev_t ggml_backend_cpu_reg_get_device(ggml_backend_reg_t reg, size_t index) {\n",
    "    GGML_ASSERT(index == 0);\n",
    "\n",
    "    static ggml_backend_cpu_device_context ctx;\n",
    "    static ggml_backend_device ggml_backend_cpu_device = {\n",
    "        /* .iface   = */ ggml_backend_cpu_device_i,\n",
    "        /* .reg     = */ reg,\n",
    "        /* .context = */ &ctx,\n",
    "    };\n",
    "\n",
    "    return &ggml_backend_cpu_device;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7cc49d",
   "metadata": {},
   "source": [
    "[`ggml_backend_registry::register_backend`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend-reg.cpp#L210)\n",
    "\n",
    "```c++\n",
    " void register_backend(ggml_backend_reg_t reg, dl_handle_ptr handle = nullptr) {\n",
    "    if (!reg) {\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    backends.push_back({ reg, std::move(handle) });\n",
    "    for (size_t i = 0; i < ggml_backend_reg_dev_count(reg); i++) {\n",
    "        --> register_device(ggml_backend_reg_dev_get(reg, i));\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e910d",
   "metadata": {},
   "source": [
    "[`ggml_backend_registry::register_device`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend-reg.cpp#L210)\n",
    "\n",
    "```c++\n",
    " void register_device(ggml_backend_dev_t device) {\n",
    "    devices.push_back(device);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c7d2ab",
   "metadata": {},
   "source": [
    "## [`common_params_parse_ex`](https://github.com/ggml-org/llama.cpp/blob/master/common/arg.cpp#L852)\n",
    "\n",
    "这个函数是为了处理具体的example用的\n",
    "\n",
    "```c++\n",
    "common_params_parse_ex(argc, argv, ctx_arg);\n",
    "----------\n",
    "static bool common_params_parse_ex(int argc, char ** argv, common_params_context & ctx_arg) {\n",
    "    // handle environment variables\n",
    "    // handle command line arguments\n",
    "    // handle model and download\n",
    "    --> common_chat_verify_template;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48b2ed-fc26-4326-b2a8-aaecfa12d61a",
   "metadata": {},
   "source": [
    "# [`common_init_from_params`](https://github.com/ggml-org/llama.cpp/blob/master/common/common.cpp#L902)\n",
    "\n",
    "```c++\n",
    "common_init_result llama_init = common_init_from_params(params);\n",
    "----------\n",
    "struct common_init_result common_init_from_params(common_params & params) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cbab3d-c6dc-4363-b124-d687c8808a0a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct common_init_result</summary>\n",
    "\n",
    "```c++\n",
    "// note: defines object's lifetime\n",
    "struct common_init_result {\n",
    "    llama_model_ptr   model;\n",
    "    llama_context_ptr context;\n",
    "\n",
    "    std::vector<llama_adapter_lora_ptr> lora;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "common_init_result iparams;\n",
    "\n",
    "auto mparams = common_model_params_to_llama(params);\n",
    "--> llama_model_load_from_file --> llama_model_load_from_file_impl --> llama_model_load --> llama_model * model;\n",
    "auto cparams = common_context_params_to_llama(params);\n",
    "--> llama_init_from_model --> llama_context::llama_context --> llama_context * lctx;\n",
    "llama_set_warmup(lctx, warmup=true); --> cparams.warmup = warmup;\n",
    "std::vector<llama_token> tmp;\n",
    "llama_token bos = llama_vocab_bos(vocab);\n",
    "llama_token eos = llama_vocab_eos(vocab);\n",
    "tmp.push_back(bos);\n",
    "tmp.push_back(eos);\n",
    "--> llama_decode --> llama_context::decode;\n",
    "llama_memory_clear(llama_get_memory(lctx), true);\n",
    "    --> lctx->memory->clear(data);\n",
    "        --> cells.reset();\n",
    "        --> ggml_backend_buffer_clear(buf.get(), 0);\n",
    "// add the evaluation to the stats\n",
    "llama_synchronize(lctx); --> lctx->synchronize; --> llama_context::synchronize; --> ggml_backend_sched_synchronize;\n",
    "llama_perf_context_reset(lctx);\n",
    "llama_set_warmup(lctx, false);\n",
    "\n",
    "iparams.model.reset(model);\n",
    "iparams.context.reset(lctx);\n",
    "return iparams;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1fd37-e078-4d43-bbe4-ef923887305b",
   "metadata": {},
   "source": [
    "## [`llama_model_load`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama.cpp#L87)\n",
    "\n",
    "```c++\n",
    "llama_model * model = llama_model_load_from_file(params.model.path.c_str(), mparams);\n",
    "----------\n",
    "struct llama_model * llama_model_load_from_file(\n",
    "    const char * path_model,\n",
    "    struct llama_model_params params) {\n",
    "    std::vector<std::string> splits = {};\n",
    "    --> return llama_model_load_from_file_impl(path_model, splits, params);\n",
    "}\n",
    "\n",
    "    enum ggml_backend_dev_type {\n",
    "        // CPU device using system memory\n",
    "        GGML_BACKEND_DEVICE_TYPE_CPU,\n",
    "        // GPU device using dedicated memory ？？？好奇统一内存的GPU用哪个\n",
    "        GGML_BACKEND_DEVICE_TYPE_GPU,\n",
    "        // accelerator devices intended to be used together with the CPU backend (e.g. BLAS or AMX)\n",
    "        GGML_BACKEND_DEVICE_TYPE_ACCEL\n",
    "    };\n",
    "\n",
    "static struct llama_model * llama_model_load_from_file_impl(\n",
    "        const std::string & path_model,\n",
    "        std::vector<std::string> & splits,\n",
    "        struct llama_model_params params) {\n",
    "    llama_model * model = new llama_model(params);\n",
    "    ggml_backend_dev_t dev = ggml_backend_dev_get(i);\n",
    "    switch (ggml_backend_dev_type(dev)) {\n",
    "        case GGML_BACKEND_DEVICE_TYPE_GPU:\n",
    "            model->devices.push_back(dev); // 这一步很隐蔽啊！\n",
    "            size_t free, total; // NOLINT\n",
    "            ggml_backend_dev_memory(dev, &free, &total);\n",
    "    }\n",
    "    --> const int status = llama_model_load(path_model, splits, *model, params);\n",
    "    return model;\n",
    "}\n",
    "\n",
    "// Returns 0 on success, -1 on error, and -2 on cancellation via llama_progress_callback\n",
    "static int llama_model_load(const std::string & fname, std::vector<std::string> & splits, llama_model & model, llama_model_params & params) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d2a10-42f8-4e7d-b13b-70e2ada15840",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct llama_model</summary>\n",
    "\n",
    "```c++\n",
    "struct llama_model {\n",
    "    llm_type type = LLM_TYPE_UNKNOWN;\n",
    "    llm_arch arch = LLM_ARCH_UNKNOWN;\n",
    "\n",
    "    std::string name = \"n/a\";\n",
    "\n",
    "    llama_hparams hparams = {};\n",
    "    llama_vocab   vocab;\n",
    "\n",
    "    struct ggml_tensor * tok_embd   = nullptr;\n",
    "    struct ggml_tensor * type_embd  = nullptr;\n",
    "    struct ggml_tensor * pos_embd   = nullptr;\n",
    "    struct ggml_tensor * tok_norm   = nullptr;\n",
    "    struct ggml_tensor * tok_norm_b = nullptr;\n",
    "\n",
    "    struct ggml_tensor * output_norm     = nullptr;\n",
    "    struct ggml_tensor * output_norm_b   = nullptr;\n",
    "    struct ggml_tensor * output          = nullptr;\n",
    "    struct ggml_tensor * output_b        = nullptr;\n",
    "    struct ggml_tensor * output_norm_enc = nullptr;\n",
    "\n",
    "    std::vector<llama_layer> layers;\n",
    "\n",
    "    llama_model_params params;\n",
    "\n",
    "    // gguf metadata\n",
    "    std::unordered_map<std::string, std::string> gguf_kv;\n",
    "\n",
    "    // list of devices used in this model\n",
    "    std::vector<ggml_backend_dev_t> devices;\n",
    "\n",
    "    // for quantize-stats only\n",
    "    std::vector<std::pair<std::string, struct ggml_tensor *>> tensors_by_name;\n",
    "\n",
    "    // note: can mutate `cparams`\n",
    "    // TODO: move this to new llm_arch_model_i interface\n",
    "    llama_memory_i * create_memory(const llama_memory_params & params, llama_cparams & cparams) const;\n",
    "\n",
    "    // TODO: move this to new llm_arch_model_i interface\n",
    "    llm_graph_result_ptr build_graph(\n",
    "            const llm_graph_params & params,\n",
    "                       ggml_cgraph * gf,\n",
    "                    llm_graph_type   type) const;\n",
    "\n",
    "private:\n",
    "    struct impl;\n",
    "    std::unique_ptr<impl> pimpl;\n",
    "};\n",
    "\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n",
    "```c++\n",
    "--> llama_model_loader::llama_model_loader --> llama_model_loader ml;\n",
    "ml.print_info();\n",
    "model.load_arch(ml);\n",
    "--> llama_model::load_hparams;\n",
    "--> llama_model::load_vocab --> llama_vocab::load --> llama_vocab::impl::load;\n",
    "model.load_stats(ml);\n",
    "model.print_info();\n",
    "--> llama_model::load_tensors;\n",
    "return 0;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b35917-d3da-45b2-996e-cf9b9e26cfb9",
   "metadata": {},
   "source": [
    "### [`llama_model_loader::llama_model_loader`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L468)\n",
    "\n",
    "> 这个函数的功效在于读取gguf文件里面的各种信息，并且构建`tensor_meta`, 类型同样是`ggml_tensor *`，但是不被用于运算，用来在后面的`llama_model::load_tensors`函数中，构建起llm_model的类中的属性，比如`tok_embd`, `layers`, `output_norm` 和 `output`\n",
    "\n",
    "> 后续的用法是这样的：`ggml_tensor * t_meta = ml.get_tensor_meta(tn.str().c_str());` 然后权重的tensor类似于是长这样的\n",
    "\n",
    "```log\n",
    ">>> p *cur\n",
    "$189 = {\n",
    "  type = GGML_TYPE_Q2_K,\n",
    "  buffer = 0x0, # 注意到此时buffer地址还是0的未初始化值\n",
    "  ne = {[0] = 4096, [1] = 128256, [2] = 1, [3] = 1},\n",
    "  nb = {[0] = 84, [1] = 1344, [2] = 172376064, [3] = 172376064},\n",
    "  op = GGML_OP_NONE,\n",
    "  op_params = {[0] = 0 <repeats 16 times>},\n",
    "  flags = 0,\n",
    "  src = {[0] = 0x0, [1] = 0x0, [2] = 0x0, [3] = 0x0, [4] = 0x0, [5] = 0x0, [6] = 0x0, [7] = 0x0, [8] = 0x0, [9] = 0x0},\n",
    "  view_src = 0x0,\n",
    "  view_offs = 0,\n",
    "  data = 0x0,\n",
    "  name = '\\000' <repeats 63 times>,\n",
    "  extra = 0x0,\n",
    "  padding = \"\\000\\000\\000\\000\\000\\000\\000\"\n",
    "}\n",
    "```\n",
    "\n",
    "```c++\n",
    "llama_model_loader ml(fname, splits, params.use_mmap, params.check_tensors, params.kv_overrides, params.tensor_buft_overrides);\n",
    "----------\n",
    "llama_model_loader::llama_model_loader(\n",
    "        const std::string & fname,\n",
    "        std::vector<std::string> & splits,\n",
    "        bool use_mmap,\n",
    "        bool check_tensors,\n",
    "        const llama_model_kv_override * param_overrides_p,\n",
    "        const llama_model_tensor_buft_override * param_tensor_buft_overrides_p) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf2a36-2b0c-4ce8-a29a-11121839b801",
   "metadata": {},
   "source": [
    "```c++\n",
    "// Load the main GGUF\n",
    "struct ggml_context * ctx = NULL;\n",
    "struct gguf_init_params params = {\n",
    "    /*.no_alloc = */ true,\n",
    "    /*.ctx      = */ &ctx,\n",
    "};\n",
    "\n",
    "llama_model_loader:: gguf_context_ptr meta;\n",
    "--> gguf_init_from_file --> gguf_init_from_file_impl --> meta;\n",
    "\n",
    "files.emplace_back(new llama_file(fname.c_str(), \"rb\"));\n",
    "contexts.emplace_back(ctx);\n",
    "\n",
    "for (ggml_tensor * cur = ggml_get_first_tensor(ctx); cur; cur = ggml_get_next_tensor(ctx, cur)) {\n",
    "    std::string tensor_name = std::string(cur->name);\n",
    "    n_elements += ggml_nelements(cur);\n",
    "    n_bytes    += ggml_nbytes(cur);\n",
    "    ---> weights_map.emplace(tensor_name, llama_tensor_weight(files.back().get(), 0, meta.get(), cur));\n",
    "}\n",
    "\n",
    "n_kv      = gguf_get_n_kv(meta.get());\n",
    "n_tensors = weights_map.size();\n",
    "\n",
    "fver = (enum llama_fver) gguf_get_version(meta.get());\n",
    "\n",
    "this->use_mmap = use_mmap;\n",
    "this->check_tensors = check_tensors;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad88911-3e43-461f-bcb1-8599dccb1fe6",
   "metadata": {},
   "source": [
    "#### [`gguf_init_from_file_impl`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/gguf.cpp#L319)\n",
    "\n",
    "```c++\n",
    "meta.reset(gguf_init_from_file(fname.c_str(), params));\n",
    "\n",
    "struct gguf_context * gguf_init_from_file(const char * fname, struct gguf_init_params params) {\n",
    "    FILE * file = ggml_fopen(fname, \"rb\");\n",
    "    --> struct gguf_context * result = gguf_init_from_file_impl(file, params);\n",
    "    fclose(file);\n",
    "    return result;\n",
    "}\n",
    "\n",
    "struct gguf_context * gguf_init_from_file_impl(FILE * file, struct gguf_init_params params) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed93c7-14ac-4787-82b3-d6f8c7ff1fc2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct gguf_context</summary>\n",
    "\n",
    "```c++\n",
    "struct gguf_context {\n",
    "    uint32_t version = GGUF_VERSION;\n",
    "\n",
    "    std::vector<struct gguf_kv> kv;\n",
    "    std::vector<struct gguf_tensor_info> info;\n",
    "\n",
    "    size_t alignment = GGUF_DEFAULT_ALIGNMENT;\n",
    "    size_t offset    = 0; // offset of `data` from beginning of file\n",
    "    size_t size      = 0; // size of `data` in bytes\n",
    "\n",
    "    void * data = nullptr;\n",
    "};\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n",
    "```c++\n",
    "const struct gguf_reader gr(file);\n",
    "struct gguf_context * ctx = new gguf_context;\n",
    "// file magic\n",
    "gr.read(magic, 4);\n",
    "// header\n",
    "gr.read(ctx->version);\n",
    "gr.read(n_tensors);\n",
    "gr.read(n_kv);\n",
    "// KV pairs\n",
    "for (int64_t i = 0; ok && i < n_kv; ++i)\n",
    "    gr.read(key);\n",
    "    gr.read(type);\n",
    "    is_array = true; gr.read(type); gr.read(n);\n",
    "    gguf_read_emplace_helper<xxx>    (gr, ctx->kv, key, is_array, n);\n",
    "const int alignment_idx = gguf_find_key(ctx, GGUF_KEY_GENERAL_ALIGNMENT);\n",
    "ctx->alignment = alignment_idx == -1 ? GGUF_DEFAULT_ALIGNMENT : gguf_get_val_u32(ctx, alignment_idx);\n",
    "\n",
    "// read the tensor info\n",
    "for (int64_t i = 0; ok && i < n_tensors; ++i)\n",
    "    struct gguf_tensor_info info;\n",
    "    std::string name; gr.read(name); ggml_set_name(&info.t, name.c_str()); // tensor name\n",
    "    uint32_t n_dims = -1; gr.read(n_dims); gr.read(info.t.ne); // tensor shape\n",
    "    gr.read(info.t.type); // tensor type\n",
    "    // calculate byte offsets given the tensor shape and type\n",
    "    const size_t  type_size = ggml_type_size(info.t.type);\n",
    "    const int64_t blck_size = ggml_blck_size(info.t.type);\n",
    "    info.t.nb;\n",
    "    gr.read(info.offset); // tensor data offset within buffer\n",
    "    ctx->info.push_back(info);\n",
    "\n",
    "// store the current file offset - this is where the data section starts\n",
    "ctx->offset = ftell(file);\n",
    "\n",
    "// compute the total size of the data section, taking into account the alignment\n",
    "ctx->size = 0;\n",
    "for (size_t i = 0; i < ctx->info.size(); ++i)\n",
    "    const gguf_tensor_info & ti = ctx->info[i];\n",
    "    ctx->size += GGML_PAD(ggml_nbytes(&ti.t), ctx->alignment);\n",
    "\n",
    "\n",
    "// load the tensor data only if requested\n",
    "// compute the exact size needed for the new ggml_context\n",
    "const size_t mem_size = n_tensors * ggml_tensor_overhead();\n",
    "\n",
    "struct ggml_init_params pdata = {\n",
    "    /*mem_size   =*/ mem_size,\n",
    "    /*mem_buffer =*/ nullptr,\n",
    "    /*no_alloc   =*/ params.no_alloc,\n",
    "};\n",
    "--> ggml_init --> *params.ctx;\n",
    "struct ggml_context * ctx_data = *params.ctx;\n",
    "\n",
    "// create the tensors\n",
    "for (size_t i = 0; i < ctx->info.size(); ++i) {\n",
    "    const struct gguf_tensor_info & info = ctx->info[i];\n",
    "    --> ggml_new_tensor --> ggml_new_tensor_impl --> struct ggml_tensor * cur;\n",
    "    ggml_set_name(cur, info.t.name);\n",
    "}\n",
    "return ctx;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186d1c0b-b609-4be9-8132-b8a912a71e7f",
   "metadata": {},
   "source": [
    "##### [`ggml_init`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L1414)\n",
    "\n",
    "```c++\n",
    "*params.ctx = ggml_init(pdata);\n",
    "----------\n",
    "struct ggml_context * ggml_init(struct ggml_init_params params) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddffe5c-6aea-4666-b676-87a6dd835057",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_context</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_context {\n",
    "    size_t mem_size;\n",
    "    void * mem_buffer;\n",
    "    bool   mem_buffer_owned;\n",
    "    bool   no_alloc;\n",
    "\n",
    "    int    n_objects;\n",
    "\n",
    "    struct ggml_object * objects_begin;\n",
    "    struct ggml_object * objects_end;\n",
    "};\n",
    "\n",
    "size_t ggml_tensor_overhead(void) {\n",
    "    return GGML_OBJECT_SIZE + GGML_TENSOR_SIZE;\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "struct ggml_context * ctx = GGML_MALLOC(sizeof(struct ggml_context));\n",
    "\n",
    "// allow to call ggml_init with 0 size\n",
    "if (params.mem_size == 0) {\n",
    "    params.mem_size = GGML_MEM_ALIGN;\n",
    "}\n",
    "\n",
    "const size_t mem_size = params.mem_buffer ? params.mem_size : GGML_PAD(params.mem_size, GGML_MEM_ALIGN);\n",
    "\n",
    "*ctx = (struct ggml_context) {\n",
    "    /*.mem_size           =*/ mem_size,\n",
    "    /*.mem_buffer         =*/ params.mem_buffer ? params.mem_buffer : ggml_aligned_malloc(mem_size),\n",
    "    /*.mem_buffer_owned   =*/ params.mem_buffer ? false : true,\n",
    "    /*.no_alloc           =*/ params.no_alloc,\n",
    "    /*.n_objects          =*/ 0,\n",
    "    /*.objects_begin      =*/ NULL,\n",
    "    /*.objects_end        =*/ NULL,\n",
    "};\n",
    "\n",
    "return ctx;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6458a4-f1d0-4a4f-8ef3-e8ba3d78e344",
   "metadata": {},
   "source": [
    "##### [`ggml_new_tensor_impl`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L1565)\n",
    "\n",
    "```c++\n",
    "struct ggml_tensor * cur = ggml_new_tensor(ctx_data, info.t.type, GGML_MAX_DIMS, info.t.ne);\n",
    "\n",
    "struct ggml_tensor * ggml_new_tensor(\n",
    "        struct ggml_context * ctx,\n",
    "        enum   ggml_type      type,\n",
    "        int                   n_dims,\n",
    "        const int64_t       * ne) {\n",
    "    --> return ggml_new_tensor_impl(ctx, type, n_dims, ne, NULL, 0);\n",
    "}\n",
    "\n",
    "static struct ggml_tensor * ggml_new_tensor_impl(\n",
    "        struct ggml_context * ctx,\n",
    "        enum   ggml_type      type,\n",
    "        int                   n_dims,\n",
    "        const int64_t       * ne,\n",
    "        struct ggml_tensor  * view_src,\n",
    "        size_t                view_offs) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d12c4-9416-4598-8809-05b5ee817c3b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_tensor</summary>\n",
    "\n",
    "```c++\n",
    "// n-dimensional tensor\n",
    "struct ggml_tensor {\n",
    "    enum ggml_type type;\n",
    "\n",
    "    struct ggml_backend_buffer * buffer;\n",
    "\n",
    "    int64_t ne[GGML_MAX_DIMS]; // number of elements\n",
    "    size_t  nb[GGML_MAX_DIMS]; // stride in bytes:\n",
    "                               // nb[0] = ggml_type_size(type)\n",
    "                               // nb[1] = nb[0]   * (ne[0] / ggml_blck_size(type)) + padding\n",
    "                               // nb[i] = nb[i-1] * ne[i-1]\n",
    "\n",
    "    // compute data\n",
    "    enum ggml_op op;\n",
    "\n",
    "    // op params - allocated as int32_t for alignment\n",
    "    int32_t op_params[GGML_MAX_OP_PARAMS / sizeof(int32_t)];\n",
    "\n",
    "    int32_t flags;\n",
    "\n",
    "    struct ggml_tensor * src[GGML_MAX_SRC];\n",
    "\n",
    "    // source tensor and offset for views\n",
    "    struct ggml_tensor * view_src;\n",
    "    size_t               view_offs;\n",
    "\n",
    "    void * data;\n",
    "\n",
    "    char name[GGML_MAX_NAME];\n",
    "\n",
    "    void * extra; // extra things e.g. for ggml-cuda.cu\n",
    "\n",
    "    char padding[8];\n",
    "};\n",
    "\n",
    "static const size_t GGML_TENSOR_SIZE = sizeof(struct ggml_tensor);\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "--> ggml_new_object --> obj_new;\n",
    "struct ggml_tensor * const result = (struct ggml_tensor *)((char *)ctx->mem_buffer + obj_new->offs);\n",
    "*result = (struct ggml_tensor) {\n",
    "    /*.type         =*/ type,\n",
    "    /*.buffer       =*/ NULL,\n",
    "    /*.ne           =*/ { 1, 1, 1, 1 },\n",
    "    /*.nb           =*/ { 0, 0, 0, 0 },\n",
    "    /*.op           =*/ GGML_OP_NONE,\n",
    "    /*.op_params    =*/ { 0 },\n",
    "    /*.flags        =*/ 0,\n",
    "    /*.src          =*/ { NULL },\n",
    "    /*.view_src     =*/ view_src,\n",
    "    /*.view_offs    =*/ view_offs,\n",
    "    /*.data         =*/ view_src != NULL ? view_src->data + view_offs : NULL,\n",
    "    /*.name         =*/ { 0 },\n",
    "    /*.extra        =*/ NULL,\n",
    "    /*.padding      =*/ { 0 },\n",
    "};\n",
    "\n",
    "result->ne;\n",
    "result->nb;\n",
    "ctx->n_objects++;\n",
    "return result;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f77f5-8623-49cc-8aa9-d6cc788469af",
   "metadata": {},
   "source": [
    "[`ggml_new_object`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L1519)\n",
    "\n",
    "```c++\n",
    "struct ggml_object * const obj_new = ggml_new_object(ctx, GGML_OBJECT_TYPE_TENSOR, GGML_TENSOR_SIZE);\n",
    "\n",
    "static struct ggml_object * ggml_new_object(struct ggml_context * ctx, enum ggml_object_type type, size_t size) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a747dcb-8a63-4695-8d73-68a3009ab9d1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_object</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_object {\n",
    "    size_t offs;\n",
    "    size_t size;\n",
    "\n",
    "    struct ggml_object * next;\n",
    "\n",
    "    enum ggml_object_type type;\n",
    "\n",
    "    char padding[4];\n",
    "};\n",
    "\n",
    "static const size_t GGML_OBJECT_SIZE = sizeof(struct ggml_object);\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "// always insert objects at the end of the context's memory pool\n",
    "struct ggml_object * obj_cur = ctx->objects_end;\n",
    "\n",
    "const size_t cur_offs = obj_cur == NULL ? 0 : obj_cur->offs;\n",
    "const size_t cur_size = obj_cur == NULL ? 0 : obj_cur->size;\n",
    "const size_t cur_end  = cur_offs + cur_size;\n",
    "\n",
    "// align to GGML_MEM_ALIGN\n",
    "size_t size_needed = GGML_PAD(size, GGML_MEM_ALIGN);\n",
    "\n",
    "char * const mem_buffer = ctx->mem_buffer;\n",
    "struct ggml_object * const obj_new = (struct ggml_object *)(mem_buffer + cur_end);\n",
    "\n",
    "*obj_new = (struct ggml_object) {\n",
    "    .offs = cur_end + GGML_OBJECT_SIZE,\n",
    "    .size = size_needed,\n",
    "    .next = NULL,\n",
    "    .type = type,\n",
    "};\n",
    "\n",
    "GGML_ASSERT_ALIGNED(mem_buffer + obj_new->offs);\n",
    "\n",
    "if (obj_cur != NULL) {\n",
    "    obj_cur->next = obj_new;\n",
    "} else {\n",
    "    // this is the first object in this context\n",
    "    ctx->objects_begin = obj_new;\n",
    "}\n",
    "\n",
    "ctx->objects_end = obj_new;\n",
    "return obj_new;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4cdb26-bd29-4915-96b6-9573765ce679",
   "metadata": {},
   "source": [
    "### [`llama_model::load_hparams`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#L425)\n",
    "\n",
    "```c++\n",
    "model.load_hparams(ml);\n",
    "----------\n",
    "void llama_model::load_hparams(llama_model_loader & ml) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0018b980-85ef-4e0f-a684-e88a10f1ae01",
   "metadata": {},
   "source": [
    "```c++\n",
    "const gguf_context * ctx = ml.meta.get();\n",
    "// get metadata as string\n",
    "// gguf metadata\n",
    "llama_model:: std::unordered_map<std::string, std::string> gguf_kv;\n",
    "gguf_kv.emplace(name, value);\n",
    "hparams.xxx = xxx; // via ml.get_key\n",
    "pimpl->n_bytes = ml.n_bytes;\n",
    "pimpl->desc_str = arch_name() + \" \" + type_name() + \" \" + ml.ftype_name();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1729941-5b75-45f6-9598-a0800fb9e442",
   "metadata": {},
   "source": [
    "### [`llama_vocab::impl::load`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-vocab.cpp#L1372)\n",
    "\n",
    "```c++\n",
    "model.load_vocab(ml);\n",
    "----------\n",
    "void llama_model::load_vocab(llama_model_loader & ml) {\n",
    "    const auto kv = LLM_KV(arch);\n",
    "    --> vocab.load(ml, kv);\n",
    "}\n",
    "\n",
    "void llama_vocab::load(llama_model_loader & ml, const LLM_KV & kv) {\n",
    "    --> pimpl->load(ml, kv);\n",
    "}\n",
    "\n",
    "void llama_vocab::impl::load(llama_model_loader & ml, const LLM_KV & kv) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f22f50-c727-4d33-8bac-95a2825460a2",
   "metadata": {},
   "source": [
    "```c++\n",
    "struct gguf_context * ctx = ml.meta.get();\n",
    "// determine vocab type\n",
    "ml.get_key(LLM_KV_TOKENIZER_MODEL, tokenizer_model);\n",
    "ml.get_key(LLM_KV_TOKENIZER_PRE,   tokenizer_pre, false);\n",
    "ml.get_key(LLM_KV_TOKENIZER_TOKEN_TYPE_COUNT, n_token_types, false);\n",
    "// for now, only BPE models have pre-tokenizers\n",
    "llama_vocab::impl::\n",
    "    std::unordered_map<std::string, llama_token> token_to_id;\n",
    "    std::vector<token_data> id_to_token;\n",
    "--> init_tokenizer(type); --> tokenizer = std::make_unique<llm_tokenizer_bpe>(vocab);\n",
    "// determine the newline token: LLaMA \"<0x0A>\" == 10 == '\\n', Falcon 193 == '\\n\n",
    "// special tokens\n",
    "    std::set<llama_token> special_eog_ids; // set of all tokens that cause \"end of generation\"\n",
    "// build special tokens cache\n",
    "    std::vector<llama_token> cache_special_tokens;\n",
    "// build token to piece cache\n",
    "    std::vector<std::string> cache_token_to_piece; // llama_token_to_piece(special = true);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c62852-51d9-4b64-9244-73cb6e2081d8",
   "metadata": {},
   "source": [
    "### [`llama_model::load_tensors`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#L1507)\n",
    "\n",
    "> 这个函数主要是进行后端的分配，然后把 `llama_model_loader` ctx 里面的tensor copy到 `llama_model` 类中；\n",
    "- 如果是CUDA后端，在 `alloc_tensor_range` 的时候给tensor分配buffer和data，然后在 `llama_model_loader::load_all_data` 上将权重传到GPU上；\n",
    "- 如果是CPU后端，在 `llama_model_loader::load_all_data` 的时候给tensor分配buffer和data；\n",
    "\n",
    "```c++\n",
    "model.load_tensors(ml);\n",
    "----------\n",
    "bool llama_model::load_tensors(llama_model_loader & ml) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3073523b-3048-4b55-bfb4-01af5ddc4195",
   "metadata": {},
   "source": [
    "```c++\n",
    "const auto & split_mode   = params.split_mode;\n",
    "const auto & n_gpu_layers = params.n_gpu_layers;\n",
    "const auto & use_mlock    = params.use_mlock;\n",
    "const auto & tensor_split = params.tensor_split;\n",
    "// build a list of buffer types for the CPU and GPU devices\n",
    "--> make_cpu_buft_list --> pimpl->cpu_buft_list;\n",
    "for (auto * dev : devices)\n",
    "    make_gpu_buft_list --> ggml_backend_cuda_buffer_type --> buft_list_t buft_list;\n",
    "    // add CPU buffer types as a fallback\n",
    "    buft_list.insert(buft_list.end(), pimpl->cpu_buft_list.begin(), pimpl->cpu_buft_list.end());\n",
    "    pimpl->gpu_buft_list.emplace(dev, std::move(buft_list));\n",
    "\n",
    "// calculate the split points\n",
    "std::vector<float> splits(n_devices());\n",
    "// sum and normalize the splits to get the split points\n",
    "ggml_backend_dev_t cpu_dev = ggml_backend_dev_by_type(GGML_BACKEND_DEVICE_TYPE_CPU);\n",
    "const int i_gpu_start = std::max((int) hparams.n_layer - n_gpu_layers, (int) 0);\n",
    "const int act_gpu_layers = devices.empty() ? 0 : std::min(n_gpu_layers, (int)n_layer + 1);\n",
    "auto get_layer_buft_list = [&](int il) -> llama_model::impl::layer_dev {\n",
    "    if (il < i_gpu_start || (il - i_gpu_start) >= act_gpu_layers) return {cpu_dev, &pimpl->cpu_buft_list};\n",
    "        const int layer_gpu = std::upper_bound(splits.begin(), splits.begin() + n_devices(), float(il - i_gpu_start)/act_gpu_layers) - splits.begin(); // 0\n",
    "        auto * dev = devices.at(layer_gpu);\n",
    "        return {dev, &pimpl->gpu_buft_list.at(dev)};\n",
    "}\n",
    "// assign the input layer, there is very little benefit to offloading the input layer, so always keep it on the CPU\n",
    "pimpl->dev_input = { cpu_dev, &pimpl->cpu_buft_list };\n",
    "// assign the repeating layers to the devices according to the splits\n",
    "pimpl->dev_layer.resize(n_layer);\n",
    "for (int il = 0; il < n_layer; ++il) pimpl->dev_layer[il] = get_layer_buft_list(il);\n",
    "// assign the output layer\n",
    "pimpl->dev_output = get_layer_buft_list(n_layer);\n",
    "// one ggml context per buffer type\n",
    "int max_n_tensors = ml.n_tensors;\n",
    "max_n_tensors += 1;         // duplicated output tensor\n",
    "max_n_tensors += n_layer*2; // duplicated rope freq tensors\n",
    "const size_t ctx_size = ggml_tensor_overhead()*max_n_tensors;\n",
    "// one ggml context per buffer type\n",
    "std::map<ggml_backend_buffer_type_t, ggml_context *> ctx_map;\n",
    "auto ctx_for_buft = [&](ggml_backend_buffer_type_t buft) -> ggml_context * {\n",
    "    auto it = ctx_map.find(buft);\n",
    "    if (it == ctx_map.end()) {\n",
    "        ggml_init_params params = {\n",
    "            /*.mem_size   =*/ ctx_size,\n",
    "            /*.mem_buffer =*/ NULL,\n",
    "            /*.no_alloc   =*/ true,\n",
    "        };\n",
    "        ggml_context * ctx = ggml_init(params);\n",
    "        ctx_map[buft] = ctx;\n",
    "        pimpl->ctxs.emplace_back(ctx);\n",
    "        return ctx;\n",
    "    }\n",
    "    return it->second;\n",
    "};\n",
    "// create tensors for the weights\n",
    "auto create_tensor = [&](const LLM_TN_IMPL & tn, const std::initializer_list<int64_t> & ne, int flags) -> ggml_tensor * {\n",
    "    ggml_tensor * t_meta = ml.get_tensor_meta(tn.str().c_str());\n",
    "    llm_tensor_info info = llm_tensor_info_for(tn_tensor);\n",
    "    // select the buffer type for this tensor\n",
    "    switch (info.layer) {}\n",
    "    --> select_weight_buft --> weight_buft_supported --> buft;\n",
    "    // avoid using a host buffer when using mmap 啊这，如果使用mmap就没法利用host buffer了！！！\n",
    "    auto * buft_dev = ggml_backend_buft_get_device(buft);\n",
    "    if (ml.use_mmap && buft_dev && buft == ggml_backend_dev_host_buffer_type(buft_dev))\n",
    "        // CPU 不符合 buft_dev 跳过了，GPU host buffer 会进入，GPU 因为host buffer == buft 所以也会跳过\n",
    "        auto * cpu_dev = ggml_backend_dev_by_type(GGML_BACKEND_DEVICE_TYPE_CPU);\n",
    "        buft = ggml_backend_dev_buffer_type(cpu_dev);\n",
    "    ggml_context * ctx = ctx_for_buft(buft);\n",
    "    --> return llama_model_loader::create_tensor;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd8350-9ed4-4b32-9a89-c73cd160f294",
   "metadata": {},
   "source": [
    "```c++\n",
    "layers.resize(n_layer);\n",
    "// TODO: move to a separate function\n",
    "const auto tn = LLM_TN(arch);\n",
    "\n",
    "tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, 0);\n",
    "\n",
    "// output\n",
    "output_norm = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd}, 0);\n",
    "output      = create_tensor(tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, TENSOR_NOT_REQUIRED);\n",
    "\n",
    "// if output is NULL, init from the input tok embed\n",
    "if (output == NULL)\n",
    "    output = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, TENSOR_DUPLICATED);\n",
    "\n",
    "for (int i = 0; i < n_layer; ++i)\n",
    "    auto & layer = layers[i];\n",
    "\n",
    "    layer.attn_norm = create_tensor(tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), {n_embd}, 0);\n",
    "\n",
    "    layer.wq = create_tensor(tn(LLM_TENSOR_ATTN_Q,   \"weight\", i), {n_embd, n_embd_head_k * n_head}, 0);\n",
    "    layer.wk = create_tensor(tn(LLM_TENSOR_ATTN_K,   \"weight\", i), {n_embd, n_embd_k_gqa}, 0);\n",
    "    layer.wv = create_tensor(tn(LLM_TENSOR_ATTN_V,   \"weight\", i), {n_embd, n_embd_v_gqa}, 0);\n",
    "    layer.wo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd_head_k * n_head, n_embd}, 0);\n",
    "\n",
    "    // optional bias tensors\n",
    "    layer.bq = create_tensor(tn(LLM_TENSOR_ATTN_Q,   \"bias\", i), {n_embd},     TENSOR_NOT_REQUIRED);\n",
    "    layer.bk = create_tensor(tn(LLM_TENSOR_ATTN_K,   \"bias\", i), {n_embd_gqa}, TENSOR_NOT_REQUIRED);\n",
    "    layer.bv = create_tensor(tn(LLM_TENSOR_ATTN_V,   \"bias\", i), {n_embd_gqa}, TENSOR_NOT_REQUIRED);\n",
    "    layer.bo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"bias\", i), {n_embd},     TENSOR_NOT_REQUIRED);\n",
    "\n",
    "    layer.ffn_norm = create_tensor(tn(LLM_TENSOR_FFN_NORM, \"weight\", i), {n_embd}, 0);\n",
    "\n",
    "    layer.ffn_gate = create_tensor(tn(LLM_TENSOR_FFN_GATE, \"weight\", i), {n_embd,   n_ff}, 0);\n",
    "    layer.ffn_down = create_tensor(tn(LLM_TENSOR_FFN_DOWN, \"weight\", i), {  n_ff, n_embd}, 0);\n",
    "    layer.ffn_up   = create_tensor(tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd,   n_ff}, 0);\n",
    "\n",
    "    // optional MLP bias\n",
    "    layer.ffn_gate_b = create_tensor(tn(LLM_TENSOR_FFN_GATE, \"bias\", i), {n_ff}, TENSOR_NOT_REQUIRED);\n",
    "    layer.ffn_down_b = create_tensor(tn(LLM_TENSOR_FFN_DOWN, \"bias\", i), {n_embd}, TENSOR_NOT_REQUIRED);\n",
    "    layer.ffn_up_b   = create_tensor(tn(LLM_TENSOR_FFN_UP,   \"bias\", i), {n_ff}, TENSOR_NOT_REQUIRED);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038a45e-ce5b-4c44-b79b-eb423abf15b6",
   "metadata": {},
   "source": [
    "```c++\n",
    "ml.done_getting_tensors();\n",
    "--> llama_model_loader::init_mappings --> ml.mappings;\n",
    "pimpl->mappings.reserve(ml.mappings.size());\n",
    "// create the backend buffers\n",
    "using llama_buf_map = std::unordered_map<uint32_t=0, ggml_backend_buffer_t>;\n",
    "std::vector<std::pair<ggml_context *, llama_buf_map>> ctx_bufs;\n",
    "ctx_bufs.reserve(ctx_map.size());\n",
    "for (auto & it : ctx_map)  ctx_bufs.emplace_back(it.second, buf_map);\n",
    "// Ensure we have enough capacity for the maximum backend buffer we will potentially create\n",
    "pimpl->bufs.reserve(ctx_map.size()); // * ml.files.size();\n",
    "\n",
    "for (auto & it : ctx_map)\n",
    "    ggml_context * ctx = it.second;\n",
    "    llama_buf_map buf_map;\n",
    "    buf_map.reserve(ctx_map.size()); // * ml.files.size()\n",
    "    ggml_backend_dev_props props;\n",
    "    --> ggml_backend_dev_get_props(dev, &props);\n",
    "        static void ggml_backend_cpu_device_get_props(ggml_backend_dev_t dev, struct ggml_backend_dev_props * props) {\n",
    "            props->name        = ggml_backend_cpu_device_get_name(dev);\n",
    "            props->description = ggml_backend_cpu_device_get_description(dev);\n",
    "            props->type        = ggml_backend_cpu_device_get_type(dev);\n",
    "            ggml_backend_cpu_device_get_memory(dev, &props->memory_free, &props->memory_total);\n",
    "            props->caps = {\n",
    "                /* .async                 = */ false,\n",
    "                /* .host_buffer           = */ false,\n",
    "                /* .buffer_from_host_ptr  = */ true,\n",
    "                /* .events                = */ false,\n",
    "            };\n",
    "        }\n",
    "        static void ggml_backend_cuda_device_get_props(ggml_backend_dev_t dev, ggml_backend_dev_props * props) {\n",
    "            props->name        = ggml_backend_cuda_device_get_name(dev);\n",
    "            props->description = ggml_backend_cuda_device_get_description(dev);\n",
    "            props->type        = ggml_backend_cuda_device_get_type(dev);\n",
    "            ggml_backend_cuda_device_get_memory(dev, &props->memory_free, &props->memory_total);\n",
    "            props->caps = {\n",
    "                /* .async                 = */ true,\n",
    "                /* .host_buffer           = */ getenv(\"GGML_CUDA_NO_PINNED\") == nullptr,\n",
    "                /* .buffer_from_host_ptr  = */ false,\n",
    "                /* .events                = */ true, //ifndef GGML_CUDA_NO_PEER_COPY\n",
    "            };\n",
    "        }\n",
    "    if (ml.use_mmap && use_mmap_buffer && buffer_from_host_ptr_supported && is_default_buft)\n",
    "        //for (uint32_t idx = 0; idx < ml.files.size(); idx++)\n",
    "        // only the mmap region containing the tensors in the model is mapped to the backend buffer\n",
    "        // this is important for metal with apple silicon: if the entire model could be mapped to a metal buffer, then we could just use metal for all layers\n",
    "        // this allows using partial offloading when the model size exceeds the metal buffer size, but not the RAM size\n",
    "        --> llama_model_loader::get_mapping_range --> void * addr, size_t first, last;\n",
    "        --> ggml_backend_dev_buffer_from_host_ptr --> ggml_backend_cpu_buffer_from_ptr --> ggml_backend_buffer_t buf;\n",
    "    else\n",
    "        --> ggml_backend_alloc_ctx_tensors_from_buft --> alloc_tensor_range --> ggml_backend_buffer_t buf;\n",
    "    \n",
    "    // indicate that this buffer contains weights, this is used by ggml_backend_sched to improve op scheduling: ops that use a weight are preferably scheduled to the backend that contains the weight\n",
    "    ggml_backend_buffer_set_usage(buf.second, GGML_BACKEND_BUFFER_USAGE_WEIGHTS);\n",
    "    pimpl->bufs.emplace_back(buf);\n",
    "    // if (use_mlock && ggml_backend_buffer_is_host(buf)) use_mlock 我们这里好像默认是false\n",
    "    // for (uint32_t idx = 0; idx < ml.files.size(); idx++)\n",
    "    buf_map.emplace(0, buf);\n",
    "\n",
    "// populate tensors_by_name\n",
    "for (auto & ctx : pimpl->ctxs) for (auto * cur = ggml_get_first_tensor(ctx.get()); cur != NULL; cur = ggml_get_next_tensor(ctx.get(), cur)) tensors_by_name.emplace_back(ggml_get_name(cur), cur);\n",
    "\n",
    "// load tensor data\n",
    "for (auto & it : ctx_bufs)\n",
    "    ggml_context * ctx = it.first;\n",
    "    auto & bufs = it.second;\n",
    "    --> llama_model_loader::load_all_data;\n",
    "\n",
    "for (auto & mapping : ml.mappings) pimpl->mappings.emplace_back(std::move(mapping));\n",
    "return true;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98b352",
   "metadata": {},
   "source": [
    "#### [`make_cpu_buft_list`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#L279)\n",
    "\n",
    "```c++\n",
    "pimpl->cpu_buft_list = make_cpu_buft_list(devices);\n",
    "----------\n",
    "static buft_list_t make_cpu_buft_list(const std::vector<ggml_backend_dev_t> & devices) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b3cd6d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_backend_buffer_type/ggml_backend_buffer_type_t</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_buffer_type {\n",
    "    struct ggml_backend_buffer_type_i  iface;\n",
    "    ggml_backend_dev_t device;\n",
    "    void * context;\n",
    "};\n",
    "typedef struct ggml_backend_buffer_type * ggml_backend_buffer_type_t;\n",
    "// lists of buffer types used for each layer\n",
    "using buft_list_t = std::vector<std::pair<ggml_backend_dev_t, ggml_backend_buffer_type_t>>;\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "// CPU: ACCEL -> GPU host -> CPU extra -> CPU\n",
    "// add ACCEL buffer types\n",
    "\n",
    "// add a host buffer type\n",
    "// storing the tensors in a host buffer is useful when the processing of large batches\n",
    "// is offloaded to a GPU device, since it reduces the time spent on data transfers\n",
    "// generally, this will be done using the first device in the list\n",
    "// a better approach would be to handle this on a weight-by-weight basis using the offload_op\n",
    "// function of the device to determine if it would benefit from being stored in a host buffer\n",
    "for (auto * dev : devices) // 这里的devices指的是GPU\n",
    "    ggml_backend_buffer_type_t buft = ggml_backend_dev_host_buffer_type(dev);\n",
    "        --> return device->iface.get_host_buffer_type(device);\n",
    "        ----------\n",
    "        static ggml_backend_buffer_type_t ggml_backend_cuda_device_get_host_buffer_type(ggml_backend_dev_t dev) {\n",
    "            GGML_UNUSED(dev);\n",
    "            --> return ggml_backend_cuda_host_buffer_type();\n",
    "        }\n",
    "\n",
    "    buft_list.emplace_back(dev, buft);\n",
    "\n",
    "// add extra buffer types, only if no GPU device is present\n",
    "// ref: https://github.com/ggml-org/llama.cpp/issues/12481#issuecomment-2743136094\n",
    "auto * cpu_reg = ggml_backend_dev_backend_reg(cpu_dev);\n",
    "auto ggml_backend_dev_get_extra_bufts_fn = (ggml_backend_dev_get_extra_bufts_t)\n",
    "    ggml_backend_reg_get_proc_address(cpu_reg, \"ggml_backend_dev_get_extra_bufts\");\n",
    "    ---> return reg->iface.get_proc_address(reg, name);\n",
    "        --> return (void *) ggml_backend_dev_get_extra_bufts_t fct = ggml_backend_cpu_device_get_extra_buffers_type;\n",
    "ggml_backend_buffer_type_t * extra_bufts = ggml_backend_dev_get_extra_bufts_fn(cpu_dev);\n",
    "buft_list.emplace_back(cpu_dev, *extra_bufts);\n",
    "\n",
    "// add the CPU buffer type\n",
    "ggml_backend_dev_t dev = ggml_backend_dev_get(i);\n",
    "buft_list.emplace_back(dev, ggml_backend_dev_buffer_type(dev)); //if (ggml_backend_dev_type(dev) == GGML_BACKEND_DEVICE_TYPE_CPU)\n",
    "    --> return device->iface.get_buffer_type(device);\n",
    "        static ggml_backend_buffer_type_t ggml_backend_cpu_device_get_buffer_type(ggml_backend_dev_t dev) {}\n",
    "        --> return ggml_backend_cpu_buffer_type();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb6754",
   "metadata": {},
   "source": [
    "```log\n",
    ">>> p buft_list\n",
    "$94 = std::vector of length 3, capacity 4 = {[0] = {\n",
    "    first = 0x555555a53760,\n",
    "    second = 0x7ffff771adc0 <ggml_backend_cuda_host_buffer_type::ggml_backend_cuda_buffer_type_host>\n",
    "  }, [1] = {\n",
    "    first = 0x7ffff784a080 <ggml_backend_cpu_reg_get_device(ggml_backend_reg*, unsigned long)::ggml_backend_cpu_device>,\n",
    "    second = 0x7ffff784a180 <ggml_backend_cpu_repack_buffer_type()::ggml_backend_cpu_buffer_type_repack>\n",
    "  }, [2] = {\n",
    "    first = 0x7ffff784a080 <ggml_backend_cpu_reg_get_device(ggml_backend_reg*, unsigned long)::ggml_backend_cpu_device>,\n",
    "    second = 0x7fffe6c03b60 <ggml_backend_cpu_buffer_type::ggml_backend_cpu_buffer_type>\n",
    "  }}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8738a5f",
   "metadata": {},
   "source": [
    "##### [`ggml_backend_cuda_host_buffer_type`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L1138)\n",
    "\n",
    "```c++\n",
    "ggml_backend_buffer_type_t ggml_backend_cuda_host_buffer_type() {\n",
    "    static struct ggml_backend_buffer_type ggml_backend_cuda_buffer_type_host = {\n",
    "        /* .iface    = */ {\n",
    "            /* .get_name         = */ ggml_backend_cuda_host_buffer_type_name,\n",
    "            /* .alloc_buffer     = */ ggml_backend_cuda_host_buffer_type_alloc_buffer,\n",
    "            /* .get_alignment    = */ ggml_backend_cpu_buffer_type()->iface.get_alignment,\n",
    "            /* .get_max_size     = */ NULL, // defaults to SIZE_MAX\n",
    "            /* .get_alloc_size   = */ ggml_backend_cpu_buffer_type()->iface.get_alloc_size,\n",
    "            /* .is_host          = */ ggml_backend_cpu_buffer_type()->iface.is_host,\n",
    "        },\n",
    "        /* .device   = */ ggml_backend_reg_dev_get(ggml_backend_cuda_reg(), 0),\n",
    "        /* .context  = */ nullptr,\n",
    "    };\n",
    "\n",
    "    return &ggml_backend_cuda_buffer_type_host;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f81eb",
   "metadata": {},
   "source": [
    "##### [`ggml_backend_cpu_repack_buffer_type`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/repack.cpp#L1555)\n",
    "\n",
    "```c++\n",
    "static ggml_backend_buffer_type_t * ggml_backend_cpu_device_get_extra_buffers_type(ggml_backend_dev_t device) {\n",
    "    --> return ggml_backend_cpu_get_extra_buffers_type().data();\n",
    "\n",
    "    GGML_UNUSED(device);\n",
    "}\n",
    "\n",
    "std::vector<ggml_backend_buffer_type_t>& ggml_backend_cpu_get_extra_buffers_type() {\n",
    "    static std::vector<ggml_backend_buffer_type_t> bufts = []() {\n",
    "        std::vector<ggml_backend_buffer_type_t> bufts;\n",
    "\n",
    "#if defined(__AMX_INT8__) && defined(__AVX512VNNI__)\n",
    "        if (ggml_backend_amx_buffer_type()) {\n",
    "            bufts.push_back(ggml_backend_amx_buffer_type());\n",
    "        }\n",
    "#endif\n",
    "\n",
    "#ifdef GGML_USE_CPU_KLEIDIAI\n",
    "        if (ggml_backend_cpu_kleidiai_buffer_type()) {\n",
    "            bufts.push_back(ggml_backend_cpu_kleidiai_buffer_type());\n",
    "        }\n",
    "#endif\n",
    "\n",
    "#ifdef GGML_USE_CPU_REPACK\n",
    "        if (ggml_backend_cpu_repack_buffer_type()) {\n",
    "            --> bufts.push_back(ggml_backend_cpu_repack_buffer_type());\n",
    "        }\n",
    "#endif\n",
    "\n",
    "        bufts.push_back(NULL);\n",
    "\n",
    "        return bufts;\n",
    "    }();\n",
    "\n",
    "    return bufts;\n",
    "}\n",
    "\n",
    "ggml_backend_buffer_type_t ggml_backend_cpu_repack_buffer_type(void) {\n",
    "    static struct ggml_backend_buffer_type ggml_backend_cpu_buffer_type_repack = {\n",
    "        /* .iface    = */ {\n",
    "                           /* .get_name         = */ ggml_backend_cpu_repack_buffer_type_get_name,\n",
    "                           /* .alloc_buffer     = */ ggml_backend_cpu_repack_buffer_type_alloc_buffer,\n",
    "                           /* .get_alignment    = */ ggml_backend_cpu_repack_buffer_type_get_alignment,\n",
    "                           /* .get_max_size     = */ nullptr,  // defaults to SIZE_MAX\n",
    "                           /* .get_alloc_size   = */ nullptr,  // defaults to ggml_nbytes\n",
    "                           /* .is_host          = */ nullptr,\n",
    "                           },\n",
    "        /* .device  = */ ggml_backend_reg_dev_get(ggml_backend_cpu_reg(), 0),\n",
    "        /* .context = */ new ggml::cpu::repack::extra_buffer_type(),\n",
    "    };\n",
    "\n",
    "    return &ggml_backend_cpu_buffer_type_repack;\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d1271",
   "metadata": {},
   "source": [
    "##### [`ggml_backend_cpu_buffer_type`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1973)\n",
    "\n",
    "```c++\n",
    "ggml_backend_buffer_type_t ggml_backend_cpu_buffer_type(void) {\n",
    "    static struct ggml_backend_buffer_type ggml_backend_cpu_buffer_type = {\n",
    "        /* .iface   = */ {\n",
    "            /* .get_name         = */ ggml_backend_cpu_buffer_type_get_name,\n",
    "            /* .alloc_buffer     = */ ggml_backend_cpu_buffer_type_alloc_buffer,\n",
    "            /* .get_alignment    = */ ggml_backend_cpu_buffer_type_get_alignment,\n",
    "            /* .get_max_size     = */ NULL, // defaults to SIZE_MAX\n",
    "            /* .get_alloc_size   = */ NULL, // defaults to ggml_nbytes\n",
    "            /* .is_host          = */ ggml_backend_cpu_buffer_type_is_host,\n",
    "        },\n",
    "        /* .device  = */ NULL, // FIXME ggml_backend_reg_dev_get(ggml_backend_cpu_reg(), 0),\n",
    "        /* .context = */ NULL,\n",
    "    };\n",
    "\n",
    "    return &ggml_backend_cpu_buffer_type;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89ffc7",
   "metadata": {},
   "source": [
    "#### [`ggml_backend_cuda_buffer_type`](https://github.com/ggml-org/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu#L730)\n",
    "\n",
    "```c++\n",
    "buft_list_t buft_list = make_gpu_buft_list(dev, split_mode, tensor_split);\n",
    "----------\n",
    "// GPU: split if LLAMA_SPLIT_MODE_ROW -> GPU\n",
    "static buft_list_t make_gpu_buft_list(ggml_backend_dev_t dev, llama_split_mode split_mode, const float * tensor_split) {\n",
    "    // split_mode == LLAMA_SPLIT_MODE_LAYER\n",
    "    // add the device default buffer type\n",
    "    --> buft_list.emplace_back(dev, ggml_backend_dev_buffer_type(dev));\n",
    "        --> return device->iface.get_buffer_type(device);\n",
    "        static ggml_backend_buffer_type_t ggml_backend_cuda_device_get_buffer_type(ggml_backend_dev_t dev) {}\n",
    "            ggml_backend_cuda_device_context * ctx = (ggml_backend_cuda_device_context *)dev->context;\n",
    "            --> return ggml_backend_cuda_buffer_type(ctx->device);\n",
    "}\n",
    "\n",
    "ggml_backend_buffer_type_t ggml_backend_cuda_buffer_type(int device) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751497f6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_backend_cuda_buffer_type_context</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_cuda_buffer_type_context {\n",
    "    int device;\n",
    "    std::string name;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "static std::mutex mutex;\n",
    "std::lock_guard<std::mutex> lock(mutex);\n",
    "\n",
    "static ggml_backend_buffer_type ggml_backend_cuda_buffer_types[GGML_CUDA_MAX_DEVICES];\n",
    "static bool ggml_backend_cuda_buffer_type_initialized = false;\n",
    "if (!ggml_backend_cuda_buffer_type_initialized) {\n",
    "    for (int i = 0; i < ggml_backend_cuda_get_device_count(); i++) {\n",
    "        ggml_backend_cuda_buffer_types[i] = {\n",
    "            /* .iface    = */ ggml_backend_cuda_buffer_type_interface,\n",
    "            /* .device   = */ ggml_backend_reg_dev_get(ggml_backend_cuda_reg(), i),\n",
    "            /* .context  = */ new ggml_backend_cuda_buffer_type_context{i, GGML_CUDA_NAME + std::to_string(i)},\n",
    "        };\n",
    "    }\n",
    "    ggml_backend_cuda_buffer_type_initialized = true;\n",
    "}\n",
    "return &ggml_backend_cuda_buffer_types[device];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22f4be-1f51-437c-ba6e-32f1caed638b",
   "metadata": {},
   "source": [
    "#### [`weight_buft_supported`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#L140)\n",
    "\n",
    "```c++\n",
    "ggml_backend_buffer_type_t buft = select_weight_buft(hparams, t_meta, op, *buft_list);\n",
    "----------\n",
    "// find the first buffer type in the list that can use the tensor\n",
    "static ggml_backend_buffer_type_t select_weight_buft(const llama_hparams & hparams, ggml_tensor * tensor, ggml_op op, const buft_list_t & buft_list) {\n",
    "    for (const auto & cur : buft_list)\n",
    "        ggml_backend_dev_t cur_dev = cur.first; // 目前只有CPU和GPU两个选项\n",
    "        ggml_backend_buffer_type_t cur_buft = cur.second; // 有CUDA, CUDA_HOST, CPU_REPACK, CPU四个选项\n",
    "        --> if (weight_buft_supported(hparams, tensor, op, cur_buft = cur.second, cur_dev = cur.first)) return cur_buft;\n",
    "}\n",
    "\n",
    "// checks if the weight tensor can be used with the specified buffer type and device\n",
    "static bool weight_buft_supported(const llama_hparams & hparams, ggml_tensor * w, ggml_op op, ggml_backend_buffer_type_t buft, ggml_backend_dev_t dev) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbb3d1-24e1-47e1-8308-ec9de9ebe01f",
   "metadata": {},
   "source": [
    "```c++\n",
    "ggml_init_params params = {\n",
    "    /*.mem_size   =*/ ggml_tensor_overhead()*8,\n",
    "    /*.mem_buffer =*/ NULL,\n",
    "    /*.no_alloc   =*/ true,\n",
    "};\n",
    "ggml_context_ptr ctx_ptr { ggml_init(params) };\n",
    "\n",
    "ggml_context * ctx = ctx_ptr.get();\n",
    "ggml_tensor * op_tensor = nullptr;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a5303-6f78-4266-a722-206ea33d8d92",
   "metadata": {},
   "source": [
    "```c++\n",
    "switch (op) {\n",
    "    case GGML_OP_GET_ROWS:\n",
    "        {\n",
    "            ggml_tensor * b = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 512);\n",
    "            op_tensor = ggml_get_rows(ctx, w, b);\n",
    "        } break;\n",
    "    case GGML_OP_MUL_MAT:\n",
    "        {\n",
    "            ggml_tensor * b = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, w->ne[0], 512, w->ne[2], w->ne[3]);\n",
    "            op_tensor = ggml_mul_mat(ctx, w, b);\n",
    "        } break;\n",
    "    case GGML_OP_ADD:\n",
    "        {\n",
    "            ggml_tensor * a = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, w->ne[0], w->ne[1], w->ne[2], w->ne[3]);\n",
    "            op_tensor = ggml_add(ctx, a, w);\n",
    "        } break;\n",
    "    case GGML_OP_MUL:\n",
    "        {\n",
    "            ggml_tensor * a = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, w->ne[0], w->ne[1], w->ne[2], w->ne[3]);\n",
    "            op_tensor = ggml_mul(ctx, a, w);\n",
    "        } break;\n",
    "    case GGML_OP_DIV:\n",
    "        {\n",
    "            ggml_tensor * a = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, w->ne[0]);\n",
    "            op_tensor = ggml_div(ctx, a, w);\n",
    "        } break;\n",
    "    case GGML_OP_ROPE:\n",
    "        {\n",
    "            int n_embd_head = hparams.n_embd_head_v;\n",
    "            int n_head = hparams.n_head();\n",
    "            ggml_tensor * a = ggml_new_tensor_3d(ctx, GGML_TYPE_F32, n_embd_head, n_head, 512);\n",
    "            ggml_tensor * b = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 512);\n",
    "            op_tensor = ggml_rope_ext(\n",
    "                ctx, a, b, w,\n",
    "                0, 0, 0, 0, 0,\n",
    "                0, 0, 0, 0\n",
    "            );\n",
    "        } break;\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>ggml_new_tensor</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_tensor * ggml_new_tensor_1d()\n",
    "    return ggml_new_tensor(ctx, type, 1, &ne0);\n",
    "\n",
    "struct ggml_tensor * ggml_new_tensor_2d()\n",
    "    const int64_t ne[2] = { ne0, ne1};\n",
    "    return ggml_new_tensor(ctx, type, 2, ne);\n",
    "\n",
    "struct ggml_tensor * ggml_new_tensor_3d()\n",
    "    const int64_t ne[3] = { ne0, ne1, ne2 };\n",
    "    return ggml_new_tensor(ctx, type, 3, ne);\n",
    "\n",
    "struct ggml_tensor * ggml_new_tensor_4d()\n",
    "    const int64_t ne[4] = { ne0, ne1, ne2, ne3 };\n",
    "    return ggml_new_tensor(ctx, type, 4, ne);\n",
    "\n",
    "struct ggml_tensor * ggml_get_rows()\n",
    "    struct ggml_tensor * result = ggml_new_tensor_4d(ctx, type, a->ne[0], b->ne[0], b->ne[1], b->ne[2]);\n",
    "    result->op     = GGML_OP_GET_ROWS;\n",
    "    result->src[0] = a;\n",
    "    result->src[1] = b;\n",
    "    return result;\n",
    "\n",
    "static struct ggml_tensor * ggml_rope_impl()\n",
    "    int sections[4] = {0, 0, 0, 0};\n",
    "\n",
    "    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n",
    "\n",
    "    int32_t params[15] = { /*n_past*/ 0, n_dims, mode, /*n_ctx*/ 0, n_ctx_orig };\n",
    "    memcpy(params +  5, &freq_base,    sizeof(float));\n",
    "    memcpy(params +  6, &freq_scale,   sizeof(float));\n",
    "    memcpy(params +  7, &ext_factor,   sizeof(float));\n",
    "    memcpy(params +  8, &attn_factor,  sizeof(float));\n",
    "    memcpy(params +  9, &beta_fast,    sizeof(float));\n",
    "    memcpy(params + 10, &beta_slow,    sizeof(float));\n",
    "    memcpy(params + 11, &sections,     sizeof(int)*4);\n",
    "    ggml_set_op_params(result, params, sizeof(params));\n",
    "\n",
    "    result->op     = GGML_OP_ROPE;\n",
    "    result->src[0] = a;\n",
    "    result->src[1] = b;\n",
    "    result->src[2] = c;\n",
    "\n",
    "    return result;\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c9982-bda8-41f4-8741-18241262b8b0",
   "metadata": {},
   "source": [
    "```c++\n",
    "w->buffer = ggml_backend_buft_alloc_buffer(buft, 0);\n",
    "    --> // return a dummy buffer for zero-sized allocations\n",
    "        return ggml_backend_buffer_init(buft, {}, NULL, 0);\n",
    "--> ggml_backend_dev_supports_op(dev, op_tensor) --> bool op_supported;\n",
    "ggml_backend_buffer_free(w->buffer);\n",
    "return op_supported;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b50d7f",
   "metadata": {},
   "source": [
    "[`ggml_backend_buffer_init`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L82)\n",
    "\n",
    "```c++\n",
    "ggml_backend_buffer_t ggml_backend_buffer_init(\n",
    "               ggml_backend_buffer_type_t buft,\n",
    "        struct ggml_backend_buffer_i      iface,\n",
    "               void *                     context,\n",
    "               size_t                     size) {\n",
    "    ggml_backend_buffer_t buffer = new ggml_backend_buffer {\n",
    "        /* .interface = */ iface,\n",
    "        /* .buft      = */ buft,\n",
    "        /* .context   = */ context,\n",
    "        /* .size      = */ size,\n",
    "        /* .usage     = */ GGML_BACKEND_BUFFER_USAGE_ANY\n",
    "    };\n",
    "\n",
    "    return buffer;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a168b2",
   "metadata": {},
   "source": [
    "##### [`ggml_backend_cuda_device_supports_op`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L3002)\n",
    "\n",
    "```c++\n",
    "static bool ggml_backend_cuda_device_supports_op(ggml_backend_dev_t dev, const ggml_tensor * op) {\n",
    "    ggml_backend_cuda_device_context * dev_ctx = (ggml_backend_cuda_device_context *) dev->context;\n",
    "    // split buffers can only be used with GGML_OP_MUL_MAT\n",
    "    if (op->op != GGML_OP_MUL_MAT) for (int i = 0; i < GGML_MAX_SRC; i++)\n",
    "        if (op->src[i] && op->src[i]->buffer && ggml_backend_buft_is_cuda_split(op->src[i]->buffer->buft)) return false;\n",
    "    // check if all the sources are allocated on this device\n",
    "    for (int i = 0; i < GGML_MAX_SRC; i++)\n",
    "    if (op->src[i] && op->src[i]->buffer && ggml_backend_buft_is_cuda(op->src[i]->buffer->buft))\n",
    "        auto * buft_ctx = (ggml_backend_cuda_buffer_type_context *)op->src[i]->buffer->buft->context;\n",
    "            if (buft_ctx->device != dev_ctx->device) return false;\n",
    "    switch (op->op) {\n",
    "        case GGML_OP_UNARY:\n",
    "            switch (ggml_get_unary_op(op)) {\n",
    "                case GGML_UNARY_OP_ABS:\n",
    "                case GGML_UNARY_OP_SGN:\n",
    "                case GGML_UNARY_OP_NEG:\n",
    "                case GGML_UNARY_OP_STEP:\n",
    "                case GGML_UNARY_OP_GELU:\n",
    "                case GGML_UNARY_OP_SILU:\n",
    "                case GGML_UNARY_OP_RELU:\n",
    "                case GGML_UNARY_OP_SIGMOID:\n",
    "                case GGML_UNARY_OP_HARDSIGMOID:\n",
    "                case GGML_UNARY_OP_HARDSWISH:\n",
    "                case GGML_UNARY_OP_GELU_ERF:\n",
    "                case GGML_UNARY_OP_GELU_QUICK:\n",
    "                case GGML_UNARY_OP_TANH:\n",
    "                case GGML_UNARY_OP_EXP:\n",
    "                    return ggml_is_contiguous(op->src[0]);\n",
    "                default:\n",
    "                    return false;\n",
    "            }\n",
    "            break;\n",
    "        case GGML_OP_MUL_MAT:\n",
    "        case GGML_OP_MUL_MAT_ID:\n",
    "            {\n",
    "                struct ggml_tensor * a = op->src[0];\n",
    "                struct ggml_tensor * b = op->src[1];\n",
    "                if (a->buffer && ggml_backend_buft_is_cuda_split(a->buffer->buft)) {\n",
    "                    if (a->ne[2] > 1 || a->ne[3] > 1) {\n",
    "                        return false;\n",
    "                    }\n",
    "                    // for small weight matrices the active device can end up without any rows, don't use row split in those cases\n",
    "                    // this avoids some edge cases (and the performance would not be good anyways)\n",
    "                    ggml_backend_cuda_split_buffer_type_context * buft_ctx = (ggml_backend_cuda_split_buffer_type_context *) a->buffer->buft->context;\n",
    "                    int64_t row_low;\n",
    "                    int64_t row_high;\n",
    "                    get_row_split(&row_low, &row_high, a, buft_ctx->tensor_split, dev_ctx->device);\n",
    "                    if (row_low == row_high) {\n",
    "                        return false;\n",
    "                    }\n",
    "                }\n",
    "                if (b->type == GGML_TYPE_F16 && a->type != GGML_TYPE_F16) {\n",
    "                    return false;\n",
    "                }\n",
    "#ifdef GGML_USE_MUSA\n",
    "                if (b->type == GGML_TYPE_F16 && b->ne[2]*b->ne[3] > 1 &&\n",
    "                    !ggml_is_transposed(a) && !ggml_is_transposed(b)) {\n",
    "                    return false;\n",
    "                }\n",
    "#endif // GGML_USE_MUSA\n",
    "                switch (a->type) {\n",
    "                    case GGML_TYPE_F32:\n",
    "                    case GGML_TYPE_F16:\n",
    "                    case GGML_TYPE_Q4_0:\n",
    "                    case GGML_TYPE_Q4_1:\n",
    "                    case GGML_TYPE_Q5_0:\n",
    "                    case GGML_TYPE_Q5_1:\n",
    "                    case GGML_TYPE_Q8_0:\n",
    "                    case GGML_TYPE_Q2_K:\n",
    "                    case GGML_TYPE_Q3_K:\n",
    "                    case GGML_TYPE_Q4_K:\n",
    "                    case GGML_TYPE_Q5_K:\n",
    "                    case GGML_TYPE_Q6_K:\n",
    "                    case GGML_TYPE_Q8_K:\n",
    "                    case GGML_TYPE_IQ1_M:\n",
    "                    case GGML_TYPE_IQ1_S:\n",
    "                    case GGML_TYPE_IQ2_S:\n",
    "                    case GGML_TYPE_IQ2_XS:\n",
    "                    case GGML_TYPE_IQ2_XXS:\n",
    "                    case GGML_TYPE_IQ3_S:\n",
    "                    case GGML_TYPE_IQ3_XXS:\n",
    "                    case GGML_TYPE_IQ4_NL:\n",
    "                    case GGML_TYPE_IQ4_XS:\n",
    "                    case GGML_TYPE_BF16:\n",
    "#ifdef GGML_USE_MUSA\n",
    "                        if (a->type == GGML_TYPE_Q3_K) {\n",
    "                            return false;\n",
    "                        }\n",
    "#endif // GGML_USE_MUSA\n",
    "                        return true;\n",
    "                    default:\n",
    "                        return false;\n",
    "                }\n",
    "            } break;\n",
    "        case GGML_OP_OUT_PROD:\n",
    "            return op->type == GGML_TYPE_F32 && op->src[0]->type == GGML_TYPE_F32 && op->src[1]->type == GGML_TYPE_F32;\n",
    "        case GGML_OP_GET_ROWS:\n",
    "            {\n",
    "                switch (op->src[0]->type) {\n",
    "                    case GGML_TYPE_F16:\n",
    "                    case GGML_TYPE_F32:\n",
    "                    case GGML_TYPE_Q4_0:\n",
    "                    case GGML_TYPE_Q4_1:\n",
    "                    case GGML_TYPE_Q5_0:\n",
    "                    case GGML_TYPE_Q5_1:\n",
    "                    case GGML_TYPE_Q8_0:\n",
    "                        return true;\n",
    "                    default:\n",
    "                        return false;\n",
    "                }\n",
    "            } break;\n",
    "        case GGML_OP_CPY:\n",
    "            {\n",
    "                ggml_type src0_type = op->src[0]->type;\n",
    "                ggml_type src1_type = op->src[1]->type;\n",
    "                if (src0_type == GGML_TYPE_F32 && src1_type == GGML_TYPE_F32)\n",
    "                if (src0_type == GGML_TYPE_F32 && src1_type == GGML_TYPE_BF16)\n",
    "                if (src0_type == GGML_TYPE_F32 && src1_type == GGML_TYPE_F16)\n",
    "                if (src0_type == GGML_TYPE_F32 && src1_type == GGML_TYPE_Q8_0)\n",
    "                if (src0_type == GGML_TYPE_Q8_0 && src1_type == GGML_TYPE_F32)\n",
    "                if (src0_type == GGML_TYPE_F32 && src1_type == GGML_TYPE_Q4_0)\n",
    "                if (src0_type == GGML_TYPE_Q4_0 && src1_type == GGML_TYPE_F32)\n",
    "                if (src0_type == GGML_TYPE_F32 && src1_type == GGML_TYPE_Q4_1)\n",
    "                if (src0_type == GGML_TYPE_Q4_1 && src1_type == GGML_TYPE_F32)\n",
    "                if (src0_type == GGML_TYPE_F32 && src1_type == GGML_TYPE_Q5_0)\n",
    "                if (src0_type == GGML_TYPE_Q5_0 && src1_type == GGML_TYPE_F32)\n",
    "                if (src0_type == GGML_TYPE_F32 && src1_type == GGML_TYPE_Q5_1)\n",
    "                if (src0_type == GGML_TYPE_Q5_1 && src1_type == GGML_TYPE_F32)\n",
    "                if (src0_type == GGML_TYPE_F32 && src1_type == GGML_TYPE_IQ4_NL)\n",
    "                if (src0_type == GGML_TYPE_F16 && src1_type == GGML_TYPE_F16)\n",
    "                if (src0_type == GGML_TYPE_F16 && src1_type == GGML_TYPE_F32)\n",
    "                if (src0_type == src1_type && ggml_is_contiguous(op->src[0]) && ggml_is_contiguous(op->src[1]))\n",
    "                    return true;\n",
    "                return false;\n",
    "            } break;\n",
    "        case GGML_OP_DUP:\n",
    "            {\n",
    "                ggml_type src0_type = op->src[0]->type;\n",
    "                return src0_type != GGML_TYPE_I32 && src0_type != GGML_TYPE_I16;\n",
    "            } break;\n",
    "        case GGML_OP_NORM:\n",
    "        case GGML_OP_RMS_NORM:\n",
    "        case GGML_OP_L2_NORM:\n",
    "            return true;\n",
    "        case GGML_OP_NONE:\n",
    "        case GGML_OP_RESHAPE:\n",
    "        case GGML_OP_VIEW:\n",
    "        case GGML_OP_PERMUTE:\n",
    "        case GGML_OP_TRANSPOSE:\n",
    "        case GGML_OP_ADD:\n",
    "        case GGML_OP_ADD1:\n",
    "        case GGML_OP_SUB:\n",
    "        case GGML_OP_MUL:\n",
    "        case GGML_OP_DIV:\n",
    "        case GGML_OP_SCALE:\n",
    "        case GGML_OP_SQR:\n",
    "        case GGML_OP_SQRT:\n",
    "        case GGML_OP_SIN:\n",
    "        case GGML_OP_COS:\n",
    "        case GGML_OP_CLAMP:\n",
    "        case GGML_OP_LOG:\n",
    "        case GGML_OP_SSM_SCAN:\n",
    "        case GGML_OP_SSM_CONV:\n",
    "            return true;\n",
    "        case GGML_OP_CONT:\n",
    "            return op->src[0]->type != GGML_TYPE_BF16;\n",
    "        case GGML_OP_DIAG_MASK_INF:\n",
    "        case GGML_OP_SOFT_MAX:\n",
    "            return true;\n",
    "        case GGML_OP_ROPE:\n",
    "        case GGML_OP_ROPE_BACK: {\n",
    "            return op->src[0]->nb[0] == ggml_type_size(op->src[0]->type) && ggml_is_contiguous_2(op->src[0]);\n",
    "        }\n",
    "        default:\n",
    "            return false;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15a38f",
   "metadata": {},
   "source": [
    "##### [`ggml_backend_cpu_device_supports_op`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.cpp:392)\n",
    "\n",
    "```c++\n",
    "static bool ggml_backend_cpu_device_supports_op(ggml_backend_dev_t dev, const struct ggml_tensor * op) {\n",
    "    const struct ggml_tensor * src0 = op->src[0];\n",
    "    const struct ggml_tensor * src1 = op->src[1];\n",
    "\n",
    "    if (op->op == GGML_OP_NONE or GGML_OP_RESHAPE or GGML_OP_VIEW or GGML_OP_PERMUTE or GGML_OP_TRANSPOSE) {\n",
    "        return true;\n",
    "    }\n",
    "\n",
    "    // extra_buffer_op?\n",
    "    for (auto extra : ggml_backend_cpu_get_extra_buffers_type()) {\n",
    "        // if (extra)\n",
    "        auto buf_extra = (ggml::cpu::extra_buffer_type*) extra->context;\n",
    "        --> if (buf_extra && buf_extra->supports_op(dev, op)) return true;\n",
    "    }\n",
    "\n",
    "    // the other case need host buffer.\n",
    "    for (int i = 0; i < GGML_MAX_SRC; i++)\n",
    "        if (op->src[i] && op->src[i]->buffer && !ggml_backend_buft_is_host(op->src[i]->buffer->buft)) return false;\n",
    "        --> return buft->iface.is_host(buft);\n",
    "            static bool ggml_backend_cpu_buffer_type_is_host(ggml_backend_buffer_type_t buft)\n",
    "            --> return true;\n",
    "\n",
    "    switch (op->op) {\n",
    "        case GGML_OP_CPY:\n",
    "            return\n",
    "                op->type != GGML_TYPE_IQ3_XXS &&\n",
    "                op->type != GGML_TYPE_IQ3_S   &&\n",
    "                op->type != GGML_TYPE_IQ2_XXS &&\n",
    "                op->type != GGML_TYPE_IQ2_XS  &&\n",
    "                op->type != GGML_TYPE_IQ2_S   &&\n",
    "                op->type != GGML_TYPE_IQ1_S   &&\n",
    "                op->type != GGML_TYPE_IQ1_M; // missing type_traits.from_float\n",
    "        case GGML_OP_MUL_MAT:\n",
    "            return src1->type == GGML_TYPE_F32 || src1->type == ggml_get_type_traits_cpu(src0->type)->vec_dot_type;\n",
    "        case GGML_OP_OUT_PROD:\n",
    "            return (src0->type == GGML_TYPE_F32 || (ggml_is_quantized(src0->type) && src0->ne[2] == src1->ne[2] && src0->ne[3] == src1->ne[3])) &&\n",
    "                src1->type == GGML_TYPE_F32 && op->type == GGML_TYPE_F32;\n",
    "        default:\n",
    "            return true;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee718e",
   "metadata": {},
   "source": [
    "[`ggml::cpu::repack::extra_buffer_type::supports_op`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/repack.cpp#L1508)\n",
    "\n",
    "```c++\n",
    "namespace ggml::cpu::repack {\n",
    "class extra_buffer_type : ggml::cpu::extra_buffer_type {\n",
    "    bool supports_op(ggml_backend_dev_t, const struct ggml_tensor * op) override {\n",
    "        if (op->op == GGML_OP_MUL_MAT && op->src[0]->buffer && (ggml_n_dims(op->src[0]) == 2) && \n",
    "            op->src[0]->buffer->buft == ggml_backend_cpu_repack_buffer_type() && \n",
    "            --> ggml_repack_get_optimal_repack_type(op->src[0])) {\n",
    "            if (op->src[1]->buffer && !ggml_backend_buft_is_host(op->src[1]->buffer->buft)) {\n",
    "                return false;\n",
    "            }\n",
    "            if (op->src[1]->type == GGML_TYPE_F32) {\n",
    "                return true;\n",
    "            }\n",
    "            //if (op->src[1]->type == GGML_TYPE_Q8_0) {\n",
    "            //    return true;\n",
    "            //}\n",
    "            // may be possible if Q8_0 packed...\n",
    "        }\n",
    "        return false;\n",
    "    }\n",
    "\n",
    "    ggml::cpu::tensor_traits * get_tensor_traits(const struct ggml_tensor * op) override {\n",
    "        if (op->op == GGML_OP_MUL_MAT || op->op == GGML_OP_MUL_MAT_ID) {\n",
    "            if (op->src[0]->buffer && op->src[0]->buffer->buft == ggml_backend_cpu_repack_buffer_type()) {\n",
    "                return (ggml::cpu::tensor_traits *) op->src[0]->extra;\n",
    "            }\n",
    "        }\n",
    "        return nullptr;\n",
    "    }\n",
    "};\n",
    "}  // namespace ggml::cpu::repack\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3867219",
   "metadata": {},
   "source": [
    "[`ggml_repack_get_optimal_repack_type`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/repack.cpp#L1416)\n",
    "\n",
    "```c++\n",
    "static const ggml::cpu::tensor_traits * ggml_repack_get_optimal_repack_type(const struct ggml_tensor * cur) {\n",
    "\n",
    "    // instance for Q4\n",
    "    static const ggml::cpu::repack::tensor_traits<block_q4_0, 4, 4, GGML_TYPE_Q8_0> q4_0_4x4_q8_0;\n",
    "    static const ggml::cpu::repack::tensor_traits<block_q4_0, 8, 4, GGML_TYPE_Q8_0> q4_0_4x8_q8_0;\n",
    "    static const ggml::cpu::repack::tensor_traits<block_q4_0, 8, 8, GGML_TYPE_Q8_0> q4_0_8x8_q8_0;\n",
    "    static const ggml::cpu::repack::tensor_traits<block_q4_K, 8, 8, GGML_TYPE_Q8_K> q4_K_8x8_q8_K;\n",
    "\n",
    "    // instance for IQ4\n",
    "    static const ggml::cpu::repack::tensor_traits<block_iq4_nl, 4, 4, GGML_TYPE_Q8_0> iq4_nl_4x4_q8_0;\n",
    "\n",
    "    if (cur->type == GGML_TYPE_Q4_0) {\n",
    "        if (ggml_cpu_has_avx2() || (ggml_cpu_has_sve() && ggml_cpu_has_matmul_int8() && ggml_cpu_get_sve_cnt() == QK8_0)) {\n",
    "            if (cur->ne[1] % 8 == 0) {\n",
    "                return &q4_0_8x8_q8_0;\n",
    "            }\n",
    "        }\n",
    "        if (ggml_cpu_has_neon() && ggml_cpu_has_matmul_int8()) {\n",
    "            if (cur->ne[1] % 4 == 0) {\n",
    "                return &q4_0_4x8_q8_0;\n",
    "            }\n",
    "        }\n",
    "        if (ggml_cpu_has_neon() && ggml_cpu_has_dotprod()) {\n",
    "            if (cur->ne[1] % 4 == 0) {\n",
    "                return &q4_0_4x4_q8_0;\n",
    "            }\n",
    "        }\n",
    "    } else if (cur->type == GGML_TYPE_Q4_K) {\n",
    "        if (ggml_cpu_has_avx2()) {\n",
    "            if (cur->ne[1] % 8 == 0) {\n",
    "                return &q4_K_8x8_q8_K;\n",
    "            }\n",
    "        }\n",
    "    } else if (cur->type == GGML_TYPE_IQ4_NL) {\n",
    "        if (ggml_cpu_has_neon() && ggml_cpu_has_dotprod()) {\n",
    "            if (cur->ne[1] % 4 == 0) {\n",
    "                return &iq4_nl_4x4_q8_0;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return nullptr;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c727af-2ca8-44e5-828f-09c3d2ec4abc",
   "metadata": {},
   "source": [
    "#### [`llama_model_loader::create_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L789)\n",
    "\n",
    "```c++\n",
    "return ml.create_tensor(ctx, tn, ne, flags);\n",
    "----------\n",
    "struct ggml_tensor * llama_model_loader::create_tensor(struct ggml_context * ctx, const std::string & name, const std::initializer_list<int64_t> & ne, int flags) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77bfe6-68d8-442e-85b9-7b9913d6e15f",
   "metadata": {},
   "source": [
    "```c++\n",
    "--> check_tensor_dims --> cur;\n",
    "bool duplicated = flags & TENSOR_DUPLICATED;\n",
    "--> ggml_dup_tensor --> tensor;\n",
    "ggml_set_name(tensor, ggml_get_name(cur));\n",
    "if (duplicated) {\n",
    "    size_data += ggml_nbytes(cur);\n",
    "} else {\n",
    "    n_created++;\n",
    "}\n",
    "return tensor;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c992f-40c0-4e93-85fa-ab915b5ee0dc",
   "metadata": {},
   "source": [
    "[`llama_model_loader::check_tensor_dims`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L759)\n",
    "\n",
    "> 这个函数能检测到转置矩阵的存在，可以加以利用！\n",
    "\n",
    "```c++\n",
    "const struct ggml_tensor * cur = check_tensor_dims(name, ne, !(flags & TENSOR_NOT_REQUIRED));\n",
    "----------\n",
    "const struct ggml_tensor * llama_model_loader::check_tensor_dims(const std::string & name, const std::vector<int64_t> & ne, bool required) const {\n",
    "    const struct ggml_tensor * cur = get_tensor_meta(name.c_str());\n",
    "    return cur;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b327e9-86e6-4308-9eb6-aa39a0812b6b",
   "metadata": {},
   "source": [
    "[`ggml_dup_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L1692)\n",
    "\n",
    "```c++\n",
    "struct ggml_tensor * tensor = ggml_dup_tensor(ctx, cur);\n",
    "----------\n",
    "struct ggml_tensor * ggml_dup_tensor(struct ggml_context * ctx, const struct ggml_tensor * src) {\n",
    "    return ggml_new_tensor(ctx, src->type, GGML_MAX_DIMS, src->ne);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54ddae-ac49-42fd-badb-fe9410f808d3",
   "metadata": {},
   "source": [
    "#### [`llama_model_loader::init_mappings`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L845)\n",
    "\n",
    "```c++\n",
    "ml.init_mappings(true, nullptr);\n",
    "----------\n",
    "void llama_model_loader::init_mappings(bool prefetch, llama_mlocks * mlock_mmaps) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5c1a2-b9aa-4269-ac36-93b676d6909e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct llama_mmap/llama_mmaps</summary>\n",
    "\n",
    "```c++\n",
    "struct llama_mmap {\n",
    "    llama_mmap(const llama_mmap &) = delete;\n",
    "    llama_mmap(struct llama_file * file, size_t prefetch = (size_t) -1, bool numa = false);\n",
    "    ~llama_mmap();\n",
    "\n",
    "    size_t size() const;\n",
    "    void * addr() const;\n",
    "\n",
    "    void unmap_fragment(size_t first, size_t last);\n",
    "\n",
    "    static const bool SUPPORTED;\n",
    "\n",
    "private:\n",
    "    struct impl;\n",
    "    std::unique_ptr<impl> pimpl;\n",
    "};\n",
    "using llama_mmaps  = std::vector<std::unique_ptr<llama_mmap>>;\n",
    "\n",
    "size_t llama_mmap::size() const { return pimpl->size; }\n",
    "void * llama_mmap::addr() const { return pimpl->addr; }\n",
    "\n",
    "void llama_mmap::unmap_fragment(size_t first, size_t last) { pimpl->unmap_fragment(first, last); }\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "// for (const auto & file : files)\n",
    "--> llama_mmap::llama_mmap --> llama_mmap::impl::impl --> mapping;\n",
    "llama_model_loader:: std::vector<std::pair<size_t, size_t>>  mmaps_used.emplace_back(mapping->size(), 0);\n",
    "llama_model_loader:: llama_mmaps mappings.emplace_back(std::move(mapping));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5504b-b640-44c4-bce7-ec63d58875f9",
   "metadata": {},
   "source": [
    "[`llama_mmap::llama_mmap`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-mmap.cpp#L441)\n",
    "\n",
    "```c++\n",
    "std::unique_ptr<llama_mmap> mapping = std::make_unique<llama_mmap>(file.get(), prefetch ? -1 : 0, is_numa);\n",
    "----------\n",
    "llama_mmap::llama_mmap(struct llama_file * file, size_t prefetch, bool numa) : pimpl(std::make_unique<impl>(file, prefetch, numa)) {}\n",
    "struct llama_mmap::impl {\n",
    "    void * addr;\n",
    "    size_t size;\n",
    "    std::vector<std::pair<size_t, size_t>> mapped_fragments;\n",
    "    static void align_range(size_t * first, size_t * last, size_t page_size) {\n",
    "        size_t offset_in_page = *first & (page_size - 1);\n",
    "        size_t offset_to_page = offset_in_page == 0 ? 0 : page_size - offset_in_page;\n",
    "        *first += offset_to_page;\n",
    "\n",
    "        *last = *last & ~(page_size - 1);\n",
    "\n",
    "        if (*last <= *first) {\n",
    "            *last = *first;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    impl(struct llama_file * file, size_t prefetch, bool numa) {\n",
    "        ---> size = file->size();\n",
    "        int fd = file->file_id();\n",
    "        int flags = MAP_SHARED;\n",
    "        if (prefetch) { flags |= MAP_POPULATE; }\n",
    "        ---> addr = mmap(NULL, file->size(), PROT_READ, flags, fd, 0);\n",
    "        if (prefetch > 0) {\n",
    "            if (posix_madvise(addr, std::min(file->size(), prefetch), POSIX_MADV_WILLNEED)) {\n",
    "                LLAMA_LOG_WARN(\"warning: posix_madvise(.., POSIX_MADV_WILLNEED) failed: %s\\n\",\n",
    "                        strerror(errno));\n",
    "            }\n",
    "        }\n",
    "        ---> mapped_fragments.emplace_back(0, file->size());\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea6ed7-7a50-44f2-8e68-37447c81f32a",
   "metadata": {},
   "source": [
    "#### [`llama_model_loader::get_mapping_range`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L878)\n",
    "\n",
    "```c++\n",
    "ml.get_mapping_range(&first, &last, &addr, idx=0, ctx);\n",
    "----------\n",
    "void llama_model_loader::get_mapping_range(size_t * first, size_t * last, void ** addr, int idx, ggml_context * ctx) const {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b56a65-03b8-4a4d-9574-caa9e8d79942",
   "metadata": {},
   "source": [
    "```c++\n",
    "const auto & mapping = mappings.at(idx);\n",
    "\n",
    "*first = mapping->size();\n",
    "*last  = 0;\n",
    "*addr = mapping->addr();\n",
    "for (ggml_tensor * tensor = ggml_get_first_tensor(ctx); tensor; tensor = ggml_get_next_tensor(ctx, tensor)) {\n",
    "    const auto * weight = get_weight(ggml_get_name(tensor));\n",
    "    if (!weight || weight->idx != idx) {\n",
    "        continue;\n",
    "    }\n",
    "    *first = std::min(*first, weight->offs);\n",
    "    *last  = std::max(*last,  weight->offs + ggml_nbytes(tensor));\n",
    "}\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>struct llama_model_loader::llama_tensor_weight</summary>\n",
    "\n",
    "```c++\n",
    "// Holds information on a model weight\n",
    "struct llama_tensor_weight {\n",
    "    uint16_t  idx; // source file index\n",
    "    size_t   offs; // tensor data offset in the original file\n",
    "    ggml_tensor * tensor;\n",
    "}\n",
    "\n",
    "const llama_model_loader::llama_tensor_weight * llama_model_loader::get_weight(const char * name) const {\n",
    "    auto pos = weights_map.find(name);\n",
    "    if (pos != weights_map.end()) {\n",
    "        return &pos->second;\n",
    "    }\n",
    "\n",
    "    return nullptr;\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed2404-8fdc-48b2-8c24-173fd6321bda",
   "metadata": {},
   "source": [
    "#### [`ggml_backend_cpu_buffer_from_ptr`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L2013)\n",
    "\n",
    "```c++\n",
    "ggml_backend_buffer_t buf = ggml_backend_dev_buffer_from_host_ptr(dev, (char *) addr + first, last - first, ggml_get_max_tensor_size(ctx));\n",
    "----------\n",
    "ggml_backend_buffer_t ggml_backend_dev_buffer_from_host_ptr(ggml_backend_dev_t device, void * ptr, size_t size, size_t max_tensor_size) {\n",
    "    --> return device->iface.buffer_from_host_ptr(device, ptr, size, max_tensor_size);\n",
    "}\n",
    "\n",
    "static ggml_backend_buffer_t ggml_backend_cpu_device_buffer_from_host_ptr(ggml_backend_dev_t dev, void * ptr, size_t size, size_t max_tensor_size) {\n",
    "    --> return ggml_backend_cpu_buffer_from_ptr(ptr, size);\n",
    "}\n",
    "\n",
    "ggml_backend_buffer_t ggml_backend_cpu_buffer_from_ptr(void * ptr, size_t size) {\n",
    "    --> return ggml_backend_buffer_init(ggml_backend_cpu_buffer_from_ptr_type(), ggml_backend_cpu_buffer_from_ptr_i, ptr, size);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ba812-14e6-4642-a5bf-6eb4509254b2",
   "metadata": {},
   "source": [
    "[`ggml_backend_buffer_init`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L82)\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_backend_buffer/ggml_backend_buffer_t</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_buffer {\n",
    "    struct ggml_backend_buffer_i  iface;\n",
    "    ggml_backend_buffer_type_t    buft;\n",
    "    void * context;\n",
    "    size_t size;\n",
    "    enum ggml_backend_buffer_usage usage;\n",
    "};\n",
    "\n",
    "typedef struct ggml_backend_buffer * ggml_backend_buffer_t;\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "// backend buffer\n",
    "\n",
    "ggml_backend_buffer_t ggml_backend_buffer_init(\n",
    "               ggml_backend_buffer_type_t buft,\n",
    "        struct ggml_backend_buffer_i      iface,\n",
    "               void *                     context,\n",
    "               size_t                     size) {\n",
    "    ggml_backend_buffer_t buffer = new ggml_backend_buffer {\n",
    "        /* .interface = */ iface,\n",
    "        /* .buft      = */ buft,\n",
    "        /* .context   = */ context,\n",
    "        /* .size      = */ size,\n",
    "        /* .usage     = */ GGML_BACKEND_BUFFER_USAGE_ANY\n",
    "    };\n",
    "\n",
    "    return buffer;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p-pkzumMj-sQ",
   "metadata": {},
   "source": [
    "[`ggml_backend_cpu_buffer_from_ptr_type`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1996)\n",
    "\n",
    "```c++\n",
    "\n",
    "static ggml_backend_buffer_type_t ggml_backend_cpu_buffer_from_ptr_type(void) {\n",
    "    static struct ggml_backend_buffer_type ggml_backend_cpu_buffer_type = {\n",
    "        /* .iface   = */ {\n",
    "            /* .get_name         = */ ggml_backend_cpu_buffer_from_ptr_type_get_name,\n",
    "            /* .alloc_buffer     = */ ggml_backend_cpu_buffer_type_alloc_buffer,\n",
    "            /* .get_alignment    = */ ggml_backend_cpu_buffer_type_get_alignment,\n",
    "            /* .get_max_size     = */ NULL, // defaults to SIZE_MAX\n",
    "            /* .get_alloc_size   = */ NULL, // defaults to ggml_nbytes\n",
    "            /* .is_host          = */ ggml_backend_cpu_buffer_type_is_host,\n",
    "        },\n",
    "        /* .device  = */ NULL, // FIXME ggml_backend_reg_dev_get(ggml_backend_cpu_reg(), 0),\n",
    "        /* .context = */ NULL,\n",
    "    };\n",
    "\n",
    "    return &ggml_backend_cpu_buffer_type;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e547ea54",
   "metadata": {},
   "source": [
    "#### [`alloc_tensor_range`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L946)\n",
    "\n",
    "[`ggml_backend_alloc_ctx_tensors_from_buft`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L987)\n",
    "\n",
    "```c++\n",
    "ggml_backend_buffer_t buf = ggml_backend_alloc_ctx_tensors_from_buft(ctx, buft);\n",
    "----------\n",
    "ggml_backend_buffer_t ggml_backend_alloc_ctx_tensors_from_buft(struct ggml_context * ctx, ggml_backend_buffer_type_t buft) {\n",
    "    size_t alignment = ggml_backend_buft_get_alignment(buft);\n",
    "    size_t max_size = ggml_backend_buft_get_max_size(buft);\n",
    "    size_t cur_buf_size = 0;\n",
    "    struct ggml_tensor * first = ggml_get_first_tensor(ctx);\n",
    "    for (struct ggml_tensor * t = first; t != NULL; t = ggml_get_next_tensor(ctx, t))\n",
    "        size_t this_size = GGML_PAD(ggml_backend_buft_get_alloc_size(buft, t), alignment); --> ggml_nbytes(tensor)\n",
    "        cur_buf_size += this_size;\n",
    "    // allocate remaining tensors\n",
    "    --> alloc_tensor_range(ctx, first, NULL, buft, cur_buf_size, &ggml_backend_buffer_t * buffers, &size_t n_buffers);\n",
    "\n",
    "    ggml_backend_buffer_t buffer = buffers[0];\n",
    "    free(buffers);\n",
    "    return buffer;\n",
    "}\n",
    "\n",
    "static bool alloc_tensor_range(struct ggml_context * ctx,\n",
    "        struct ggml_tensor * first, struct ggml_tensor * last,\n",
    "        ggml_backend_buffer_type_t buft, size_t size,\n",
    "        ggml_backend_buffer_t ** buffers, size_t * n_buffers) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0e491",
   "metadata": {},
   "source": [
    "\n",
    "```c++\n",
    "// ggml_backend_buffer_t buffer = ggml_backend_buft_alloc_buffer(buft, size);\n",
    "--> ggml_backend_buft_alloc_buffer --> ggml_backend_buffer_t buffer;\n",
    "    ggml_backend_buffer_t ggml_backend_buft_alloc_buffer(ggml_backend_buffer_type_t buft, size_t size)\n",
    "    --> return buft->iface.alloc_buffer(buft, size);\n",
    "        static ggml_backend_buffer_t ggml_backend_cuda_buffer_type_alloc_buffer(ggml_backend_buffer_type_t buft, size_t size) {}\n",
    "*buffers = realloc(*buffers, sizeof(ggml_backend_buffer_t) * (*n_buffers + 1));\n",
    "(*buffers)[(*n_buffers)++] = buffer;\n",
    "--> ggml_tallocr_new --> struct ggml_tallocr tallocr;\n",
    "for (struct ggml_tensor * t = first; t != last; t = ggml_get_next_tensor(ctx, t))\n",
    "    --> ggml_tallocr_alloc;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b938d2b5",
   "metadata": {},
   "source": [
    "##### [`ggml_backend_cuda_buffer_type_alloc_buffer`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L680)\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_backend_cuda_buffer_context</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_cuda_buffer_context {\n",
    "    int device;\n",
    "    void * dev_ptr = nullptr;\n",
    "    std::string name;\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "ggml_backend_cuda_buffer_type_context * buft_ctx = (ggml_backend_cuda_buffer_type_context *)buft->context;\n",
    "ggml_cuda_set_device(buft_ctx->device);\n",
    "--> cudaError_t err = ggml_cuda_device_malloc(&void *dev_ptr, size, buft_ctx->device);\n",
    "ggml_backend_cuda_buffer_context * ctx = new ggml_backend_cuda_buffer_context(buft_ctx->device, dev_ptr);\n",
    "return ggml_backend_buffer_init(buft, ggml_backend_cuda_buffer_interface, ctx, size);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d3028",
   "metadata": {},
   "source": [
    "[`ggml_cuda_device_malloc`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L100)\n",
    "```c++\n",
    "static cudaError_t ggml_cuda_device_malloc(void ** ptr, size_t size, int device) {\n",
    "    ggml_cuda_set_device(device);\n",
    "    cudaError_t err;\n",
    "    if (getenv(\"GGML_CUDA_ENABLE_UNIFIED_MEMORY\") != nullptr) err = cudaMallocManaged(ptr, size);\n",
    "    else err = cudaMalloc(ptr, size);\n",
    "    return err;\n",
    "}\n",
    "```\n",
    "\n",
    "```log\n",
    ">>> p size\n",
    "$335 = 2463531264\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb50b9",
   "metadata": {},
   "source": [
    "##### [`ggml_tallocr_new`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L77)\n",
    "\n",
    "> tallocr 看起来就是buffer的管理器而已，因为base, align和offset都是靠buffer拿到的。应该tallocr只会管理着offset\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_tallocr</summary>\n",
    "\n",
    "```c++\n",
    "// Tensor allocator\n",
    "struct ggml_tallocr {\n",
    "    ggml_backend_buffer_t buffer;\n",
    "    void * base;\n",
    "    size_t alignment;\n",
    "    size_t offset;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "struct ggml_tallocr tallocr = ggml_tallocr_new(buffer);\n",
    "----------\n",
    "struct ggml_tallocr ggml_tallocr_new(ggml_backend_buffer_t buffer) {\n",
    "    void * base = ggml_backend_buffer_get_base(buffer);\n",
    "    size_t align = ggml_backend_buffer_get_alignment(buffer);\n",
    "\n",
    "    assert(align && !(align & (align - 1))); // power of 2\n",
    "\n",
    "    struct ggml_tallocr talloc = (struct ggml_tallocr) {\n",
    "        /*.buffer    = */ buffer,\n",
    "        /*.base      = */ base,\n",
    "        /*.alignment = */ align,\n",
    "        /*.offset    = */ aligned_offset(base, 0, align),\n",
    "    };\n",
    "    return talloc;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772629ab",
   "metadata": {},
   "source": [
    "##### [`ggml_tallocr_alloc`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L92)\n",
    "\n",
    "```c++\n",
    "status = ggml_tallocr_alloc(&tallocr, t);\n",
    "----------\n",
    "enum ggml_status ggml_tallocr_alloc(struct ggml_tallocr * talloc, struct ggml_tensor * tensor) {\n",
    "    size_t size = ggml_backend_buffer_get_alloc_size(talloc->buffer, tensor);\n",
    "    --> return ggml_backend_buft_get_alloc_size(ggml_backend_buffer_get_type(buffer), tensor); --> ggml_nbytes(tensor);\n",
    "\n",
    "    size = GGML_PAD(size, talloc->alignment);\n",
    "\n",
    "    void * addr = (char *)ggml_backend_buffer_get_base(talloc->buffer) + talloc->offset;\n",
    "    talloc->offset += size;\n",
    "\n",
    "    assert(((uintptr_t)addr % talloc->alignment) == 0);\n",
    "\n",
    "    return ggml_backend_tensor_alloc(talloc->buffer, tensor, addr);\n",
    "    ---> // 非常重要，是更进一步的标志！ *key*\n",
    "        tensor->buffer = buffer;\n",
    "        tensor->data = addr;\n",
    "        --> return ggml_backend_buffer_init_tensor(buffer, tensor);\n",
    "            static enum ggml_status ggml_backend_cuda_buffer_init_tensor(ggml_backend_buffer_t buffer, ggml_tensor * tensor) {}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5b47a",
   "metadata": {},
   "source": [
    "[`ggml_backend_cuda_buffer_init_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L578)\n",
    "\n",
    "```c++\n",
    "ggml_backend_cuda_buffer_context * ctx = (ggml_backend_cuda_buffer_context *)buffer->context;\n",
    "if (tensor->view_src != NULL) {\n",
    "    assert(tensor->view_src->buffer->buft == buffer->buft);\n",
    "    return GGML_STATUS_SUCCESS;\n",
    "}\n",
    "if (ggml_is_quantized(tensor->type) && tensor->view_src == nullptr && ggml_backend_buffer_get_usage(buffer) != GGML_BACKEND_BUFFER_USAGE_COMPUTE) {\n",
    "    // initialize padding to 0 to avoid possible NaN values\n",
    "    const size_t original_size = ggml_nbytes(tensor);\n",
    "    const size_t padded_size = ggml_backend_buft_get_alloc_size(buffer->buft, tensor);\n",
    "    if (padded_size > original_size) {\n",
    "        ggml_cuda_set_device(ctx->device);\n",
    "        cudaMemset((char *)tensor->data + original_size, 0, padded_size - original_size);\n",
    "    }\n",
    "}\n",
    "return GGML_STATUS_SUCCESS;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded1b402-3b72-4096-9f7a-961391ff4079",
   "metadata": {},
   "source": [
    "#### [`llama_model_loader::load_all_data`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L918)\n",
    "\n",
    "```c++\n",
    "ml.load_all_data(ctx, bufs, use_mlock ? &pimpl->mlock_mmaps : NULL, params.progress_callback, params.progress_callback_user_data);\n",
    "----------\n",
    "bool llama_model_loader::load_all_data(\n",
    "        struct ggml_context * ctx,\n",
    "        llama_buf_map & bufs,\n",
    "        llama_mlocks * lmlocks,\n",
    "        llama_progress_callback progress_callback,\n",
    "        void * progress_callback_user_data) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3dd224-27e8-467f-aad1-7901c3893ce9",
   "metadata": {},
   "source": [
    "```c++\n",
    "for (struct ggml_tensor * cur = ggml_get_first_tensor(ctx); cur != NULL; cur = ggml_get_next_tensor(ctx, cur))\n",
    "    const auto * weight = get_weight(ggml_get_name(cur));\n",
    "    size_t n_size = ggml_nbytes(cur);\n",
    "    const auto & mapping = mappings.at(weight->idx);\n",
    "    ggml_backend_buffer_t buf_mmap = bufs.at(weight->idx);\n",
    "    uint8_t * data = (uint8_t *) mapping->addr() + weight->offs;\n",
    "    // either we have a buffer to allocate the tensor in, or it is already allocated\n",
    "    GGML_ASSERT(buf_mmap || cur->data);\n",
    "    if (buf_mmap && cur->data == nullptr)\n",
    "        // CPU 侧的 tensor 的 buffer 和 data 在这儿赋值的\n",
    "        --> ggml_backend_tensor_alloc(buffer=buf_mmap, tensor=cur, addr=data);\n",
    "            tensor->buffer = buffer;\n",
    "            tensor->data = addr;\n",
    "            // 下面的CPU没有提供函数指针，所以不起作用\n",
    "            // return ggml_backend_buffer_init_tensor(buffer, tensor);\n",
    "        auto & mmap_used = mmaps_used[weight->idx];\n",
    "        mmap_used.first  = std::min(mmap_used.first,  weight->offs);\n",
    "        mmap_used.second = std::max(mmap_used.second, weight->offs + n_size);\n",
    "    else // *key* 非常关键的一步，将权重拷贝到GPU显存中\n",
    "        --> ggml_backend_tensor_set(cur, data, 0, n_size);\n",
    "            ggml_backend_buffer_t buf = tensor->view_src ? tensor->view_src->buffer : tensor->buffer;\n",
    "            --> buf->iface.set_tensor(buf, tensor, data, offset, size) --> ggml_backend_cuda_buffer_set_tensor;\n",
    "    size_done += n_size;\n",
    "// check if this is the last call and do final cleanup\n",
    "for (uint32_t idx = 0; idx < mappings.size(); idx++)\n",
    "    const auto & mmap_used = mmaps_used.at(idx);\n",
    "    auto & mapping = mappings.at(idx);\n",
    "    --> mapping->unmap_fragment(0, mmap_used.first);\n",
    "    --> if (mmap_used.second != 0) mapping->unmap_fragment(mmap_used.second, mapping->size());\n",
    "return true;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40142326-2ae9-489a-addc-70ddb31e281b",
   "metadata": {},
   "source": [
    "[`llama_mmap::impl::unmap_fragment`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-mmap.cpp#L321)\n",
    "\n",
    "```c++\n",
    "void llama_mmap::unmap_fragment(size_t first, size_t last) { pimpl->unmap_fragment(first, last); }\n",
    "\n",
    "struct llama_mmap::impl {\n",
    "    void unmap_fragment(size_t first, size_t last) {\n",
    "        int page_size = sysconf(_SC_PAGESIZE);\n",
    "        align_range(&first, &last, page_size);\n",
    "        size_t len = last - first;\n",
    "        void * next_page_start = (uint8_t *) addr + first;\n",
    "        munmap(next_page_start, len);\n",
    "        for (const auto & frag : mapped_fragments) new_mapped_fragments.emplace_back(last, frag.second);\n",
    "        mapped_fragments = std::move(new_mapped_fragments);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635e210-5a12-4060-b2ef-1aae86fe9b14",
   "metadata": {},
   "source": [
    "## [`llama_context::llama_context`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L19)\n",
    "\n",
    "```c++\n",
    "llama_context * lctx = llama_init_from_model(model, cparams);\n",
    "----------\n",
    "llama_context * llama_init_from_model(\n",
    "                 llama_model * model,\n",
    "        llama_context_params   params) {\n",
    "    --> auto * ctx = new llama_context(*model, params);\n",
    "    return ctx;\n",
    "}\n",
    "\n",
    "llama_context::llama_context(\n",
    "        const llama_model & model,\n",
    "              llama_context_params params) :\n",
    "    model(model),\n",
    "    batch_allocr(std::make_unique<llama_batch_allocr>()) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42f04f-0568-431f-940f-bf7e71945255",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct llama_context</summary>\n",
    "\n",
    "```c++\n",
    "struct llama_context {\n",
    "private:\n",
    "    const llama_model & model;\n",
    "\n",
    "    llama_cparams       cparams;\n",
    "\n",
    "    std::unique_ptr<llama_memory_i> memory;\n",
    "\n",
    "    // TODO: temporary, until the llama_kv_self_defrag() API is removed\n",
    "    bool memory_force_optimize = false;\n",
    "\n",
    "    // decode output (2-dimensional array: [n_outputs][n_vocab])\n",
    "    size_t  logits_size = 0; // capacity (of floats) for logits\n",
    "    float * logits      = nullptr;\n",
    "\n",
    "    // reuse the batch_allocr to avoid unnecessary memory allocations\n",
    "    std::unique_ptr<llama_batch_allocr> batch_allocr;\n",
    "\n",
    "    int32_t n_outputs     = 0; // number of actually-used outputs in the current ubatch or last logical batch\n",
    "    int32_t n_outputs_max = 0; // capacity (of tokens positions) for the output buffers\n",
    "\n",
    "    std::vector<int32_t> output_ids; // map batch token positions to ids of the logits and embd buffers\n",
    "\n",
    "    ggml_backend_sched_ptr sched;\n",
    "\n",
    "    ggml_backend_t backend_cpu = nullptr;\n",
    "    std::vector<ggml_backend_ptr> backends;\n",
    "\n",
    "    ggml_context_ptr ctx_compute;\n",
    "\n",
    "    // training\n",
    "    ggml_opt_context_t opt_ctx = nullptr;\n",
    "\n",
    "    ggml_threadpool_t threadpool       = nullptr;\n",
    "    ggml_threadpool_t threadpool_batch = nullptr;\n",
    "\n",
    "    ggml_abort_callback abort_callback      = nullptr;\n",
    "    void *              abort_callback_data = nullptr;\n",
    "\n",
    "    std::vector<std::pair<ggml_backend_t, ggml_backend_set_n_threads_t>> set_n_threads_fns;\n",
    "\n",
    "    // buffer types used for the compute buffer of each backend\n",
    "    std::vector<ggml_backend_t>             backend_ptrs;\n",
    "    std::vector<ggml_backend_buffer_type_t> backend_buft;\n",
    "\n",
    "    // memory buffers used to evaluate the model\n",
    "    std::vector<uint8_t> buf_compute_meta;\n",
    "\n",
    "    // host buffer for the model output (logits and embeddings)\n",
    "    ggml_backend_buffer_ptr buf_output;\n",
    "\n",
    "    bool has_evaluated_once = false;\n",
    "\n",
    "    // perf\n",
    "    mutable int64_t t_start_us  = 0;\n",
    "    mutable int64_t t_load_us   = 0;\n",
    "    mutable int64_t t_p_eval_us = 0;\n",
    "    mutable int64_t t_eval_us   = 0;\n",
    "\n",
    "    mutable int64_t t_compute_start_us = 0;\n",
    "    mutable int64_t n_queued_tokens    = 0;\n",
    "\n",
    "    mutable int32_t n_p_eval = 0; // number of tokens in eval calls for the prompt (with batch size > 1)\n",
    "    mutable int32_t n_eval   = 0; // number of eval calls\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_backend/ggml_backend_t</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend {\n",
    "    ggml_guid_t guid;\n",
    "    struct ggml_backend_i iface;\n",
    "    ggml_backend_dev_t device;\n",
    "    void * context;\n",
    "};\n",
    "typedef struct ggml_backend * ggml_backend_t;\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "// GPU backends\n",
    "for (auto * dev : model.devices) llama_context:: std::vector<ggml_backend_ptr> backends.emplace_back(\n",
    "    --> ggml_backend_dev_init(dev, nullptr));\n",
    "    static ggml_backend_t ggml_backend_cuda_device_init_backend(ggml_backend_dev_t dev, const char * params)\n",
    "        --> ggml_backend_cuda_init;\n",
    "// add ACCEL backends (such as BLAS)\n",
    "// add CPU backend\n",
    "--> ggml_backend_init_by_type --> ggml_backend_dev_init --> ggml_backend_cpu_device_init_backend --> ggml_backend_cpu_init --> llama_context:: ggml_backend_t backend_cpu;\n",
    "backends.emplace_back(backend_cpu);\n",
    "// create a list of the set_n_threads functions in the backends\n",
    "//for (auto & backend : backends)\n",
    "auto ggml_backend_set_n_threads_fn = (ggml_backend_set_n_threads_t) ggml_backend_reg_get_proc_address(ggml_backend_dev_backend_reg(ggml_backend_get_device(backend.get())), \"ggml_backend_set_n_threads\");\n",
    "llama_context:: std::vector<std::pair<ggml_backend_t, ggml_backend_set_n_threads_t>> set_n_threads_fns.emplace_back(backend.get(), ggml_backend_set_n_threads_fn);\n",
    "// graph outputs buffer\n",
    "// resized during inference when a batch uses more outputs\n",
    "--> llama_context::output_reserve --> llama_context:: logits_size, output_ids, buf_output, logits, n_outputs=0;\n",
    "// init the memory module\n",
    "llama_memory_params params_mem = {\n",
    "    /*.type_k   =*/ params.type_k,\n",
    "    /*.type_v   =*/ params.type_v,\n",
    "    /*.swa_full =*/ params.swa_full,\n",
    "};\n",
    "--> llama_model::create_memory --> llama_kv_cache_unified::llama_kv_cache_unified --> llama_context:: std::unique_ptr<llama_memory_i> memory;\n",
    "// init backends\n",
    "llama_context:: std::vector<ggml_backend_t> backend_buft.clear();\n",
    "llama_context:: std::vector<ggml_backend_buffer_type_t> backend_ptrs.clear();\n",
    "for (auto & backend : backends)\n",
    "    auto * buft = ggml_backend_get_default_buffer_type(backend.get());\n",
    "    auto backend_type = ggml_backend_dev_type(ggml_backend_get_device(backend.get()));\n",
    "    if (backend_type == GGML_BACKEND_DEVICE_TYPE_CPU && !model.devices.empty()) {\n",
    "        // use the host buffer of the first device CPU for faster transfer of the intermediate state\n",
    "        auto * dev = model.devices[0];\n",
    "        auto * host_buft = ggml_backend_dev_host_buffer_type(dev);\n",
    "        if (host_buft) buft = host_buft;\n",
    "    }\n",
    "    // buffer types used for the compute buffer of each backend\n",
    "    backend_buft.push_back(buft);\n",
    "    backend_ptrs.push_back(backend.get());\n",
    "// memory buffers used to evaluate the model\n",
    "// buffer used to store the computation graph and the tensor meta data\n",
    "const size_t max_nodes = this->graph_max_nodes();\n",
    "--> ggml_graph_nbytes --> llama_context:: std::vector<uint8_t> buf_compute_meta.resize(ggml_tensor_overhead()*max_nodes + ggml_graph_overhead_custom(max_nodes, false));\n",
    "--> ggml_backend_sched_new --> llama_context:: ggml_backend_sched_ptr sched.reset(ggml_backend_sched_new(backend_ptrs.data(), backend_buft.data(), backend_ptrs.size(), max_nodes, pipeline_parallel, cparams.op_offload));\n",
    "// reserve worst-case graph\n",
    "// simulate full KV cache\n",
    "// const auto mctx = memory->init_full();\n",
    "--> llama_kv_cache_unified_state::llama_kv_cache_unified_state --> std::make_unique<llama_kv_cache_unified_context> mctx;\n",
    "const uint32_t n_seqs = cparams.n_seq_max;\n",
    "const uint32_t n_tokens = std::min(cparams.n_ctx, cparams.n_ubatch);\n",
    "// reserve pp graph first so that buffers are only allocated once\n",
    "auto * gf   = graph_reserve(n_tokens, n_seqs, n_tokens, mctx.get());\n",
    "n_splits_pp = ggml_backend_sched_get_n_splits(sched.get());\n",
    "n_nodes_pp  = ggml_graph_n_nodes(gf);\n",
    "// reserve with tg graph to get the number of splits and nodes\n",
    "auto * gf   = graph_reserve(1, 1, 1, mctx.get());\n",
    "n_splits_tg = ggml_backend_sched_get_n_splits(sched.get());\n",
    "n_nodes_tg  = ggml_graph_n_nodes(gf);\n",
    "// reserve again with pp graph to avoid ggml-alloc reallocations during inference\n",
    "auto * gf   = graph_reserve(n_tokens, n_seqs, n_tokens, mctx.get());\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da65199c",
   "metadata": {},
   "source": [
    "### [`ggml_backend_cuda_init`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L3545)\n",
    "\n",
    "```c++\n",
    "return ggml_backend_cuda_init(ctx->device);\n",
    "----------\n",
    "ggml_backend_t ggml_backend_cuda_init(int device) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfceb9d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_backend_cuda_context</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_cuda_context {\n",
    "    int device;\n",
    "    std::string name;\n",
    "    cudaEvent_t copy_event = nullptr;\n",
    "\n",
    "    cudaStream_t streams[GGML_CUDA_MAX_DEVICES][GGML_CUDA_MAX_STREAMS] = { { nullptr } };\n",
    "    cublasHandle_t cublas_handles[GGML_CUDA_MAX_DEVICES] = {nullptr};\n",
    "\n",
    "    std::unique_ptr<ggml_cuda_graph> cuda_graph;\n",
    "\n",
    "    explicit ggml_backend_cuda_context(int device) :\n",
    "        device(device),\n",
    "        name(GGML_CUDA_NAME + std::to_string(device)) {\n",
    "    }\n",
    "\n",
    "    ~ggml_backend_cuda_context();\n",
    "\n",
    "    cudaStream_t stream(int device, int stream) {\n",
    "        if (streams[device][stream] == nullptr) {\n",
    "            ggml_cuda_set_device(device);\n",
    "            CUDA_CHECK(cudaStreamCreateWithFlags(&streams[device][stream], cudaStreamNonBlocking));\n",
    "        }\n",
    "        return streams[device][stream];\n",
    "    }\n",
    "\n",
    "    cudaStream_t stream() {\n",
    "        return stream(device, 0);\n",
    "    }\n",
    "\n",
    "    cublasHandle_t cublas_handle(int device) {\n",
    "        if (cublas_handles[device] == nullptr) {\n",
    "            ggml_cuda_set_device(device);\n",
    "            CUBLAS_CHECK(cublasCreate(&cublas_handles[device]));\n",
    "            CUBLAS_CHECK(cublasSetMathMode(cublas_handles[device], CUBLAS_TF32_TENSOR_OP_MATH));\n",
    "        }\n",
    "        return cublas_handles[device];\n",
    "    }\n",
    "\n",
    "    cublasHandle_t cublas_handle() {\n",
    "        return cublas_handle(device);\n",
    "    }\n",
    "\n",
    "    // pool\n",
    "    std::unique_ptr<ggml_cuda_pool> pools[GGML_CUDA_MAX_DEVICES];\n",
    "\n",
    "    static std::unique_ptr<ggml_cuda_pool> new_pool_for_device(int device);\n",
    "\n",
    "    ggml_cuda_pool & pool(int device) {\n",
    "        if (pools[device] == nullptr) {\n",
    "            pools[device] = new_pool_for_device(device);\n",
    "        }\n",
    "        return *pools[device];\n",
    "    }\n",
    "\n",
    "    ggml_cuda_pool & pool() {\n",
    "        return pool(device);\n",
    "    }\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "ggml_backend_cuda_context * ctx = new ggml_backend_cuda_context(device);\n",
    "ggml_backend_t cuda_backend = new ggml_backend {\n",
    "    /* .guid      = */ ggml_backend_cuda_guid(),\n",
    "    /* .interface = */ ggml_backend_cuda_interface,\n",
    "    /* .device    = */ ggml_backend_reg_dev_get(ggml_backend_cuda_reg(), device),\n",
    "    /* .context   = */ ctx,\n",
    "};\n",
    "return cuda_backend;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844a666-be63-4d77-bfde-6ee25055d585",
   "metadata": {},
   "source": [
    "### [`ggml_backend_cpu_init`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.cpp#L196)\n",
    "\n",
    "```c++\n",
    "backend_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);\n",
    "----------\n",
    "ggml_backend_t ggml_backend_init_by_type(enum ggml_backend_dev_type type, const char * params) {\n",
    "    ggml_backend_dev_t dev = ggml_backend_dev_by_type(type);\n",
    "    if (!dev) {\n",
    "        return nullptr;\n",
    "    }\n",
    "    --> return ggml_backend_dev_init(dev, params);\n",
    "}\n",
    "\n",
    "\n",
    "ggml_backend_dev_t ggml_backend_dev_by_type(enum ggml_backend_dev_type type)\n",
    "    for (size_t i = 0; i < ggml_backend_dev_count(); i++) {\n",
    "        ggml_backend_dev_t dev = ggml_backend_dev_get(i);\n",
    "        if (ggml_backend_dev_type(dev) == type) {\n",
    "            return dev;\n",
    "        }\n",
    "    }\n",
    "\n",
    "ggml_backend_t ggml_backend_dev_init(ggml_backend_dev_t device, const char * params)\n",
    "    --> return device->iface.init_backend(device, params);\n",
    "\n",
    "static ggml_backend_t ggml_backend_cpu_device_init_backend(ggml_backend_dev_t dev, const char * params)\n",
    "    --> return ggml_backend_cpu_init();\n",
    "\n",
    "ggml_backend_t ggml_backend_cpu_init(void) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198c8f7-2d5f-4d39-928d-122e8bb7b8f2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_backend_cpu_context</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_cpu_context {\n",
    "    int                 n_threads;\n",
    "    ggml_threadpool_t   threadpool;\n",
    "\n",
    "    uint8_t *           work_data;\n",
    "    size_t              work_size;\n",
    "\n",
    "    ggml_abort_callback abort_callback;\n",
    "    void *              abort_callback_data;\n",
    "};\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n",
    "```c++\n",
    "// initialize CPU backend now to avoid slowing the first graph computation\n",
    "ggml_cpu_init();\n",
    "struct ggml_backend_cpu_context * ctx = new ggml_backend_cpu_context;\n",
    "ctx->n_threads           = GGML_DEFAULT_N_THREADS;\n",
    "ctx->threadpool          = NULL;\n",
    "ctx->work_data           = NULL;\n",
    "ctx->work_size           = 0;\n",
    "ctx->abort_callback      = NULL;\n",
    "ctx->abort_callback_data = NULL;\n",
    "ggml_backend_t cpu_backend = new ggml_backend {\n",
    "    /* .guid      = */ ggml_backend_cpu_guid(),\n",
    "    /* .interface = */ ggml_backend_cpu_i,\n",
    "    /* .device    = */ ggml_backend_reg_dev_get(ggml_backend_cpu_reg(), 0),\n",
    "    /* .context   = */ ctx,\n",
    "};\n",
    "return cpu_backend;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f065a-2f96-4902-8503-06b7e5d3ed02",
   "metadata": {},
   "source": [
    "### [`llama_context::output_reserve`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L1239)\n",
    "\n",
    "```c++\n",
    "output_reserve(params.n_seq_max);\n",
    "----------\n",
    "int32_t llama_context::output_reserve(int32_t n_outputs) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbf15b-b5f0-4e74-9322-0f72dcf2c072",
   "metadata": {},
   "source": [
    "```c++\n",
    "const int64_t n_outputs_max = std::max<int64_t>(n_outputs, n_seq_max());\n",
    "logits_size = n_vocab*n_outputs_max;\n",
    "// map batch token positions to ids of the logits and embd buffers\n",
    "// init, never resized afterwards\n",
    "llama_context:: std::vector<int32_t> output_ids.resize(n_batch);\n",
    "const size_t new_size  = (logits_size + embd_size) * sizeof(float);\n",
    "// try to use the host buffer of the device where the output tensor is allocated for faster transfer to system memory\n",
    "auto * buft = ggml_backend_dev_host_buffer_type(model.dev_output());\n",
    "// host buffer for the model output (logits and embeddings)\n",
    "//buf_output.reset(ggml_backend_buft_alloc_buffer(buft, new_size));\n",
    "--> ggml_backend_buft_alloc_buffer\n",
    "    --> ggml_backend_cpu_buffer_type_alloc_buffer\n",
    "    --> ggml_backend_cuda_host_buffer_type_alloc_buffer // only if ngl > n_layers\n",
    "        --> llama_context:: ggml_backend_buffer_ptr buf_output;\n",
    "--> ggml_backend_buffer_get_base --> ggml_backend_cpu_buffer_get_base --> output_base;\n",
    "llama_context:: float * logits = output_base;\n",
    "// set all ids as invalid (negative)\n",
    "std::fill(output_ids.begin(), output_ids.end(), -1);\n",
    "this->n_outputs = 0;\n",
    "return n_outputs_max;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477f94d-3156-4dc7-ac70-7d921e4a3f2d",
   "metadata": {},
   "source": [
    "#### [`ggml_backend_cpu_buffer_type_alloc_buffer`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1950)\n",
    "\n",
    "```c++\n",
    "buf_output.reset(ggml_backend_buft_alloc_buffer(buft, new_size));\n",
    "----------\n",
    "static ggml_backend_buffer_t ggml_backend_cpu_buffer_type_alloc_buffer(ggml_backend_buffer_type_t buft, size_t size) {\n",
    "    void * data = ggml_aligned_malloc(size);\n",
    "        const int alignment = 64;\n",
    "        void * aligned_memory = NULL;\n",
    "        int result = posix_memalign(&aligned_memory, alignment, size);\n",
    "        return aligned_memory;\n",
    "    return ggml_backend_buffer_init(buft, ggml_backend_cpu_buffer_i, data, size);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b8f969",
   "metadata": {},
   "source": [
    "#### [`ggml_backend_cuda_host_buffer_type_alloc_buffer`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L1123)\n",
    "\n",
    "```c++\n",
    "buf_output.reset(ggml_backend_buft_alloc_buffer(buft, new_size));\n",
    "----------\n",
    "static ggml_backend_buffer_t ggml_backend_cuda_host_buffer_type_alloc_buffer(ggml_backend_buffer_type_t buft, size_t size) {\n",
    "    --> void * ptr = ggml_cuda_host_malloc(size);\n",
    "        void * ptr = nullptr;\n",
    "        cudaError_t err = cudaMallocHost((void **) &ptr, size);\n",
    "        return ptr;\n",
    "    ggml_backend_buffer_t buffer = ggml_backend_cpu_buffer_from_ptr(ptr, size);\n",
    "    buffer->buft = buft;\n",
    "    buffer->iface.free_buffer = ggml_backend_cuda_host_buffer_free_buffer;\n",
    "    return buffer;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b348dedc-0e61-4c11-91bf-a825ae29e133",
   "metadata": {},
   "source": [
    "#### [`ggml_backend_cpu_buffer_get_base`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1869)\n",
    "\n",
    "```c++\n",
    "float * output_base = (float *) ggml_backend_buffer_get_base(buf_output.get());\n",
    "----------\n",
    "void * ggml_backend_buffer_get_base(ggml_backend_buffer_t buffer) {\n",
    "    --> return buffer->iface.get_base(buffer);\n",
    "}\n",
    "\n",
    "static void * ggml_backend_cpu_buffer_get_base(ggml_backend_buffer_t buffer) {\n",
    "    uintptr_t data = (uintptr_t)buffer->context;\n",
    "    // align the buffer\n",
    "    if (data % TENSOR_ALIGNMENT != 0) data = GGML_PAD(data, TENSOR_ALIGNMENT);\n",
    "    return (void *)data;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c21825-1ade-484e-a922-f1f009a36f1b",
   "metadata": {},
   "source": [
    "### [`llama_kv_cache_unified::llama_kv_cache_unified`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L19)\n",
    "\n",
    "[`llama_model::create_memory`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#L13751)\n",
    "\n",
    "```c++\n",
    "memory.reset(model.create_memory(params_mem, cparams));\n",
    "----------\n",
    "llama_memory_i * llama_model::create_memory(const llama_memory_params & params, llama_cparams & cparams) const {\n",
    "    llama_memory_i * res;\n",
    "    switch (arch) {\n",
    "        const auto padding = llama_kv_cache_unified::get_padding(cparams);\n",
    "            --> // the FA kernels require padding to avoid extra runtime boundary checks\n",
    "            return cparams.flash_attn ? 256u : 32u;\n",
    "        cparams.n_ctx = GGML_PAD(cparams.n_ctx, padding);\n",
    "        --> res = new llama_kv_cache_unified(\n",
    "                            *this,\n",
    "                            nullptr,\n",
    "                            params.type_k,\n",
    "                            params.type_v,\n",
    "                            !cparams.flash_attn,\n",
    "                            cparams.offload_kqv,\n",
    "                            kv_size=cparams.n_ctx,\n",
    "                            cparams.n_seq_max,\n",
    "                            padding,\n",
    "                            hparams.n_swa,\n",
    "                            hparams.swa_type);\n",
    "    }\n",
    "}\n",
    "\n",
    "llama_kv_cache_unified::llama_kv_cache_unified(\n",
    "        const llama_model &  model,\n",
    "          layer_filter_cb && filter,\n",
    "                ggml_type    type_k,\n",
    "                ggml_type    type_v,\n",
    "                     bool    v_trans,\n",
    "                     bool    offload,\n",
    "                 uint32_t    kv_size,\n",
    "                 uint32_t    n_seq_max,\n",
    "                 uint32_t    n_pad,\n",
    "                 uint32_t    n_swa,\n",
    "           llama_swa_type    swa_type) :\n",
    "    model(model), hparams(model.hparams), v_trans(v_trans),\n",
    "    n_seq_max(n_seq_max), n_pad(n_pad), n_swa(n_swa), swa_type(swa_type) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CLwdfZ0uodJv",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>class llama_kv_cache_unified</summary>\n",
    "\n",
    "```c++\n",
    "class llama_kv_cache_unified : public llama_memory_i {\n",
    "    using ubatch_heads = std::vector<uint32_t>;\n",
    "private:\n",
    "    const llama_model & model;\n",
    "    const llama_hparams & hparams;\n",
    "\n",
    "    struct kv_layer {\n",
    "        // layer index in the model\n",
    "        // note: can be different from the layer index in the KV cache\n",
    "        uint32_t il;\n",
    "\n",
    "        ggml_tensor * k;\n",
    "        ggml_tensor * v;\n",
    "    };\n",
    "\n",
    "    bool v_trans = true;  // the value tensor is transposed\n",
    "\n",
    "    // the current index from where we start searching for a free slot in the ring buffer of KV cells (see find_slot())\n",
    "    // note: this is not part of the KV state and it's only used to speed-up the find_slot() method\n",
    "    uint32_t head = 0;\n",
    "\n",
    "    const uint32_t n_seq_max = 1;\n",
    "\n",
    "    // required padding\n",
    "\n",
    "    const llama_swa_type swa_type = LLAMA_SWA_TYPE_NONE;\n",
    "\n",
    "    std::vector<ggml_context_ptr>        ctxs;\n",
    "    std::vector<ggml_backend_buffer_ptr> bufs;\n",
    "\n",
    "    llama_kv_cells_unified cells;\n",
    "\n",
    "    std::vector<kv_layer> layers;\n",
    "\n",
    "    // model layer id -> KV cache layer id\n",
    "    std::unordered_map<int32_t, int32_t> map_layer_ids;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>class llama_kv_cells_unified</summary>\n",
    "\n",
    "```c++\n",
    "class llama_kv_cells_unified {\n",
    "private:\n",
    "    bool has_shift = false;\n",
    "\n",
    "    // set of indices of used cells (i.e. pos[i] != -1, allowed to not have any seq_id)\n",
    "    std::set<uint32_t> used;\n",
    "\n",
    "    std::vector<llama_pos> pos;\n",
    "\n",
    "    // this array accumulates any applied shifts to the pos array since the last reset_shift() call\n",
    "    // this is used to queue multiple updates to the pos array, which in the end can be applied in one go:\n",
    "    //\n",
    "    //   cells.pos_add(x, shift_x);\n",
    "    //   cells.pos_div(y, shift_y);\n",
    "    //   ...\n",
    "    //\n",
    "    //   if (cells.has_shift()) {\n",
    "    //      for (int i = 0; i < n; ++i) {\n",
    "    //          auto shift_i = cells.get_shift(i);\n",
    "    //          ...\n",
    "    //      }\n",
    "    //      cells.reset_shift();\n",
    "    //   }\n",
    "    //\n",
    "    std::vector<llama_pos> shift;\n",
    "\n",
    "    using bits_t = std::bitset<LLAMA_MAX_SEQ>;\n",
    "\n",
    "    // the bitset seq[i] tells us which sequences are currently occupying the i-th cell\n",
    "    std::vector<bits_t> seq;\n",
    "\n",
    "    // the set seq_pos[s] tells us which positions are currently present for sequence s\n",
    "    // this way seq_pos[s].begin() and seq_pos[s].rbegin() give us the min/max positions currently in the cache\n",
    "    std::set<llama_pos> seq_pos[LLAMA_MAX_SEQ];\n",
    "};\n",
    "\n",
    "    void reset() {\n",
    "        for (uint32_t i = 0; i < pos.size(); ++i) {\n",
    "            pos[i]   = -1;\n",
    "            shift[i] =  0;\n",
    "            seq[i].reset();\n",
    "        }\n",
    "\n",
    "        has_shift = false;\n",
    "\n",
    "        used.clear();\n",
    "\n",
    "        for (uint32_t s = 0; s < LLAMA_MAX_SEQ; ++s) {\n",
    "            seq_pos[s].clear();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    void resize(uint32_t n) {\n",
    "        pos.resize(n);\n",
    "        shift.resize(n);\n",
    "        seq.resize(n);\n",
    "\n",
    "        reset();\n",
    "    }\n",
    "\n",
    "// copy the state of cells [i, i + n) (used for save/restore the state of the cells)\n",
    "llama_kv_cells_unified cp(uint32_t i, uint32_t n) const {\n",
    "    llama_kv_cells_unified res;\n",
    "    res.resize(n);\n",
    "    for (uint32_t j = 0; j < n; ++j) {\n",
    "        res.pos[j] = pos[i + j];\n",
    "        res.seq[j] = seq[i + j];\n",
    "    }\n",
    "    return res;\n",
    "}\n",
    "\n",
    "// set the state of cells [i, i + other.pos.size()) (used for save/restore the state of the cells)\n",
    "void set(uint32_t i, const llama_kv_cells_unified & other) {\n",
    "    for (uint32_t j = 0; j < other.pos.size(); ++j) {\n",
    "        if (pos[i + j] == -1 && other.pos[j] != -1) {\n",
    "            used.insert(i + j);\n",
    "        }\n",
    "\n",
    "        if (pos[i + j] != -1 && other.pos[j] == -1) {\n",
    "            used.erase(i + j);\n",
    "        }\n",
    "\n",
    "        if (pos[i + j] != -1) {\n",
    "            seq_pos_rm(i + j);\n",
    "        }\n",
    "\n",
    "        pos[i + j] = other.pos[j];\n",
    "        seq[i + j] = other.seq[j];\n",
    "\n",
    "        if (pos[i + j] != -1) {\n",
    "            seq_pos_add(i + j);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "    // remove cell i\n",
    "    void seq_pos_rm(uint32_t i) {\n",
    "        for (int s = 0; s < LLAMA_MAX_SEQ; ++s) {\n",
    "            if (seq[i].test(s)) {\n",
    "                seq_pos[s].erase(pos[i]);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // add cell i\n",
    "    void seq_pos_add(uint32_t i) {\n",
    "        for (int s = 0; s < LLAMA_MAX_SEQ; ++s) {\n",
    "            if (seq[i].test(s)) {\n",
    "                seq_pos[s].insert(pos[i]);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // set the position of an empty cell\n",
    "    // does not modify \"has_shift\"\n",
    "    // note: call only if the cell is empty\n",
    "    void pos_set(uint32_t i, llama_pos p) {\n",
    "        pos[i] = p;\n",
    "        used.insert(i);\n",
    "    }\n",
    "\n",
    "    // note: call only if the cell is not empty and the seq_id is not in the cell\n",
    "    void seq_add(uint32_t i, llama_seq_id seq_id) {\n",
    "        seq[i].set(seq_id);\n",
    "        seq_pos[seq_id].insert(pos[i]);\n",
    "    }\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "// create a context for each buffer type\n",
    "std::map<ggml_backend_buffer_type_t, ggml_context *> ctx_map;\n",
    "auto ctx_for_buft = [&](ggml_backend_buffer_type_t buft) -> ggml_context * {\n",
    "    auto it = ctx_map.find(buft);\n",
    "    if (it == ctx_map.end()) {\n",
    "        ggml_init_params params = {\n",
    "            /*.mem_size   =*/ size_t(2u*hparams.n_layer*ggml_tensor_overhead()),\n",
    "            /*.mem_buffer =*/ NULL,\n",
    "            /*.no_alloc   =*/ true,\n",
    "        };\n",
    "        ggml_context * ctx = ggml_init(params);\n",
    "        ctx_map[buft] = ctx;\n",
    "        ctxs.emplace_back(ctx); //上下文的信息就写在这里了啊 *key*\n",
    "        return ctx;\n",
    "    }\n",
    "    return it->second;\n",
    "};\n",
    "\n",
    "head = 0;\n",
    "// llama_kv_cache_unified:: llama_kv_cells_unified cells;\n",
    "cells.resize(kv_size);\n",
    "for (uint32_t il = 0; il < hparams.n_layer; il++)\n",
    "    const uint32_t n_embd_k_gqa = hparams.n_embd_k_gqa(il) + hparams.n_embd_k_s();\n",
    "    const uint32_t n_embd_v_gqa = hparams.n_embd_v_gqa(il) + hparams.n_embd_v_s();\n",
    "    auto * dev = model.dev_layer(il);\n",
    "    buft = ggml_backend_dev_buffer_type(dev);\n",
    "    dev_name = ggml_backend_dev_name(dev);\n",
    "    ggml_context * ctx = ctx_for_buft(buft);\n",
    "    ggml_tensor * k = ggml_new_tensor_2d(ctx, type_k, n_embd_k_gqa, kv_size);\n",
    "    ggml_tensor * v = ggml_new_tensor_2d(ctx, type_v, n_embd_v_gqa, kv_size);\n",
    "    ggml_format_name(k, \"cache_k_l%d\", il);\n",
    "    ggml_format_name(v, \"cache_v_l%d\", il);\n",
    "    // model layer id -> KV cache layer id\n",
    "    // llama_kv_cache_unified:: std::unordered_map<int32_t, int32_t> map_layer_ids;\n",
    "    map_layer_ids[il] = layers.size();\n",
    "    // llama_kv_cache_unified:: std::vector<kv_layer> layers;\n",
    "    layers.push_back({ il, k, v });\n",
    "\n",
    "// allocate tensors and initialize the buffers to avoid NaNs in the padding\n",
    "for (auto it : ctx_map) {\n",
    "    auto * buft = ctx_map[0].first;\n",
    "    auto * ctx  = ctx_map[0].second;\n",
    "    --> ggml_backend_alloc_ctx_tensors_from_buft -> alloc_tensor_range --> ggml_backend_buffer_t buf;\n",
    "    --> ggml_backend_buffer_clear(buf, 0);\n",
    "        --> static void ggml_backend_cpu_buffer_clear(ggml_backend_buffer_t buffer, uint8_t value)\n",
    "            memset(buffer->context, value, buffer->size);\n",
    "        --> static void ggml_backend_cuda_buffer_clear(ggml_backend_buffer_t buffer, uint8_t value)\n",
    "            ggml_backend_cuda_buffer_context * ctx = (ggml_backend_cuda_buffer_context *)buffer->context;\n",
    "            ggml_cuda_set_device(ctx->device);\n",
    "            cudaMemsetAsync(ctx->dev_ptr, value, buffer->size, cudaStreamPerThread);\n",
    "            cudaStreamSynchronize(cudaStreamPerThread);\n",
    "\n",
    "    llama_kv_cache_unified:: std::vector<ggml_backend_buffer_ptr> bufs.emplace_back(buf);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7da1ea-1d80-4200-8a17-00d86ace2407",
   "metadata": {},
   "source": [
    "### [`ggml_graph_nbytes`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L5961)\n",
    "\n",
    "```c++\n",
    "buf_compute_meta.resize(ggml_tensor_overhead()*max_nodes + ggml_graph_overhead_custom(max_nodes, false));\n",
    "----------\n",
    "size_t ggml_graph_overhead_custom(size_t size, bool grads) {\n",
    "    --> return GGML_OBJECT_SIZE + GGML_PAD(ggml_graph_nbytes(size, grads), GGML_MEM_ALIGN);\n",
    "}\n",
    "\n",
    "static size_t ggml_graph_nbytes(size_t size, bool grads) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4298b9-4dbc-4a12-9752-2297cb732f4c",
   "metadata": {},
   "source": [
    "```c++\n",
    "size_t hash_size = ggml_hash_size(size * 2);\n",
    "void * p = 0;\n",
    "incr_ptr_aligned(&p, sizeof(struct ggml_cgraph), 1);\n",
    "incr_ptr_aligned(&p, size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)); // nodes\n",
    "incr_ptr_aligned(&p, size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)); // leafs\n",
    "incr_ptr_aligned(&p, hash_size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)); // hash keys\n",
    "incr_ptr_aligned(&p, ggml_bitset_size(hash_size) * sizeof(ggml_bitset_t), sizeof(ggml_bitset_t));\n",
    "\n",
    "size_t nbytes = (size_t) p;\n",
    "return nbytes;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e48e9-d085-43ff-bae4-8a10ce230c0f",
   "metadata": {},
   "source": [
    "### [`ggml_backend_sched_new`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1455)\n",
    "\n",
    "```c++\n",
    "sched.reset(ggml_backend_sched_new(backend_ptrs.data(), backend_buft.data(), backend_ptrs.size(), max_nodes, pipeline_parallel, cparams.op_offload));\n",
    "----------\n",
    "ggml_backend_sched_t ggml_backend_sched_new(\n",
    "        ggml_backend_t * backends,\n",
    "        ggml_backend_buffer_type_t * bufts,\n",
    "        int n_backends,\n",
    "        size_t graph_size,\n",
    "        bool parallel,\n",
    "        bool op_offload) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4GO0tWJL0zt5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_hash_set</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_hash_set {\n",
    "    size_t size;\n",
    "    ggml_bitset_t * used;       // whether or not the keys are in use i.e. set\n",
    "    struct ggml_tensor ** keys; // actual tensors in the set, keys[i] is only defined if ggml_bitset_get(used, i)\n",
    "};\n",
    "\n",
    "static size_t ggml_hash_insert(struct ggml_hash_set * hash_set, struct ggml_tensor * key) {\n",
    "    size_t h = ggml_hash(key) % hash_set->size;\n",
    "\n",
    "    // linear probing\n",
    "    size_t i = h;\n",
    "    do {\n",
    "        if (!ggml_bitset_get(hash_set->used, i)) {\n",
    "            ggml_bitset_set(hash_set->used, i);\n",
    "            hash_set->keys[i] = key;\n",
    "            return i;\n",
    "        }\n",
    "        if (hash_set->keys[i] == key) {\n",
    "            return GGML_HASHSET_ALREADY_EXISTS;\n",
    "        }\n",
    "        i = (i + 1) % hash_set->size;\n",
    "    } while (i != h);\n",
    "\n",
    "    // visited all hash table entries -> not found\n",
    "    GGML_ABORT(\"fatal error\");\n",
    "}\n",
    "\n",
    "static size_t ggml_hash_find_or_insert(struct ggml_hash_set * hash_set, struct ggml_tensor * key) {\n",
    "    ...\n",
    "        if (hash_set->keys[i] == key) {\n",
    "            return i;\n",
    "        }\n",
    "    ...\n",
    "}\n",
    "\n",
    "struct ggml_hash_set ggml_hash_set_new(size_t size) {\n",
    "    size = ggml_hash_size(size);\n",
    "    struct ggml_hash_set result;\n",
    "    result.size = size;\n",
    "    result.keys = GGML_MALLOC(sizeof(struct ggml_tensor *) * size);\n",
    "    result.used = GGML_CALLOC(ggml_bitset_size(size), sizeof(ggml_bitset_t));\n",
    "    return result;\n",
    "}\n",
    "\n",
    "void ggml_hash_set_free(struct ggml_hash_set * hash_set) {\n",
    "    GGML_FREE(hash_set->used);\n",
    "    GGML_FREE(hash_set->keys);\n",
    "}\n",
    "\n",
    "void ggml_hash_set_reset(struct ggml_hash_set * hash_set) {\n",
    "    memset(hash_set->used, 0, sizeof(ggml_bitset_t) * ggml_bitset_size(hash_set->size));\n",
    "}\n",
    "\n",
    "static struct hash_node * ggml_gallocr_hash_get(ggml_gallocr_t galloc, struct ggml_tensor * t) {\n",
    "    size_t i = ggml_hash_find_or_insert(&galloc->hash_set, t);\n",
    "    return &galloc->hash_values[i];\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_backend_sched/ggml_backend_sched_t</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_sched {\n",
    "    bool is_reset; // true if the scheduler has been reset since the last graph split\n",
    "    bool is_alloc;\n",
    "\n",
    "    int n_backends;\n",
    "\n",
    "    ggml_backend_t backends[GGML_SCHED_MAX_BACKENDS];\n",
    "    ggml_backend_buffer_type_t bufts[GGML_SCHED_MAX_BACKENDS];\n",
    "    ggml_gallocr_t galloc;\n",
    "\n",
    "    // hash map of the nodes in the graph\n",
    "    struct ggml_hash_set  hash_set;\n",
    "    int                 * hv_tensor_backend_ids; // [hash_set.size]\n",
    "    struct ggml_tensor ** hv_tensor_copies;      // [hash_set.size][n_backends][n_copies]\n",
    "\n",
    "    int * node_backend_ids; // [graph_size]\n",
    "    int * leaf_backend_ids; // [graph_size]\n",
    "\n",
    "    int * prev_node_backend_ids; // [graph_size]\n",
    "    int * prev_leaf_backend_ids; // [graph_size]\n",
    "\n",
    "    // copy of the graph with modified inputs\n",
    "    struct ggml_cgraph graph;\n",
    "\n",
    "    // graph splits\n",
    "    struct ggml_backend_sched_split * splits;\n",
    "    int n_splits;\n",
    "    int splits_capacity;\n",
    "\n",
    "    // pipeline parallelism support\n",
    "    int n_copies;\n",
    "    int cur_copy;\n",
    "    ggml_backend_event_t events[GGML_SCHED_MAX_BACKENDS][GGML_SCHED_MAX_COPIES];\n",
    "    struct ggml_tensor * graph_inputs[GGML_SCHED_MAX_SPLIT_INPUTS];\n",
    "    int n_graph_inputs;\n",
    "\n",
    "    struct ggml_context * ctx;\n",
    "\n",
    "    ggml_backend_sched_eval_callback callback_eval;\n",
    "    void * callback_eval_user_data;\n",
    "\n",
    "    char * context_buffer;\n",
    "    size_t context_buffer_size;\n",
    "\n",
    "    bool op_offload;\n",
    "\n",
    "    int debug;\n",
    "};\n",
    "\n",
    "typedef struct ggml_backend_sched * ggml_backend_sched_t;\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_sched * sched = (ggml_backend_sched *) calloc(1, sizeof(struct ggml_backend_sched));\n",
    "sched->n_backends = n_backends;\n",
    "sched->n_copies = parallel ? GGML_SCHED_MAX_COPIES : 1;\n",
    "// initialize hash table\n",
    "// FIXME: needs to be size*2 to account for leafs (do it in graph_split instead)\n",
    "sched->hash_set    = ggml_hash_set_new(graph_size);\n",
    "sched->hv_tensor_backend_ids = (int *) malloc(sched->hash_set.size * sizeof(sched->hv_tensor_backend_ids[0]));\n",
    "sched->hv_tensor_copies      = (ggml_tensor **) malloc(sched->hash_set.size * sched->n_backends * sched->n_copies * sizeof(struct ggml_tensor *));\n",
    "const size_t ggml_sched_max_splits = graph_size; // at most there is one split for each node in the graph\n",
    "const size_t nodes_size = graph_size + ggml_sched_max_splits*GGML_SCHED_MAX_SPLIT_INPUTS*2;\n",
    "sched->node_backend_ids = (int *) calloc(nodes_size, sizeof(sched->node_backend_ids[0]));\n",
    "sched->leaf_backend_ids = (int *) calloc(nodes_size, sizeof(sched->leaf_backend_ids[0]));\n",
    "sched->prev_node_backend_ids = (int *) calloc(nodes_size, sizeof(sched->prev_node_backend_ids[0]));\n",
    "sched->prev_leaf_backend_ids = (int *) calloc(nodes_size, sizeof(sched->prev_leaf_backend_ids[0]));\n",
    "\n",
    "sched->context_buffer_size = ggml_sched_max_splits*GGML_SCHED_MAX_SPLIT_INPUTS*2*sizeof(struct ggml_tensor) + ggml_graph_overhead_custom(graph_size, false);\n",
    "sched->context_buffer = (char *) malloc(sched->context_buffer_size);\n",
    "\n",
    "const int initial_splits_capacity = 16;\n",
    "sched->splits = (ggml_backend_sched_split *) calloc(initial_splits_capacity, sizeof(sched->splits[0]));\n",
    "sched->splits_capacity = initial_splits_capacity;\n",
    "for (int b = 0; b < n_backends; b++)\n",
    "    sched->backends[b] = backends[b];\n",
    "    sched->bufts[b] = bufts[b];\n",
    "--> ggml_gallocr_new_n --> sched->galloc;\n",
    "sched->op_offload = op_offload;\n",
    "ggml_backend_sched_reset(sched);\n",
    "    --> // reset state for the next run\n",
    "    //if (!sched->is_reset)\n",
    "    ggml_hash_set_reset(&sched->hash_set);\n",
    "    memset(sched->hv_tensor_backend_ids, -1, sched->hash_set.size * sizeof(sched->hv_tensor_backend_ids[0]));\n",
    "    memset(sched->hv_tensor_copies,       0, sched->hash_set.size * sched->n_backends * sched->n_copies * sizeof(struct ggml_tensor *));\n",
    "    sched->is_reset = true;\n",
    "    sched->is_alloc = false;\n",
    "return sched;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ksw-c8aM4zCN",
   "metadata": {},
   "source": [
    "#### [`ggml_gallocr_new_n`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L380)\n",
    "\n",
    "```c++\n",
    "// Graph allocator\n",
    "/*\n",
    "  Example usage:\n",
    "    ggml_gallocr_t galloc = ggml_gallocr_new(ggml_backend_cpu_buffer_type());\n",
    "\n",
    "    // optional: create a worst-case graph and reserve the buffers to avoid reallocations\n",
    "    ggml_gallocr_reserve(galloc, build_graph(max_batch));\n",
    "\n",
    "    // allocate the graph\n",
    "    struct ggml_cgraph * graph = build_graph(batch);\n",
    "    ggml_gallocr_alloc_graph(galloc, graph);\n",
    "\n",
    "    printf(\"compute buffer size: %zu bytes\\n\", ggml_gallocr_get_buffer_size(galloc, 0));\n",
    "\n",
    "    // evaluate the graph\n",
    "    ggml_backend_graph_compute(backend, graph);\n",
    "*/\n",
    "\n",
    "// special tensor flags for use with the graph allocator:\n",
    "//   ggml_set_input(): all input tensors are allocated at the beginning of the graph in non-overlapping addresses\n",
    "//   ggml_set_output(): output tensors are never freed and never overwritten\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_gallocr/ggml_gallocr_t</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_gallocr {\n",
    "    ggml_backend_buffer_type_t * bufts; // [n_buffers]\n",
    "    ggml_backend_buffer_t * buffers; // [n_buffers]\n",
    "    struct ggml_dyn_tallocr ** buf_tallocs; // [n_buffers]\n",
    "    int n_buffers;\n",
    "\n",
    "    struct ggml_hash_set hash_set;\n",
    "    struct hash_node * hash_values; // [hash_set.size]\n",
    "\n",
    "    struct node_alloc * node_allocs; // [n_nodes]\n",
    "    int n_nodes;\n",
    "\n",
    "    struct leaf_alloc * leaf_allocs; // [n_leafs]\n",
    "    int n_leafs;\n",
    "};\n",
    "\n",
    "\n",
    "typedef struct ggml_gallocr * ggml_gallocr_t;\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "sched->galloc = ggml_gallocr_new_n(sched->bufts, n_backends);\n",
    "----------\n",
    "ggml_gallocr_t ggml_gallocr_new_n(ggml_backend_buffer_type_t * bufts, int n_bufs) {\n",
    "    galloc->bufts = calloc(n_bufs, sizeof(ggml_backend_buffer_type_t));\n",
    "    galloc->buffers = calloc(n_bufs, sizeof(ggml_backend_buffer_t));\n",
    "    galloc->buf_tallocs = calloc(n_bufs, sizeof(struct ggml_dyn_tallocr *));\n",
    "    for (int i = 0; i < n_bufs; i++)\n",
    "        galloc->bufts[i] = bufts[i];\n",
    "        galloc->buffers[i] = NULL;\n",
    "        --> ggml_dyn_tallocr_new(ggml_backend_buft_get_alignment(bufts[i])) --> galloc->buf_tallocs[i];\n",
    "    galloc->n_buffers = n_bufs;\n",
    "    return galloc;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DAlj_2O_5XTi",
   "metadata": {},
   "source": [
    "[`ggml_dyn_tallocr_new`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L310)\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_dyn_tallocr</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_dyn_tallocr {\n",
    "    size_t alignment;\n",
    "    int n_free_blocks;\n",
    "    struct free_block free_blocks[MAX_FREE_BLOCKS];\n",
    "    size_t max_size;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "galloc->buf_tallocs[i] = ggml_dyn_tallocr_new(alignment);\n",
    "----------\n",
    "static struct ggml_dyn_tallocr * ggml_dyn_tallocr_new(size_t alignment) {\n",
    "    struct ggml_dyn_tallocr * alloc = (struct ggml_dyn_tallocr *)malloc(sizeof(struct ggml_dyn_tallocr));\n",
    "\n",
    "    *alloc = (struct ggml_dyn_tallocr) {\n",
    "        /*.alignment     = */ alignment,\n",
    "        /*.n_free_blocks = */ 0,\n",
    "        /*.free_blocks   = */ {{0}},\n",
    "        /*.max_size      = */ 0,\n",
    "    };\n",
    "\n",
    "    ggml_dyn_tallocr_reset(alloc);\n",
    "        alloc->n_free_blocks = 1;\n",
    "        alloc->free_blocks[0].offset = 0;\n",
    "        alloc->free_blocks[0].size = SIZE_MAX/2; // restrict maximum size of a measure allocator to half size_t max to avoid overflows\n",
    "        alloc->max_size = 0;\n",
    "    return alloc;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c4e74-0310-4b08-98f4-ac271b0818d3",
   "metadata": {},
   "source": [
    "### [`llama_kv_cache_unified_context::llama_kv_cache_unified_context`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L1718)\n",
    "\n",
    "<details>\n",
    "<summary>class llama_kv_cache_unified_context</summary>\n",
    "\n",
    "```c++\n",
    "class llama_kv_cache_unified_context : public llama_memory_context_i {\n",
    "public:\n",
    "    // some shorthands\n",
    "    using ubatch_heads = llama_kv_cache_unified::ubatch_heads;\n",
    "    using defrag_info  = llama_kv_cache_unified::defrag_info;\n",
    "\n",
    "    // used for errors\n",
    "    llama_kv_cache_unified_context(llama_memory_status status);\n",
    "\n",
    "    // used to create a full-cache context\n",
    "    llama_kv_cache_unified_context(\n",
    "            llama_kv_cache_unified * kv);\n",
    "\n",
    "    // used to create an update context\n",
    "    llama_kv_cache_unified_context(\n",
    "            llama_kv_cache_unified * kv,\n",
    "            llama_context * lctx,\n",
    "            bool do_shift,\n",
    "            defrag_info dinfo);\n",
    "\n",
    "    // used to create a batch procesing context from a batch\n",
    "    llama_kv_cache_unified_context(\n",
    "            llama_kv_cache_unified * kv,\n",
    "            ubatch_heads heads,\n",
    "            std::vector<llama_ubatch> ubatches);\n",
    "\n",
    "    virtual ~llama_kv_cache_unified_context();\n",
    "\n",
    "    //\n",
    "    // llama_memory_context_i\n",
    "    //\n",
    "\n",
    "    bool next()  override;\n",
    "    bool apply() override;\n",
    "\n",
    "    llama_memory_status  get_status() const override;\n",
    "    const llama_ubatch & get_ubatch() const override;\n",
    "\n",
    "    //\n",
    "    // llama_kv_cache_unified_context specific API\n",
    "    //\n",
    "\n",
    "    uint32_t get_n_kv() const;\n",
    "\n",
    "    // get views of the current state of the cache\n",
    "    ggml_tensor * get_k(ggml_context * ctx, int32_t il) const;\n",
    "    ggml_tensor * get_v(ggml_context * ctx, int32_t il) const;\n",
    "\n",
    "    // store k_cur and v_cur in the cache based on the provided head location\n",
    "    ggml_tensor * cpy_k(ggml_context * ctx, ggml_tensor * k_cur, int32_t il) const;\n",
    "    ggml_tensor * cpy_v(ggml_context * ctx, ggml_tensor * v_cur, int32_t il) const;\n",
    "\n",
    "    void set_input_k_shift(ggml_tensor * dst) const;\n",
    "\n",
    "    void set_input_kq_mask   (ggml_tensor * dst, const llama_ubatch * ubatch, bool causal_attn) const;\n",
    "    void set_input_pos_bucket(ggml_tensor * dst, const llama_ubatch * ubatch) const;\n",
    "\n",
    "private:\n",
    "    llama_memory_status status;\n",
    "\n",
    "    llama_kv_cache_unified * kv;\n",
    "    llama_context * lctx;\n",
    "\n",
    "    //\n",
    "    // update context\n",
    "    //\n",
    "\n",
    "    bool do_shift = false;\n",
    "\n",
    "    defrag_info dinfo;\n",
    "\n",
    "    //\n",
    "    // batch processing context\n",
    "    //\n",
    "\n",
    "    // the index of the next ubatch to process\n",
    "    size_t i_next = 0;\n",
    "\n",
    "    ubatch_heads heads;\n",
    "\n",
    "    std::vector<llama_ubatch> ubatches;\n",
    "\n",
    "    //\n",
    "    // data needed for building the compute graph for the current ubatch:\n",
    "    //\n",
    "\n",
    "    // a heuristic, to avoid attending the full cache if it is not yet utilized\n",
    "    // as the cache gets filled, the benefit from this heuristic disappears\n",
    "    int32_t n_kv;\n",
    "\n",
    "    // the beginning of the current slot in which the ubatch will be inserted\n",
    "    int32_t head;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "const auto mstate = memory->init_full();\n",
    "----------\n",
    "llama_memory_state_ptr llama_kv_cache_unified::init_full() {\n",
    "    --> return std::make_unique<llama_kv_cache_unified_context>(this);\n",
    "}\n",
    "\n",
    "llama_kv_cache_unified_context::llama_kv_cache_unified_context(\n",
    "        llama_kv_cache_unified * kv) : status(LLAMA_MEMORY_STATUS_SUCCESS), kv(kv) {\n",
    "    n_kv = kv->get_size();\n",
    "    head = 0;\n",
    "}      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc1a12-1a27-4be5-9673-58f3d51d7c5c",
   "metadata": {},
   "source": [
    "### [`llama_context::graph_reserve`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L1295)\n",
    "\n",
    "```c++\n",
    "// reserve pp graph first so that buffers are only allocated once\n",
    "auto * gf = graph_reserve(n_tokens, n_seqs, n_tokens, mctx.get());\n",
    "// reserve with tg graph to get the number of splits and nodes\n",
    "auto * gf = graph_reserve(1, 1, 1, mctx.get());\n",
    "// reserve again with pp graph to avoid ggml-alloc reallocations during inference\n",
    "auto * gf = graph_reserve(n_tokens, n_seqs, n_tokens, mctx.get());\n",
    "----------\n",
    "ggml_cgraph * llama_context::graph_reserve(uint32_t n_tokens, uint32_t n_seqs, uint32_t n_outputs, const llama_memory_context_i * mctx) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PcT9obSo9i6T",
   "metadata": {},
   "source": [
    "```c++\n",
    "this->n_outputs = n_outputs;\n",
    "--> llama_batch_allocr::llama_batch_allocr --> llama_batch_allocr balloc;\n",
    "--> llama_batch_allocr::ubatch_reserve --> llama_ubatch ubatch;\n",
    "--> llama_context::graph_init --> ggml_cgraph * gf;\n",
    "--> llama_context::graph_build --> llama_model::build_graph --> llm_build_llama::llm_build_llama --> std::unique_ptr<llm_graph_result> res;\n",
    "ggml_backend_sched_reset(sched.get());\n",
    "// initialize scheduler with the specified graph\n",
    "--> ggml_backend_sched_reserve;\n",
    "return gf;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29bfc8",
   "metadata": {},
   "source": [
    "#### [`llama_batch_allocr::llama_batch_allocr`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L12)\n",
    "\n",
    "<details>\n",
    "<summary>class llama_batch_allocr</summary>\n",
    "\n",
    "```c++\n",
    "// a helper for sanitizing, fulfilling and splitting a batch\n",
    "class llama_batch_allocr {\n",
    "    llama_batch batch;\n",
    "\n",
    "    // only for debugging purposes\n",
    "    const llama_vocab * vocab;\n",
    "\n",
    "    // TODO: this is more of a temporary solution until we have a better way to handle multiple positions per token/embd\n",
    "    //       ref: https://github.com/ggml-org/llama.cpp/issues/13694#issuecomment-2983871762\n",
    "    const uint32_t n_pos_per_embd;\n",
    "\n",
    "    uint32_t n_embd;\n",
    "    uint32_t n_outputs;\n",
    "\n",
    "    std::array<llama_seq_id, 1> seq_id_0 = { 0 }; // default sequence id\n",
    "\n",
    "    std::vector<llama_pos>      pos;\n",
    "    std::vector<int32_t>        n_seq_id;\n",
    "    std::vector<llama_seq_id *> seq_id;\n",
    "    std::vector<llama_seq_id>   seq_id_unq;\n",
    "    std::vector<int32_t>        seq_idx;\n",
    "    std::vector<int8_t>         output;\n",
    "\n",
    "    using pos_set_t = std::set<llama_pos>;\n",
    "    using seq_cpl_t = std::vector<bool>;\n",
    "\n",
    "    std::vector<pos_set_t> seq_pos; // seq_pos[s]: the set of positions in sequence s\n",
    "    std::vector<seq_cpl_t> seq_cpl; // seq_cpl[s0][s1]: if sequence s0 is coupled to sequence s1\n",
    "\n",
    "    using idx_vec_t = std::vector<int32_t>;\n",
    "    using seq_set_t = std::bitset<LLAMA_MAX_SEQ>;\n",
    "\n",
    "    std::vector<seq_set_t> seq_set; // seq_set[i]: the sequence set of token i\n",
    "\n",
    "    std::unordered_map<seq_set_t, idx_vec_t> seq_set_map; // the indices at which the sequence set appears\n",
    "\n",
    "    // batch indices of the output\n",
    "    std::vector<int32_t> out_ids;\n",
    "\n",
    "    // used[i] indicates if token i has already been used in a previous ubatch\n",
    "    std::vector<bool> used;\n",
    "\n",
    "    // llama_ubatch points to this data:\n",
    "    struct ubatch {\n",
    "        std::vector<llama_token>    token;\n",
    "        std::vector<float>          embd;\n",
    "        std::vector<llama_pos>      pos;\n",
    "        std::vector<int32_t>        n_seq_id;\n",
    "        std::vector<llama_seq_id *> seq_id;\n",
    "        std::vector<llama_seq_id>   seq_id_unq;\n",
    "        std::vector<int32_t>        seq_idx;\n",
    "        std::vector<int8_t>         output;\n",
    "    };\n",
    "\n",
    "    // current splitting state:\n",
    "    std::vector<ubatch> ubatches;\n",
    "\n",
    "    int debug;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "llama_batch_allocr balloc(model.hparams.n_pos_per_embd());\n",
    "----------\n",
    "llama_batch_allocr::llama_batch_allocr(uint32_t n_pos_per_embd) : n_pos_per_embd(n_pos_per_embd) {\n",
    "    const char * LLAMA_BATCH_DEBUG = getenv(\"LLAMA_BATCH_DEBUG\");\n",
    "    debug = LLAMA_BATCH_DEBUG ? atoi(LLAMA_BATCH_DEBUG) : 0;\n",
    "    seq_pos.resize(LLAMA_MAX_SEQ);\n",
    "    seq_cpl.resize(LLAMA_MAX_SEQ);\n",
    "    for (auto & cur : seq_cpl) cur.resize(LLAMA_MAX_SEQ);\n",
    "    seq_idx.resize(LLAMA_MAX_SEQ, -1);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e216ed",
   "metadata": {},
   "source": [
    "#### [`lama_batch_allocr::ubatch_reserve`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L339)\n",
    "\n",
    "```c++\n",
    "llama_ubatch ubatch = balloc.ubatch_reserve(n_tokens/n_seqs, n_seqs);\n",
    "----------\n",
    "llama_ubatch llama_batch_allocr::ubatch_reserve(uint32_t n_seq_tokens, uint32_t n_seqs) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731630c8",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct llama_ubatch</summary>\n",
    "\n",
    "```c++\n",
    "// keep this struct lightweight\n",
    "// it points to data in `llama_batch_allocr`\n",
    "struct llama_ubatch {\n",
    "    bool equal_seqs;\n",
    "    // TODO: whole_seqs for embeddings?\n",
    "    uint32_t n_tokens;     // total tokens (n_seq_tokens * n_seqs)\n",
    "    uint32_t n_seq_tokens; // tokens per sequence set\n",
    "    uint32_t n_seqs;       // sequence sets in the ubatch\n",
    "    uint32_t n_seqs_unq;   // unique sequence ids in the ubatch\n",
    "    // seq_id_unq: unique sequence ids in the ubatch\n",
    "    // seq_idx:    indices of the unique sequence ids in the ubatch in [0, n_seqs_unq)\n",
    "    //             used for extracting sequence pooled embeddings\n",
    "    //                          // size               | idx | val\n",
    "    llama_token  *  token;      // [n_tokens]         | i   | id, token\n",
    "    float        *  embd;       // [n_embd, n_tokens] | i   | embd\n",
    "    llama_pos    *  pos;        // [n_tokens]         | i   | pos\n",
    "    int32_t      *  n_seq_id;   // [n_tokens]         | i   | -\n",
    "    llama_seq_id ** seq_id;     // [n_tokens]         | s   | s0, s1, seq_id\n",
    "    llama_seq_id *  seq_id_unq; // [n_seqs_unq]       | s   | seq_id\n",
    "    int32_t      *  seq_idx;    // [LLAMA_MAX_SEQ]    | -   | seq_idx\n",
    "    int8_t       *  output;     // [n_tokens]         | i   | -\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "const uint32_t n_tokens = n_seq_tokens*n_seqs;\n",
    "--> clear();\n",
    "--> split_reset();\n",
    "ubatches.emplace_back();\n",
    "auto & ubatch = ubatches.back();\n",
    "ubatch.token     .resize(n_tokens);\n",
    "ubatch.embd      .clear();\n",
    "ubatch.pos       .resize(n_tokens);\n",
    "ubatch.n_seq_id  .resize(n_tokens);\n",
    "ubatch.seq_id    .resize(n_tokens);\n",
    "ubatch.seq_id_unq.resize(0);\n",
    "ubatch.seq_idx   .resize(LLAMA_MAX_SEQ, -1);\n",
    "ubatch.output    .resize(n_tokens);\n",
    "// for (uint32_t s = 0; s < n_seqs; ++s)\n",
    "ubatch.seq_idx[0] = 0;\n",
    "ubatch.seq_id_unq.push_back(0);\n",
    "llama_ubatch res {\n",
    "    /*.equal_seqs   =*/ true,\n",
    "    /*.n_tokens     =*/ n_tokens,\n",
    "    /*.n_seq_tokens =*/ n_seq_tokens,\n",
    "    /*.n_seqs       =*/ n_seqs,\n",
    "    /*.n_seqs_unq   =*/ n_seqs,\n",
    "    /*.token        =*/ ubatch.token.data(),\n",
    "    /*.embd         =*/ nullptr,\n",
    "    /*.pos          =*/ ubatch.pos.data(),\n",
    "    /*.n_seq_id     =*/ ubatch.n_seq_id.data(),\n",
    "    /*.seq_id       =*/ ubatch.seq_id.data(),\n",
    "    /*.seq_id_unq   =*/ ubatch.seq_id_unq.data(),\n",
    "    /*.seq_idx      =*/ ubatch.seq_idx.data(),\n",
    "    /*.output       =*/ ubatch.output.data(),\n",
    "};\n",
    "return res;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b03a8",
   "metadata": {},
   "source": [
    "[`llama_batch_allocr::clear`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L579)\n",
    "\n",
    "```c++\n",
    "void llama_batch_allocr::clear() {\n",
    "    n_outputs = 0;\n",
    "    batch = {};\n",
    "    pos       .clear();\n",
    "    n_seq_id  .clear();\n",
    "    seq_id    .clear();\n",
    "    seq_id_unq.clear();\n",
    "    output    .clear();\n",
    "    for (auto & cur : seq_pos) cur.clear();\n",
    "    for (auto & cur : seq_cpl) std::fill(cur.begin(), cur.end(), false);\n",
    "    seq_set.clear();\n",
    "    seq_set_map.clear();\n",
    "    std::fill(seq_idx.begin(), seq_idx.end(), -1);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F2yre4rS_tfl",
   "metadata": {},
   "source": [
    "#### [`llama_context::graph_init`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L1319)\n",
    "\n",
    "> ctx_compute 存放的是原始的静态计算图\n",
    "\n",
    "```c++\n",
    "auto * gf = graph_init();\n",
    "----------\n",
    "ggml_cgraph * llama_context::graph_init() {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WriaqPmAABfH",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_cgraph</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_cgraph {\n",
    "    int size;    // maximum number of nodes/leafs/grads/grad_accs\n",
    "    int n_nodes; // number of nodes currently in use\n",
    "    int n_leafs; // number of leafs currently in use\n",
    "\n",
    "    struct ggml_tensor ** nodes;     // tensors with data that can change if the graph is evaluated\n",
    "    struct ggml_tensor ** grads;     // the outputs of these tensors are the gradients of the nodes\n",
    "    struct ggml_tensor ** grad_accs; // accumulators for node gradients\n",
    "    struct ggml_tensor ** leafs;     // tensors with constant data\n",
    "\n",
    "    struct ggml_hash_set visited_hash_set;\n",
    "\n",
    "    enum ggml_cgraph_eval_order order;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "ggml_init_params params = {\n",
    "    /*.mem_size   =*/ buf_compute_meta.size(),\n",
    "    /*.mem_buffer =*/ buf_compute_meta.data(),\n",
    "    /*.no_alloc   =*/ true,\n",
    "};\n",
    "llama_context:: ggml_context_ptr ctx_compute.reset(ggml_init(params));\n",
    "--> return ggml_new_graph_custom;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mKZYl3SVHSMe",
   "metadata": {},
   "source": [
    "[`ggml_new_graph_custom`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L5986)\n",
    "\n",
    "\n",
    "```c++\n",
    "return ggml_new_graph_custom(ctx_compute.get(), graph_max_nodes(), false);\n",
    "----------\n",
    "struct ggml_cgraph * ggml_new_graph_custom(struct ggml_context * ctx, size_t size, bool grads) {\n",
    "    const size_t obj_size = ggml_graph_nbytes(size, grads);\n",
    "    struct ggml_object * obj = ggml_new_object(ctx, GGML_OBJECT_TYPE_GRAPH, obj_size);\n",
    "    struct ggml_cgraph * cgraph = (struct ggml_cgraph *) ((char *) ctx->mem_buffer + obj->offs);\n",
    "\n",
    "    // the size of the hash table is doubled since it needs to hold both nodes and leafs\n",
    "    size_t hash_size = ggml_hash_size(size * 2);\n",
    "\n",
    "    void * p = cgraph + 1;\n",
    "\n",
    "    struct ggml_tensor ** nodes_ptr     =         incr_ptr_aligned(&p, size      * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *));\n",
    "    struct ggml_tensor ** leafs_ptr     =         incr_ptr_aligned(&p, size      * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *));\n",
    "    struct ggml_tensor ** hash_keys_ptr =         incr_ptr_aligned(&p, hash_size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *));\n",
    "    struct ggml_tensor ** grads_ptr     = grads ? incr_ptr_aligned(&p, hash_size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)) : NULL;\n",
    "    struct ggml_tensor ** grad_accs_ptr = grads ? incr_ptr_aligned(&p, hash_size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)) : NULL;\n",
    "\n",
    "    ggml_bitset_t * hash_used = incr_ptr_aligned(&p, ggml_bitset_size(hash_size) * sizeof(ggml_bitset_t), sizeof(ggml_bitset_t));\n",
    "\n",
    "    *cgraph = (struct ggml_cgraph) {\n",
    "        /*.size         =*/ size,\n",
    "        /*.n_nodes      =*/ 0,\n",
    "        /*.n_leafs      =*/ 0,\n",
    "        /*.nodes        =*/ nodes_ptr,\n",
    "        /*.grads        =*/ grads_ptr,\n",
    "        /*.grad_accs    =*/ grad_accs_ptr,\n",
    "        /*.leafs        =*/ leafs_ptr,\n",
    "        /*.hash_table   =*/ { hash_size, hash_used, hash_keys_ptr },\n",
    "        /*.order        =*/ GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT,\n",
    "    };\n",
    "\n",
    "    ggml_hash_set_reset(&cgraph->visited_hash_set);\n",
    "    return cgraph;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YkLEyS7AAgLL",
   "metadata": {},
   "source": [
    "#### [`llm_build_llama::llm_build_llama`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#L4686)\n",
    "\n",
    "> 这里开始真正构建模型的拓扑关系了，但是尚未给中间的激活tensor分配buffer和data域，所以是静态的计算图\n",
    "\n",
    "> 注意到全程都没有用到sched的信息，就只用了 model(负责权重tensor的提供)，mctx(负责kvcache tensor的提供)， ctx_compute(记录最后生成的带有拓扑结构的静态图的tensors 信息) \n",
    "\n",
    "<details>\n",
    "<summary>class llm_graph_input_i/llm_graph_input_ptr</summary>\n",
    "\n",
    "```c++\n",
    "class llm_graph_input_i {\n",
    "public:\n",
    "    virtual ~llm_graph_input_i() = default;\n",
    "    virtual void set_input(const llama_ubatch * ubatch) = 0;\n",
    "};\n",
    "using llm_graph_input_ptr = std::unique_ptr<llm_graph_input_i>;\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>class llm_graph_result</summary>\n",
    "\n",
    "```c++\n",
    "//\n",
    "// llm_graph_result\n",
    "//\n",
    "\n",
    "// these objects deliver the result from the graph build process back to the llama_context\n",
    "// note that the input tensors created for the graph are referenced here - the goal is to be able to populate their\n",
    "//   specific data, by calling the set_inputs() method\n",
    "// along with the input tensors, the object also provides commonly used outputs tensors, such as logits, embeddings, etc.\n",
    "//   these are used by the llama_context to extact the relevant data, based on the compute parameters\n",
    "\n",
    "class llm_graph_result_i {\n",
    "public:\n",
    "    virtual ~llm_graph_result_i() = default;\n",
    "\n",
    "    virtual ggml_tensor * get_tokens()      = 0;\n",
    "    virtual ggml_tensor * get_logits()      = 0;\n",
    "    virtual ggml_tensor * get_embd()        = 0;\n",
    "    virtual ggml_tensor * get_embd_pooled() = 0;\n",
    "\n",
    "    virtual void set_inputs(const llama_ubatch * ubatch) = 0;\n",
    "};\n",
    "\n",
    "using llm_graph_result_ptr = std::unique_ptr<llm_graph_result_i>;\n",
    "\n",
    "class llm_graph_result : public llm_graph_result_i {\n",
    "public:\n",
    "    // important graph nodes\n",
    "    ggml_tensor * t_tokens      = nullptr;\n",
    "    ggml_tensor * t_logits      = nullptr;\n",
    "    ggml_tensor * t_embd        = nullptr;\n",
    "    ggml_tensor * t_embd_pooled = nullptr;\n",
    "\n",
    "    std::vector<llm_graph_input_ptr> inputs;\n",
    "}\n",
    "\n",
    "    void set_inputs(const llama_ubatch * ubatch) override {\n",
    "        for (auto & input : inputs) {\n",
    "            input->set_input(ubatch);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    llm_graph_input_i * add_input(llm_graph_input_ptr input) {\n",
    "        inputs.emplace_back(std::move(input));\n",
    "        return inputs.back().get();\n",
    "    }\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>struct llm_graph_params</summary>\n",
    "\n",
    "```c++\n",
    "struct llm_graph_params {\n",
    "    ggml_context * ctx;\n",
    "\n",
    "    const llm_arch arch;\n",
    "\n",
    "    const llama_hparams & hparams;\n",
    "    const llama_cparams & cparams;\n",
    "    const llama_ubatch  & ubatch;\n",
    "\n",
    "    ggml_backend_sched_t sched;\n",
    "    ggml_backend_t backend_cpu;\n",
    "\n",
    "    const llama_adapter_cvec     * cvec;\n",
    "    const llama_adapter_loras    * loras;\n",
    "    const llama_memory_context_i * mctx;\n",
    "    const llama_cross            * cross;\n",
    "\n",
    "    uint32_t n_outputs;\n",
    "\n",
    "    const llm_graph_cb & cb;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>struct llm_graph_context</summary>\n",
    "\n",
    "```c++\n",
    "struct llm_graph_context {\n",
    "    const llm_arch arch;\n",
    "\n",
    "    const llama_hparams & hparams;\n",
    "    const llama_cparams & cparams;\n",
    "    const llama_ubatch  & ubatch;\n",
    "\n",
    "    const int64_t n_embd;\n",
    "    const int64_t n_layer;\n",
    "    const int64_t n_rot;\n",
    "    const int64_t n_ctx;       // user-specified context size (can be different from n_ctx_train)\n",
    "    const int64_t n_head;\n",
    "    const int64_t n_head_kv;\n",
    "    const int64_t n_embd_head_k;\n",
    "    const int64_t n_embd_k_gqa;\n",
    "    const int64_t n_embd_head_v;\n",
    "    const int64_t n_embd_v_gqa;\n",
    "    const int64_t n_expert;\n",
    "    const int64_t n_expert_used;\n",
    "\n",
    "    const float freq_base;\n",
    "    const float freq_scale;\n",
    "    const float ext_factor;\n",
    "    const float attn_factor;\n",
    "    const float beta_fast;\n",
    "    const float beta_slow;\n",
    "    const float norm_eps;\n",
    "    const float norm_rms_eps;\n",
    "\n",
    "    const int32_t n_tokens;\n",
    "    const int32_t n_outputs;\n",
    "    const int32_t n_ctx_orig; // yarn\n",
    "\n",
    "    const enum llama_pooling_type pooling_type;\n",
    "    const enum llama_rope_type    rope_type;\n",
    "\n",
    "    ggml_context * ctx0 = nullptr;\n",
    "\n",
    "    ggml_backend_sched_t sched;\n",
    "\n",
    "    ggml_backend_t backend_cpu; // TODO: needed by build_attn_mha, figure out a way to remove?\n",
    "\n",
    "    const llama_adapter_cvec   * cvec;\n",
    "    const llama_adapter_loras  * loras;\n",
    "    const llama_memory_state_i * mstate;\n",
    "    const llama_cross          * cross;\n",
    "\n",
    "    const llm_graph_cb & cb_func;\n",
    "\n",
    "    std::unique_ptr<llm_graph_result> res;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "auto res = graph_build(ctx_compute.get(), gf, ubatch, LLM_GRAPH_TYPE_DEFAULT, mctx);\n",
    "----------\n",
    "llm_graph_result_ptr llama_context::graph_build(\n",
    "                    ggml_context * ctx,\n",
    "                     ggml_cgraph * gf,\n",
    "              const llama_ubatch & ubatch,\n",
    "                  llm_graph_type   gtype,\n",
    "      const llama_memory_context_i * mctx) {\n",
    "      --> return model.build_graph(\n",
    "            struct llm_graph_params = {\n",
    "                /*.ctx         =*/ ctx,\n",
    "                /*.arch        =*/ model.arch,\n",
    "                /*.hparams     =*/ model.hparams,\n",
    "                /*.cparams     =*/ cparams,\n",
    "                /*.ubatch      =*/ ubatch,\n",
    "                /*.sched       =*/ sched.get(),\n",
    "                /*.backend_cpu =*/ backend_cpu,\n",
    "                /*.cvec        =*/ &cvec,\n",
    "                /*.loras       =*/ &loras,\n",
    "                /*.mstate      =*/ mstate,\n",
    "                /*.cross       =*/ &cross,\n",
    "                /*.n_outputs   =*/ n_outputs,\n",
    "                /*.cb          =*/ graph_get_cb(),\n",
    "            }, gf, gtype);\n",
    "}\n",
    "\n",
    "llm_graph_result_ptr llama_model::build_graph(\n",
    "        const llm_graph_params & params,\n",
    "                   ggml_cgraph * gf,\n",
    "                llm_graph_type   type) const {\n",
    "    --> std::unique_ptr<llm_graph_context> llm = std::make_unique<llm_build_llama>(*this, params, gf);\n",
    "    return std::move(llm->res);\n",
    "}\n",
    "\n",
    "struct llm_build_llama : public llm_graph_context {\n",
    "    llm_build_llama(const llama_model & model, const llm_graph_params & params, ggml_cgraph * gf) :\n",
    "    --> llm_graph_context(params) {}\n",
    "        ctx0 (params.ctx),\n",
    "        mctx (params.mctx),\n",
    "};\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0gjc_f94Bhyi",
   "metadata": {},
   "source": [
    "```c++\n",
    "const int64_t n_embd_head = hparams.n_embd_head_v;\n",
    "ggml_tensor * cur;\n",
    "ggml_tensor * inpL;\n",
    "--> build_inp_embd --> res->add_input(inp), ggml_tensor * inpL = ggml_get_rows(ctx0, tok_embd, inp->tokens);;\n",
    "// inp_pos - contains the positions\n",
    "--> build_inp_pos --> res->add_input(inp), ggml_tensor * inp_pos = inp->pos;\n",
    "--> build_attn_inp_kv_unified --> res->add_input(inp), auto * inp_attn = inp;\n",
    "--> ggml_tensor * inp_out_ids = build_inp_out_ids();\n",
    "const float kq_scale = hparams.f_attention_scale == 0.0f ? 1.0f/sqrtf(float(n_embd_head)) : hparams.f_attention_scale;\n",
    "for (int il = 0; il < n_layer; ++il) {\n",
    "    ggml_tensor * inpSA = inpL;\n",
    "    // norm\n",
    "    cur = build_norm(inpL,\n",
    "            model.layers[il].attn_norm, NULL,\n",
    "            LLM_NORM_RMS, il);\n",
    "    cb(cur, \"attn_norm\", il);\n",
    "    // self-attention\n",
    "    {\n",
    "        // rope freq factors for llama3; may return nullptr for llama2 and other models\n",
    "        ggml_tensor * rope_factors = model.get_rope_factors(cparams, il);\n",
    "        // compute Q and K and RoPE them\n",
    "        ggml_tensor * Qcur = build_lora_mm(model.layers[il].wq, cur);\n",
    "        cb(Qcur, \"Qcur\", il);\n",
    "        ggml_tensor * Kcur = build_lora_mm(model.layers[il].wk, cur);\n",
    "        cb(Kcur, \"Kcur\", il);\n",
    "        ggml_tensor * Vcur = build_lora_mm(model.layers[il].wv, cur);\n",
    "        cb(Vcur, \"Vcur\", il);\n",
    "        Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens);\n",
    "        Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens);\n",
    "        Vcur = ggml_reshape_3d(ctx0, Vcur, n_embd_head, n_head_kv, n_tokens);\n",
    "        Qcur = ggml_rope_ext(\n",
    "                ctx0, Qcur, inp_pos, rope_factors,\n",
    "                n_rot, rope_type, n_ctx_orig, freq_base, freq_scale,\n",
    "                ext_factor, attn_factor, beta_fast, beta_slow\n",
    "                );\n",
    "        Kcur = ggml_rope_ext(\n",
    "                ctx0, Kcur, inp_pos, rope_factors,\n",
    "                n_rot, rope_type, n_ctx_orig, freq_base, freq_scale,\n",
    "                ext_factor, attn_factor, beta_fast, beta_slow\n",
    "                );\n",
    "        cb(Qcur, \"Qcur\", il);\n",
    "        cb(Kcur, \"Kcur\", il);\n",
    "        cb(Vcur, \"Vcur\", il);\n",
    "        --> cur = build_attn(inp_attn, gf,\n",
    "                model.layers[il].wo, model.layers[il].bo,\n",
    "                Qcur, Kcur, Vcur, nullptr, nullptr, kq_scale, il);\n",
    "        cb(cur, \"attn_out\", il);\n",
    "    }\n",
    "    if (il == n_layer - 1) {\n",
    "        // skip computing output for unused tokens\n",
    "        cur   = ggml_get_rows(ctx0,   cur, inp_out_ids); // node_1144\n",
    "        inpSA = ggml_get_rows(ctx0, inpSA, inp_out_ids); // node_1145\n",
    "    }\n",
    "    ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpSA);\n",
    "    cb(ffn_inp, \"ffn_inp\", il);\n",
    "    // feed-forward network (non-MoE)\n",
    "    cur = build_norm(ffn_inp,\n",
    "            model.layers[il].ffn_norm, NULL,\n",
    "            LLM_NORM_RMS, il);\n",
    "    cb(cur, \"ffn_norm\", il);\n",
    "    cur = build_ffn(cur,\n",
    "            model.layers[il].ffn_up,   model.layers[il].ffn_up_b,   NULL,\n",
    "            model.layers[il].ffn_gate, model.layers[il].ffn_gate_b, NULL,\n",
    "            model.layers[il].ffn_down, model.layers[il].ffn_down_b, NULL,\n",
    "            NULL,\n",
    "            LLM_FFN_SILU, LLM_FFN_PAR, il);\n",
    "    cb(cur, \"ffn_out\", il);\n",
    "    cur = ggml_add(ctx0, cur, ffn_inp);\n",
    "    cb(cur, \"ffn_out\", il);\n",
    "    cur = build_cvec(cur, il);\n",
    "    cb(cur, \"l_out\", il);\n",
    "    // input for next layer\n",
    "    inpL = cur;\n",
    "}\n",
    "// cur = inpL;\n",
    "cur = build_norm(cur,\n",
    "        model.output_norm, NULL,\n",
    "        LLM_NORM_RMS, -1);\n",
    "cb(cur, \"result_norm\", -1);\n",
    "res->t_embd = cur;\n",
    "// lm_head\n",
    "cur = build_lora_mm(model.output, cur);\n",
    "cb(cur, \"result_output\", -1);\n",
    "res->t_logits = cur;\n",
    "--> ggml_build_forward_expand --> ggml_build_forward_impl --> ggml_visit_parents;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w--wDN_kJMJm",
   "metadata": {},
   "source": [
    "##### [`llm_graph_context::build_inp_embd`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-graph.cpp#L847)\n",
    "\n",
    "```c++\n",
    "inpL = build_inp_embd(model.tok_embd);\n",
    "----------\n",
    "// input embeddings with optional lora\n",
    "ggml_tensor * llm_graph_context::build_inp_embd(ggml_tensor * tok_embd) const {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0egbXjv1T0jK",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>class llm_graph_input_embd</summary>\n",
    "\n",
    "```c++\n",
    "class llm_graph_input_embd : public llm_graph_input_i {\n",
    "public:\n",
    "    ggml_tensor * tokens = nullptr; // I32 [n_batch]\n",
    "    ggml_tensor * embd   = nullptr; // F32 [n_embd, n_batch]\n",
    "};\n",
    "\n",
    "\n",
    "void llm_graph_input_embd::set_input(const llama_ubatch * ubatch) {\n",
    "    const int64_t n_tokens = ubatch->n_tokens;\n",
    "    ggml_backend_tensor_set(tokens, ubatch->token, 0, n_tokens*ggml_element_size(tokens));\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "const int64_t n_embd = hparams.n_embd;\n",
    "auto inp = std::make_unique<llm_graph_input_embd>();\n",
    "inp->tokens = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, ubatch.n_tokens);\n",
    "ggml_set_input(inp->tokens);\n",
    "    tensor->flags |= GGML_TENSOR_FLAG_INPUT;\n",
    "res->t_tokens = inp->tokens;\n",
    "ggml_tensor * cur = ggml_get_rows(ctx0, tok_embd, inp->tokens);\n",
    "cb(cur, \"inp_embd\", -1);\n",
    "res->add_input(std::move(inp));\n",
    "return cur;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ADE3KY3bLEHM",
   "metadata": {},
   "source": [
    "##### [`llm_graph_context::build_inp_pos`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-graph.cpp#L898)\n",
    "\n",
    "```c++\n",
    "// inp_pos - contains the positions\n",
    "ggml_tensor * inp_pos = build_inp_pos();\n",
    "----------\n",
    "ggml_tensor * llm_graph_context::build_inp_pos() const {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PwO2B7uhUGTX",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>class llm_graph_input_pos</summary>\n",
    "\n",
    "```c++\n",
    "class llm_graph_input_pos : public llm_graph_input_i {\n",
    "public:\n",
    "    ggml_tensor * pos = nullptr; // I32 [n_batch]\n",
    "    const int64_t n_pos_per_embd = 1;\n",
    "};\n",
    "\n",
    "void llm_graph_input_pos::set_input(const llama_ubatch * ubatch) {\n",
    "    // if (ubatch->pos && pos)\n",
    "    const int64_t n_tokens = ubatch->n_tokens;\n",
    "    ggml_backend_tensor_set(pos, ubatch->pos, 0, n_tokens*n_pos_per_embd*ggml_element_size(pos));\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "auto inp = std::make_unique<llm_graph_input_pos>(n_pos_per_embd());\n",
    "auto & cur = inp->pos;\n",
    "cur = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, n_tokens*n_pos_per_embd());\n",
    "ggml_set_input(cur);\n",
    "res->add_input(std::move(inp));\n",
    "return cur;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FAdQS1MRL7_n",
   "metadata": {},
   "source": [
    "##### [`llm_graph_context::build_attn_inp_kv_unified`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-graph.cpp#L1224)\n",
    "\n",
    "```c++\n",
    "auto * inp_attn = build_attn_inp_kv_unified();\n",
    "----------\n",
    "llm_graph_input_attn_kv_unified * llm_graph_context::build_attn_inp_kv_unified() const {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uI4t1QuKUX9l",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>llm_graph_input_attn_kv_unified</summary>\n",
    "\n",
    "```c++\n",
    "class llm_graph_input_attn_kv_unified : public llm_graph_input_i {\n",
    "public:\n",
    "    ggml_tensor * self_kq_mask     = nullptr; // F32 [n_kv, n_batch]\n",
    "    ggml_tensor * self_kq_mask_cnv = nullptr; //     [n_kv, n_batch]\n",
    "\n",
    "    const llama_hparams & hparams;\n",
    "    const llama_cparams & cparams;\n",
    "\n",
    "    const llama_kv_cache_unified_state * kv_state;\n",
    "};\n",
    "\n",
    "void llm_graph_input_attn_kv_unified::set_input(const llama_ubatch * ubatch) {\n",
    "    if (self_kq_mask) {\n",
    "        mctx->set_input_kq_mask(self_kq_mask, ubatch, cparams.causal_attn);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "const auto * mctx_cur = static_cast<const llama_kv_cache_unified_context *>(mctx);\n",
    "auto inp = std::make_unique<llm_graph_input_attn_kv_unified>(hparams, cparams, mctx_cur);\n",
    "const auto n_kv = mctx_cur->get_n_kv(); // 这里应该每一次都会不一样\n",
    "inp->self_kq_mask = ggml_new_tensor_2d(ctx0, GGML_TYPE_F32, n_kv, GGML_PAD(n_tokens, GGML_KQ_MASK_PAD));\n",
    "ggml_set_input(inp->self_kq_mask);\n",
    "inp->self_kq_mask_cnv = cparams.flash_attn ? ggml_cast(ctx0, inp->self_kq_mask, GGML_TYPE_F16) : inp->self_kq_mask;\n",
    "return (llm_graph_input_attn_kv_unified *) res->add_input(std::move(inp));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba439262",
   "metadata": {},
   "source": [
    "##### [`llm_graph_context::build_inp_out_ids`](src/llama-graph.cpp#L925)\n",
    "\n",
    "```c++\n",
    "ggml_tensor * inp_out_ids = build_inp_out_ids();\n",
    "----------\n",
    "ggml_tensor * llm_graph_context::build_inp_out_ids() const {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df36fc78",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>class llm_graph_input_out_ids</summary>\n",
    "\n",
    "```c++\n",
    "class llm_graph_input_out_ids : public llm_graph_input_i {\n",
    "public:\n",
    "    ggml_tensor * out_ids; // I32 [n_outputs]\n",
    "\n",
    "    const llama_hparams & hparams;\n",
    "    const llama_cparams & cparams;\n",
    "\n",
    "    const int32_t n_outputs;\n",
    "};\n",
    "\n",
    "\n",
    "void llm_graph_input_out_ids::set_input(const llama_ubatch * ubatch) {\n",
    "    const int64_t n_tokens = ubatch->n_tokens;\n",
    "\n",
    "    int32_t * data = (int32_t *) out_ids->data;\n",
    "\n",
    "    if (n_outputs == n_tokens) {\n",
    "        for (int i = 0; i < n_tokens; ++i) {\n",
    "            data[i] = i;\n",
    "        }\n",
    "    } else if (ubatch->output) {\n",
    "        int32_t n_outputs = 0;\n",
    "        for (int i = 0; i < n_tokens; ++i) {\n",
    "            if (ubatch->output[i]) {\n",
    "                data[n_outputs++] = i;\n",
    "            }\n",
    "        }\n",
    "    } else if (n_outputs == 1) {\n",
    "        // only keep last output\n",
    "        data[0] = n_tokens - 1;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "auto inp = std::make_unique<llm_graph_input_out_ids>(hparams, cparams, n_outputs);\n",
    "auto & cur = inp->out_ids;\n",
    "cur = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, n_outputs);\n",
    "ggml_set_input(cur);\n",
    "res->add_input(std::move(inp));\n",
    "return cur;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bQB42zMPNdj",
   "metadata": {},
   "source": [
    "##### [`llm_graph_context::build_attn`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-graph.cpp#L1255)\n",
    "\n",
    "\n",
    "```c++\n",
    "cur = build_attn(inp_attn, gf,\n",
    "                model.layers[il].wo, model.layers[il].bo,\n",
    "                Qcur, Kcur, Vcur, nullptr, nullptr, kq_scale, il);\n",
    "----------\n",
    "ggml_tensor * llm_graph_context::build_attn(\n",
    "        llm_graph_input_attn_kv_unified * inp,\n",
    "        ggml_cgraph * gf,\n",
    "        ggml_tensor * wo,\n",
    "        ggml_tensor * wo_b,\n",
    "        ggml_tensor * q_cur,\n",
    "        ggml_tensor * k_cur,\n",
    "        ggml_tensor * v_cur,\n",
    "        ggml_tensor * kq_b,\n",
    "        ggml_tensor * v_mla,\n",
    "            float     kq_scale,\n",
    "            int       il) const {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HtLNrXSJVJhh",
   "metadata": {},
   "source": [
    "```c++\n",
    "// these nodes are added to the graph together so that they are not reordered\n",
    "// by doing so, the number of splits in the graph is reduced\n",
    "--> ggml_build_forward_expand(gf, q_cur) --> ggml_build_forward_impl --> ggml_visit_parents;\n",
    "--> ggml_build_forward_expand(gf, k_cur) --> ggml_build_forward_impl --> ggml_visit_parents;\n",
    "--> ggml_build_forward_expand(gf, v_cur) --> ggml_build_forward_impl --> ggml_visit_parents;\n",
    "const auto * mctx_cur = static_cast<const llama_kv_cache_unified_context *>(mctx);\n",
    "// store to KV cache\n",
    "--> ggml_build_forward_expand(gf, kv_state->cpy_k(ctx0, k_cur, il)) --> ggml_build_forward_impl --> ggml_visit_parents;\n",
    "--> ggml_build_forward_expand(gf, kv_state->cpy_v(ctx0, v_cur, il)) --> ggml_build_forward_impl --> ggml_visit_parents;\n",
    "const auto & kq_mask = inp->get_kq_mask();\n",
    "ggml_tensor * q = q_cur;\n",
    "ggml_tensor * k = kv_state->get_k(ctx0, il);\n",
    "ggml_tensor * v = kv_state->get_v(ctx0, il);\n",
    "--> lm_graph_context::build_attn_mha --> ggml_tensor * cur;\n",
    "cb(cur, \"kqv_out\", il);\n",
    "cur = build_lora_mm(wo, cur);\n",
    "return cur;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NVM9wnjRqChc",
   "metadata": {},
   "source": [
    "- [`llama_kv_cache_unified_state::cpy_k`]()\n",
    "- [`llama_kv_cache_unified_state::cpy_v`]()\n",
    "- [`ggml_cpy_impl`]()\n",
    "\n",
    "```c++\n",
    "kv_state->cpy_k(ctx0, k_cur, il);\n",
    "kv_state->cpy_v(ctx0, v_cur, il);\n",
    "----------\n",
    "ggml_tensor * llama_kv_cache_unified_state::cpy_k(ggml_context * ctx, ggml_tensor * k_cur, int32_t il) const\n",
    "    return kv->cpy_k(ctx, k_cur, il, head);\n",
    "\n",
    "ggml_tensor * llama_kv_cache_unified_state::cpy_v(ggml_context * ctx, ggml_tensor * v_cur, int32_t il) const\n",
    "    return kv->cpy_v(ctx, v_cur, il, head);\n",
    "\n",
    "ggml_tensor * llama_kv_cache_unified::cpy_k(ggml_context * ctx, ggml_tensor * k_cur, int32_t il, uint32_t head_cur) const {\n",
    "    const int32_t ikv = map_layer_ids.at(il);\n",
    "\n",
    "    auto * k = layers[ikv].k;\n",
    "\n",
    "    const int64_t n_tokens = k_cur->ne[2];\n",
    "\n",
    "    ggml_tensor * k_view = ggml_view_1d(ctx, k,\n",
    "            n_tokens*hparams.n_embd_k_gqa(il),\n",
    "            ggml_row_size(k->type, hparams.n_embd_k_gqa(il))*head_cur);\n",
    "\n",
    "    --> return ggml_cpy(ctx, k_cur, k_view);\n",
    "}\n",
    "\n",
    "ggml_tensor * llama_kv_cache_unified::cpy_v(ggml_context * ctx, ggml_tensor * v_cur, int32_t il, uint32_t head_cur) const {\n",
    "    const int32_t ikv = map_layer_ids.at(il);\n",
    "\n",
    "    auto * v = layers[ikv].v;\n",
    "\n",
    "    const int64_t n_tokens = v_cur->ne[2];\n",
    "\n",
    "    v_cur = ggml_reshape_2d(ctx, v_cur, hparams.n_embd_v_gqa(il), n_tokens);\n",
    "\n",
    "    ggml_tensor * v_view = nullptr;\n",
    "\n",
    "    if (!v_trans) {\n",
    "        v_view = ggml_view_1d(ctx, v,\n",
    "                n_tokens*hparams.n_embd_v_gqa(il),\n",
    "                ggml_row_size(v->type, hparams.n_embd_v_gqa(il))*head_cur);\n",
    "    --> } else { //一般走这个路径\n",
    "        // note: the V cache is transposed when not using flash attention\n",
    "        v_view = ggml_view_2d(ctx, v, n_tokens, hparams.n_embd_v_gqa(il),\n",
    "                (v->ne[1])*ggml_element_size(v),\n",
    "                (head_cur)*ggml_element_size(v));\n",
    "\n",
    "        v_cur = ggml_transpose(ctx, v_cur);\n",
    "    }\n",
    "\n",
    "    --> return ggml_cpy(ctx, v_cur, v_view);\n",
    "}\n",
    "\n",
    "struct ggml_tensor * ggml_cpy(\n",
    "        struct ggml_context * ctx,\n",
    "        struct ggml_tensor * a,\n",
    "        struct ggml_tensor * b) {\n",
    "    --> return ggml_cpy_impl(ctx, a, b);\n",
    "}\n",
    "\n",
    "static struct ggml_tensor * ggml_cpy_impl(\n",
    "        struct ggml_context * ctx,\n",
    "        struct ggml_tensor  * a,\n",
    "        struct ggml_tensor  * b) {\n",
    "    // make a view of the destination\n",
    "    struct ggml_tensor * result = ggml_view_tensor(ctx, b);\n",
    "    if (strlen(b->name) > 0) {\n",
    "        ggml_format_name(result, \"%s (copy of %s)\", b->name, a->name);\n",
    "    } else {\n",
    "        ggml_format_name(result, \"%s (copy)\", a->name);\n",
    "    }\n",
    "\n",
    "    result->op     = GGML_OP_CPY;\n",
    "    result->src[0] = a;\n",
    "    result->src[1] = b;\n",
    "\n",
    "    return result;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wHCp4rkElmB7",
   "metadata": {},
   "source": [
    "[`llm_graph_context::build_attn_mha`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-graph.cpp#L1061)\n",
    "\n",
    "```c++\n",
    "ggml_tensor * cur = build_attn_mha(gf, q, k, v, kq_b, kq_mask, v_mla, kq_scale);\n",
    "----------\n",
    "ggml_tensor * llm_graph_context::build_attn_mha(\n",
    "         ggml_cgraph * gf,\n",
    "         ggml_tensor * q,\n",
    "         ggml_tensor * k,\n",
    "         ggml_tensor * v,\n",
    "         ggml_tensor * kq_b,\n",
    "         ggml_tensor * kq_mask,\n",
    "         ggml_tensor * v_mla,\n",
    "             float     kq_scale) const {\n",
    "    const bool v_trans = v->nb[1] > v->nb[2];\n",
    "\n",
    "    q = ggml_permute(ctx0, q, 0, 2, 1, 3);\n",
    "    k = ggml_permute(ctx0, k, 0, 2, 1, 3);\n",
    "    v = ggml_permute(ctx0, v, 0, 2, 1, 3);\n",
    "\n",
    "    const auto n_tokens = q->ne[1];\n",
    "    const auto n_head   = q->ne[2];\n",
    "    ggml_tensor * kq = ggml_mul_mat(ctx0, k, q);\n",
    "    // note: this op tends to require high floating point range\n",
    "    // while for some models F16 is enough, for others it is not, so we default to F32 here\n",
    "    ggml_mul_mat_set_prec(kq, GGML_PREC_F32);\n",
    "    kq = ggml_soft_max_ext(ctx0, kq, kq_mask, kq_scale, hparams.f_max_alibi_bias);\n",
    "    ggml_tensor * kqv = ggml_mul_mat(ctx0, v, kq);\n",
    "    ggml_tensor * cur = ggml_permute(ctx0, kqv, 0, 2, 1, 3);\n",
    "    cur = ggml_cont_2d(ctx0, cur, cur->ne[0]*n_head, n_tokens);\n",
    "    ggml_build_forward_expand(gf, cur);\n",
    "    return cur;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gJ8MiSGWTSPC",
   "metadata": {},
   "source": [
    "##### [`ggml_visit_parents`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L5794)\n",
    "\n",
    "> 这函数主要是为了构建 gf，也即 ggml_cgraph * cgraph;\n",
    "> cgraph->leafs[cgraph->n_leafs] = node; cgraph->n_leafs++; cgraph->nodes[cgraph->n_nodes] = node; cgraph->n_nodes++;\n",
    "\n",
    "```c++\n",
    "ggml_build_forward_expand(gf, cur);\n",
    "----------\n",
    "void ggml_build_forward_expand(struct ggml_cgraph * cgraph, struct ggml_tensor * tensor) {\n",
    "    --> ggml_build_forward_impl(cgraph, tensor, true);\n",
    "}\n",
    "\n",
    "static void ggml_build_forward_impl(struct ggml_cgraph * cgraph, struct ggml_tensor * tensor, bool expand) {\n",
    "    const int n0 = cgraph->n_nodes;\n",
    "    --> ggml_visit_parents(cgraph, tensor);\n",
    "    const int n_new = cgraph->n_nodes - n0;\n",
    "}\n",
    "\n",
    "static void ggml_visit_parents(struct ggml_cgraph * cgraph, struct ggml_tensor * node) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KmMaxLhUn9KV",
   "metadata": {},
   "source": [
    "```c++\n",
    "// check if already visited\n",
    "if (ggml_hash_insert(&cgraph->visited_hash_set, node) == GGML_HASHSET_ALREADY_EXISTS) {\n",
    "    return;\n",
    "}\n",
    "\n",
    "for (int i = 0; i < GGML_MAX_SRC; ++i) {\n",
    "    const int k =\n",
    "        (cgraph->order == GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT) ? i :\n",
    "        (cgraph->order == GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT) ? (GGML_MAX_SRC-1-i) :\n",
    "        /* unknown order, just fall back to using i*/ i;\n",
    "    if (node->src[k]) {\n",
    "        ggml_visit_parents(cgraph, node->src[k]);\n",
    "    }\n",
    "}\n",
    "\n",
    "if (node->op == GGML_OP_NONE && !(node->flags & GGML_TENSOR_FLAG_PARAM)) {\n",
    "    // reached a leaf node, not part of the gradient graph (e.g. a constant)\n",
    "    if (strlen(node->name) == 0) {\n",
    "        ggml_format_name(node, \"leaf_%d\", cgraph->n_leafs);\n",
    "    }\n",
    "    cgraph->leafs[cgraph->n_leafs] = node;\n",
    "    cgraph->n_leafs++;\n",
    "} else {\n",
    "    if (strlen(node->name) == 0) {\n",
    "        ggml_format_name(node, \"node_%d\", cgraph->n_nodes);\n",
    "    }\n",
    "    cgraph->nodes[cgraph->n_nodes] = node;\n",
    "    cgraph->n_nodes++;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8GQzND6_PTl",
   "metadata": {},
   "source": [
    "#### [`ggml_backend_sched_reserve`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1549)\n",
    "\n",
    "```c++\n",
    "ggml_backend_sched_reserve(sched.get(), gf);\n",
    "----------\n",
    "bool ggml_backend_sched_reserve(ggml_backend_sched_t sched, struct ggml_cgraph * measure_graph) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fCgF3QMosNtv",
   "metadata": {},
   "source": [
    "```c++\n",
    "--> ggml_backend_sched_split_graph --> sched->splits, sched->graph, sched->node_backend_ids, sched->leaf_backend_ids;\n",
    "--> ggml_backend_sched_synchronize;\n",
    "--> ggml_gallocr_reserve_n --> sched->galloc;\n",
    "ggml_backend_sched_reset(sched);\n",
    "return true;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gNhRm_HisqMZ",
   "metadata": {},
   "source": [
    "##### [`ggml_backend_sched_split_graph`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L865)\n",
    "\n",
    "> 分配每个node/leaf的后端类型，构建动态的部署计算图，并且归纳出需要split的node\n",
    "\n",
    "> 还有就是初始化自己域内的ctx变量，并且把需要split的节点在ctx上开辟出来，而且看起来有很多余量\n",
    "\n",
    "> 根据后端资源的分配构建动态图，存入sched->graph中\n",
    "\n",
    "```c++\n",
    "ggml_backend_sched_split_graph(sched, measure_graph);\n",
    "----------\n",
    "// assigns backends to ops and splits the graph into subgraphs that can be computed on the same backend\n",
    "static void ggml_backend_sched_split_graph(ggml_backend_sched_t sched, struct ggml_cgraph * graph) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UlW7C9WvvdYd",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_backend_sched_split</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_sched_split {\n",
    "    int backend_id;\n",
    "    int i_start;\n",
    "    int i_end;\n",
    "    struct ggml_tensor * inputs[GGML_SCHED_MAX_SPLIT_INPUTS];\n",
    "    int n_inputs;\n",
    "    // graph view of this split\n",
    "    struct ggml_cgraph graph;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "// reset splits\n",
    "sched->n_splits = 0;\n",
    "sched->n_graph_inputs = 0;\n",
    "sched->is_reset = false;\n",
    "\n",
    "struct ggml_init_params params = {\n",
    "    /* .mem_size =   */ sched->context_buffer_size,\n",
    "    /* .mem_buffer = */ sched->context_buffer,\n",
    "    /* .no_alloc =   */ true\n",
    "};\n",
    "\n",
    "ggml_free(sched->ctx);\n",
    "sched->ctx = ggml_init(params);\n",
    "// pass 1: assign backends to ops with pre-allocated inputs\n",
    "for (int i = 0; i < graph->n_leafs; i++)\n",
    "    int * leaf_backend_id = &sched->hv_tensor_backend_ids[hash_id(graph->leafs[i])];\n",
    "    if (*leaf_backend_id == -1)\n",
    "    --> *leaf_backend_id = ggml_backend_sched_backend_id_from_cur(sched, graph->leafs[i]);\n",
    "for (int i = 0; i < graph->n_nodes; i++)\n",
    "    int * node_backend_id = sched->hv_tensor_backend_ids[hash_id(graph->nodes[i])]\n",
    "    if (*node_backend_id == -1)\n",
    "    --> *node_backend_id = ggml_backend_sched_backend_id_from_cur(sched, graph->nodes[i]);\n",
    "// pass 2: expand current backend assignments\n",
    "// assign the same backend to adjacent nodes\n",
    "// expand gpu backends (i.e. non last prio) up and down, ignoring cpu (the lowest priority backend)\n",
    "// thus, cpu will never be used unless weights are on cpu, or there are no gpu ops between cpu ops\n",
    "// ops unsupported by the backend being expanded will be left unassigned so that they can be assigned later when the locations of its inputs are known\n",
    "// expand gpu down\n",
    "for (int i = 0; i < graph->n_nodes; i++)\n",
    "    if (*node_backend_id == sched->n_backends - 1)\n",
    "        cur_backend_id = -1; // skip cpu (lowest prio backend)\n",
    "    ggml_backend_sched_set_if_supported(sched, node, cur_backend_id, node_backend_id); //SET_CAUSE(node, \"2.sup\");\n",
    "    //node_id = [5,8,28,29,32,34,36,37,41,44,58,59,60,62,64, so on...]\n",
    "// expand gpu up\n",
    "for (int i = graph->n_nodes - 1; i >= 0; i--)\n",
    "    if (*node_backend_id == sched->n_backends - 1)\n",
    "        cur_backend_id = -1; // skip cpu (lowest prio backend)\n",
    "    ggml_backend_sched_set_if_supported(sched, node, cur_backend_id, node_backend_id); //SET_CAUSE(node, \"2.sup\");\n",
    "    //node_id = [26,24,23,22,1]\n",
    "// expand rest down 没有起作用\n",
    "// expand rest up 没有起作用\n",
    "////pass 3没有起作用\n",
    "// pass 3: upgrade nodes to higher prio backends with compatible buffer types\n",
    "// if the tensor is already in the same buffer type (*) as another higher priority backend, we should move it there\n",
    "// however, we also need to verify that the sources are in compatible buffer types\n",
    "// (*) the actual requirement is more relaxed, the buffer type of the backend should be supported by all the users of this tensor further down the graph\n",
    "// however, this is slow to verify, so we have a more strict requirement that the buffer type is the same\n",
    "// this is not uncommon since multiple backends can use host memory, with the same buffer type (eg. BLAS and CPU)\n",
    "// additionally, set remaining unassigned nodes to the backend with the most supported inputs\n",
    "// only nodes that could not be assigned during expansion due to the backend not supporting the op should be unassigned at this point\n",
    "// pass 4: assign backends to remaining src from dst and view_src\n",
    "////开始针对view_src了，后半段遍历SRC的没起作用\n",
    "for (int i = 0; i < graph->n_nodes; i++)\n",
    "    struct ggml_tensor * node = graph->nodes[i];\n",
    "    int * cur_backend_id = &tensor_backend_id(node);\n",
    "    if (node->view_src != NULL && *cur_backend_id == -1) *cur_backend_id = tensor_backend_id(node->view_src); //SET_CAUSE(node, \"4.vsrc\");\n",
    "// pass 5: split graph, find tensors that need to be copied\n",
    "int i_split = 0;\n",
    "struct ggml_backend_sched_split * split = &sched->splits[0];\n",
    "// find the backend of the first split, skipping view ops\n",
    "int cur_backend_id = split->backend_id = tensor_backend_id(graph->nodes[0]);\n",
    "split->i_start = 0;\n",
    "split->n_inputs = 0;\n",
    "for (int i = 0; i < graph->n_nodes; i++)\n",
    "    // check if we should start a new split based on the sources of the current node\n",
    "    if (node_backend_id == cur_backend_id && split->n_inputs > 0)\n",
    "        for (int j = 0; j < GGML_MAX_SRC; j++) struct ggml_tensor * src = node->src[j]; size_t src_id = hash_id(src);\n",
    "        // check if a weight is on a different and incompatible backend by starting a new split,\n",
    "        // the memory of the previously offloaded weights can be reused\n",
    "        if (src->buffer != NULL && src->buffer->usage == GGML_BACKEND_BUFFER_USAGE_WEIGHTS)\n",
    "            //如果出现已经有split了，且weight不在当前的backend上，且当前的backend的不支持，需要新的split\n",
    "            sched->hv_tensor_backend_ids[src_id] != cur_backend_id && \\\n",
    "            --> ggml_backend_sched_buffer_supported --> ggml_backend_supports_buft;\n",
    "    if (node_backend_id != cur_backend_id || need_new_split)\n",
    "        split->i_end = i;\n",
    "        i_split++;\n",
    "        split = &sched->splits[i_split];\n",
    "        split->backend_id = node_backend_id;\n",
    "        split->i_start = i;\n",
    "        split->n_inputs = 0;\n",
    "        cur_backend_id = node_backend_id;\n",
    "    // find inputs that are not on the same backend\n",
    "    for (int j = 0; j < GGML_MAX_SRC; j++)\n",
    "        struct ggml_tensor * src = node->src[j];\n",
    "        size_t src_id = hash_id(src);\n",
    "        sched->hv_tensor_backend_ids[src_id] != cur_backend_id && \\\n",
    "        --> !ggml_backend_sched_buffer_supported --> ggml_backend_supports_buft; //这个条件必不可能为true啊\n",
    "            // create a copy of the input in the split's backend\n",
    "            if (sched->hv_tensor_copies[hash_set.size=src_id][n_backends=cur_backend_id][n_copies=0] == NULL)\n",
    "                ggml_backend_t backend = sched->backends[cur_backend_id];\n",
    "                // for (int c = 0; c < sched->n_copies; c++)\n",
    "                --> ggml_dup_tensor_layout(sched->ctx, src) --> struct ggml_tensor * tensor_copy;\n",
    "                    return struct ggml_tensor * dup = ggml_dup_tensor(ctx, tensor);\n",
    "                    for (int i = 0; i < GGML_MAX_DIMS; i++) dup->nb[i] = tensor->nb[i];\n",
    "                sched->hv_tensor_copies[hash_set.size=src_id][n_backends=cur_backend_id][n_copies=0] = tensor_copy;\n",
    "                int n_inputs = split->n_inputs++;\n",
    "                split->inputs[n_inputs] = src;\n",
    "            //后续还承接了 ROPE/SOFT_MAX 的处理\n",
    "            // node->src[j] = tensor_id_copy(src_id, cur_backend_id, sched->cur_copy=0);\n",
    "            node->src[j] = tensor_copy\n",
    "split->i_end = graph->n_nodes;\n",
    "sched->n_splits = i_split + 1;\n",
    "// swap node_backend_ids and leaf _backend_ids with prevs\n",
    "int * tmp = sched->node_backend_ids;\n",
    "sched->node_backend_ids = sched->prev_node_backend_ids;\n",
    "sched->prev_node_backend_ids = tmp;\n",
    "tmp = sched->leaf_backend_ids;\n",
    "sched->leaf_backend_ids = sched->prev_leaf_backend_ids;\n",
    "sched->prev_leaf_backend_ids = tmp;\n",
    "// --> for `ggml_backend_sched_alloc_splits` func\n",
    "int graph_size = std::max(graph->n_nodes, graph->n_leafs) + sched->n_splits*GGML_SCHED_MAX_SPLIT_INPUTS*2*sched->n_copies;\n",
    "// 这里第二次有可能就不执行了\n",
    "if (sched->graph.size < graph_size)\n",
    "    sched->graph.size = graph_size;\n",
    "    sched->graph.nodes = (ggml_tensor **) realloc(sched->graph.nodes, graph_size * sizeof(struct ggml_tensor *));\n",
    "    sched->graph.leafs = (ggml_tensor **) realloc(sched->graph.leafs, graph_size * sizeof(struct ggml_tensor *));\n",
    "sched->graph.n_nodes = 0;\n",
    "sched->graph.n_leafs = 0;\n",
    "struct ggml_cgraph * graph_copy = &sched->graph;\n",
    "for (int i = 0; i < sched->n_splits; i++) {\n",
    "    struct ggml_backend_sched_split * split = &sched->splits[i];\n",
    "    // 这里也非常巧妙 *key*\n",
    "    --> split->graph = ggml_graph_view(graph, split->i_start, split->i_end);\n",
    "    // add inputs to the graph copy so that they are allocated by ggml-alloc at the start of the split\n",
    "    for (int j = 0; j < split->n_inputs; j++) {\n",
    "        struct ggml_tensor * input = split->inputs[j];\n",
    "        const size_t input_id = hash_id(input);\n",
    "        struct ggml_tensor * input_cpy = sched->hv_tensor_copies[input_id][split->backend_id][0];\n",
    "        // add a dependency to the input source so that it is not freed before the copy is done\n",
    "        // 就是占个位置，建立依赖关系，以免被后面的tensor给覆盖掉\n",
    "        struct ggml_tensor * input_dep = ggml_view_tensor(sched->ctx, input);\n",
    "        input_dep->src[0] = input;\n",
    "        sched->node_backend_ids[graph_copy->n_nodes] = sched->hv_tensor_backend_ids[input_id];\n",
    "        graph_copy->nodes[graph_copy->n_nodes++] = input_dep;\n",
    "        // add a dependency to the input copy so that it is allocated at the start of the split\n",
    "        sched->node_backend_ids[graph_copy->n_nodes] = split->backend_id;\n",
    "        graph_copy->nodes[graph_copy->n_nodes++] = input_cpy;\n",
    "    }\n",
    "    for (int j = split->i_start; j < split->i_end; j++) {\n",
    "        assert(graph_copy->size > graph_copy->n_nodes);\n",
    "        sched->node_backend_ids[graph_copy->n_nodes] = tensor_backend_id(graph->nodes[j]);\n",
    "        graph_copy->nodes[graph_copy->n_nodes++] = graph->nodes[j];\n",
    "    }\n",
    "}\n",
    "// add leafs from the original graph\n",
    "for (int i = 0; i < graph->n_leafs; i++) {\n",
    "    struct ggml_tensor * leaf = graph->leafs[i];\n",
    "    sched->leaf_backend_ids[graph_copy->n_leafs] = tensor_backend_id(leaf);\n",
    "    assert(graph_copy->size > graph_copy->n_leafs);\n",
    "    graph_copy->leafs[graph_copy->n_leafs++] = leaf;\n",
    "}\n",
    "```\n",
    "\n",
    "```log\n",
    ">>> p *leaf_buffer_ids@graph->n_leafs\n",
    "$1094 = {[0] = 1 <repeats 11 times>, [11] = 0, [12] = 1, [13] = 1, [14] = 1, [15] = 1, [16] = 1, [17] = 0 <repeats 336 times>, [353] = 1, [354] = 0, [355] = 0, [356] = 0, [357] = 1, [358] = 0, [359] = 0, [360] = 1}\n",
    ">>> p *node_buffer_ids@graph->n_nodes\n",
    "$1095 = {[0] = 1, [1] = 1, [2] = 0, [3] = 0, [4] = 1, [5] = 0, [6] = 0, [7] = 1, [8] = 0, [9] = 0, [10] = 0, [11] = 1, [12] = 0, [13] = 1, [14] = 0, [15] = 0, [16] = 1, [17] = 0, [18] = 0, [19] = 0, [20] = 1, [21] = 0, [22] = 0, [23] = 0, [24] = 0, [25] = 1, [26] = 0, [27] = 1, [28] = 0, [29] = 1, [30] = 1, [31] = 0, [32] = 0, [33] = 1, [34] = 1, [35] = 1, [36] = 1, [37] = 1, [38] = 1, [39] = 0, [40] = 1, [41] = 0, [42] = 1, [43] = 0, [44] = 1, [45] = 0, [46] = 0, [47] = 0, [48] = 0, [49] = 0, [50] = 0, [51] = 1, [52] = 0, [53] = 0, [54] = 0, [55] = 0, [56] = 1, [57] = 0, [58] = 0, [59] = 1, [60] = 0, [61] = 0, [62] = 0, [63] = 1, [64] = 0, [65] = 0, [66] = 0, [67] = 1, [68] = 0, [69] = 1, [70] = 0 <repeats 1122 times>, [1192] = 1, [1193] = 0, [1194] = 0, [1195] = 1, [1196] = 0, [1197] = 0}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6fbca",
   "metadata": {},
   "source": [
    "[`ggml_backend_sched_backend_id_from_cur`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L730)\n",
    "\n",
    "```c++\n",
    "// pass 1: assign backends to ops with pre-allocated inputs\n",
    "*leaf_backend_id = ggml_backend_sched_backend_id_from_cur(sched, leaf);\n",
    "*node_backend_id = ggml_backend_sched_backend_id_from_cur(sched, node);\n",
    "----------\n",
    "static int ggml_backend_sched_backend_id_from_cur(ggml_backend_sched_t sched, struct ggml_tensor * tensor) {\n",
    "    // assign pre-allocated nodes to their backend\n",
    "    --> ggml_backend_sched_backend_from_buffer(sched, tensor, tensor) --> int cur_backend_id;\n",
    "        ggml_backend_buffer_t buffer = tensor->view_src ? tensor->view_src->buffer : tensor->buffer;\n",
    "        if (buffer == NULL) return -1;\n",
    "        // find highest prio backend that supports the buffer type and the op\n",
    "        for (int i = 0; i < sched->n_backends; i++)\n",
    "            if (ggml_backend_supports_buft(sched->backends[i], buffer->buft) && ggml_backend_supports_op(sched->backends[i], op))\n",
    "                return i;\n",
    "    return cur_backend_id; // SET_CAUSE(tensor, \"1.dst\");\n",
    "    if (tensor->view_src != NULL) // view_src\n",
    "        --> ggml_backend_sched_backend_from_buffer(sched, tensor->view_src, tensor) --> int cur_backend_id;\n",
    "    return cur_backend_id; // SET_CAUSE(tensor, \"1.vsrc\");\n",
    "    if (tensor->buffer || (tensor->view_src && tensor->view_src->buffer))\n",
    "        GGML_ABORT;// since the tensor is pre-allocated, it cannot be moved to another backend\n",
    "    if (tensor->flags & GGML_TENSOR_FLAG_INPUT) // graph input\n",
    "        cur_backend_id = sched->n_backends - 1; // last backend (assumed CPU)\n",
    "    for (int i = 0; i < GGML_MAX_SRC; i++) // operations with weights are preferably run on the same backend as the weights\n",
    "        const struct ggml_tensor * src = tensor->src[i];\n",
    "        // skip ROPE since the rope freqs tensor is too small to choose a backend based on it not an ideal solution\n",
    "        if (tensor->op != GGML_OP_ROPE && src->buffer != NULL && src->buffer->usage == GGML_BACKEND_BUFFER_USAGE_WEIGHTS)\n",
    "            --> ggml_backend_sched_backend_from_buffer(sched, src, tensor) --> int src_backend_id;\n",
    "            // check if a backend with higher prio wants to offload the op\n",
    "            if (sched->op_offload && src_backend_id == sched->n_backends - 1 && ggml_backend_buffer_is_host(src->buffer))\n",
    "            for (int b = 0; b < src_backend_id; b++)\n",
    "            if (ggml_backend_supports_op(sched->backends[b], tensor) && ggml_backend_offload_op(sched->backends[b], tensor))\n",
    "                // 这里是区分PP和TG的关键！！！ *key*\n",
    "                return b; // SET_CAUSE(tensor, \"1.off\");\n",
    "            return src_backend_id; // SET_CAUSE(tensor, \"1.wgt%d\", i);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2b223",
   "metadata": {},
   "source": [
    "```c++\n",
    "bool ggml_backend_supports_buft(ggml_backend_t backend, ggml_backend_buffer_type_t buft) {\n",
    "    return ggml_backend_dev_supports_buft(backend->device, buft);\n",
    "}\n",
    "bool ggml_backend_dev_supports_buft(ggml_backend_dev_t device, ggml_backend_buffer_type_t buft) {\n",
    "    return device->iface.supports_buft(device, buft);\n",
    "}\n",
    "static bool ggml_backend_cuda_device_supports_buft(ggml_backend_dev_t dev, ggml_backend_buffer_type_t buft) {\n",
    "    ggml_backend_cuda_device_context * dev_ctx = (ggml_backend_cuda_device_context *) dev->context;\n",
    "    const bool integrated = ggml_cuda_info().devices[dev_ctx->device].integrated;\n",
    "    return (((ggml_backend_buft_is_cuda(buft) || ggml_backend_buft_is_cuda_split(buft)) && buft->device == dev) || (integrated && ggml_backend_buft_is_cuda_host(buft)));\n",
    "}\n",
    "static bool ggml_backend_cpu_device_supports_buft(ggml_backend_dev_t dev, ggml_backend_buffer_type_t buft) {\n",
    "    return ggml_backend_buft_is_host(buft) || ggml_backend_cpu_is_extra_buffer_type(buft);\n",
    "}\n",
    "\n",
    "bool ggml_backend_supports_op(ggml_backend_t backend, const struct ggml_tensor * op) {\n",
    "    return ggml_backend_dev_supports_op(backend->device, op);\n",
    "}\n",
    "bool ggml_backend_dev_supports_op(ggml_backend_dev_t device, const struct ggml_tensor * op) {\n",
    "    return device->iface.supports_op(device, op);\n",
    "    --> ggml_backend_cuda_device_supports_op;\n",
    "    --> ggml_backend_cpu_device_supports_op;\n",
    "}\n",
    "\n",
    "bool ggml_backend_buffer_is_host(ggml_backend_buffer_t buffer) {\n",
    "    return ggml_backend_buft_is_host(ggml_backend_buffer_get_type(buffer));\n",
    "}\n",
    "bool ggml_backend_buft_is_host(ggml_backend_buffer_type_t buft) {\n",
    "    if (buft->iface.is_host) {\n",
    "        return buft->iface.is_host(buft);\n",
    "    }\n",
    "    return false;\n",
    "}\n",
    "static bool ggml_backend_cpu_buffer_type_is_host(ggml_backend_buffer_type_t buft) {\n",
    "    return true;\n",
    "}\n",
    "\n",
    "bool ggml_backend_offload_op(ggml_backend_t backend, const struct ggml_tensor * op) {\n",
    "    return ggml_backend_dev_offload_op(backend->device, op);\n",
    "}\n",
    "bool ggml_backend_dev_offload_op(ggml_backend_dev_t device, const struct ggml_tensor * op) {\n",
    "    if (device->iface.offload_op != NULL) {\n",
    "        return device->iface.offload_op(device, op);\n",
    "    }\n",
    "    return false;\n",
    "}\n",
    "static bool ggml_backend_cuda_device_offload_op(ggml_backend_dev_t dev, const ggml_tensor * op) {\n",
    "    const int min_batch_size = 32;\n",
    "    return get_op_batch_size(op) >= min_batch_size;\n",
    "        --> static int64_t get_op_batch_size(const ggml_tensor * op)\n",
    "        switch (op->op) {\n",
    "            case GGML_OP_GET_ROWS:\n",
    "                return 0;\n",
    "            case GGML_OP_MUL_MAT:\n",
    "                return op->ne[1];\n",
    "            case GGML_OP_MUL_MAT_ID:\n",
    "            case GGML_OP_ROPE:\n",
    "            case GGML_OP_ROPE_BACK:\n",
    "                return op->ne[2];\n",
    "            default:\n",
    "                return ggml_nrows(op);\n",
    "        }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9udilRb2tF3-",
   "metadata": {},
   "source": [
    "##### [`ggml_backend_sched_synchronize`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1599)\n",
    "\n",
    "```c++\n",
    "ggml_backend_sched_synchronize(sched);\n",
    "----------\n",
    "void ggml_backend_sched_synchronize(ggml_backend_sched_t sched) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K2LknXny8lHb",
   "metadata": {},
   "source": [
    "```c++\n",
    "for (int i = 0; i < sched->n_backends; i++) {\n",
    "    --> ggml_backend_synchronize(sched->backends[i]);\n",
    "}\n",
    "if (!sched->is_alloc) {\n",
    "    // if the graph is not already allocated, always use copy 0 after a synchronization\n",
    "    // this ensures that during generation the same copy is used every time,\n",
    "    // which avoids changes in the graph that could cause CUDA or other graphs to be disabled\n",
    "    sched->cur_copy = 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23b2a2",
   "metadata": {},
   "source": [
    "[`ggml_backend_cuda_synchronize`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu:2497)\n",
    "\n",
    "```c++\n",
    "ggml_backend_synchronize(sched->backends[i]);\n",
    "----------\n",
    "void ggml_backend_synchronize(ggml_backend_t backend) {\n",
    "    if (backend->iface.synchronize == NULL) return; //CPU\n",
    "    backend->iface.synchronize(backend); //GPU\n",
    "}\n",
    "static void ggml_backend_cuda_synchronize(ggml_backend_t backend) {\n",
    "    ggml_backend_cuda_context * cuda_ctx = (ggml_backend_cuda_context *)backend->context;\n",
    "    --> cudaStreamSynchronize(cuda_ctx->stream());\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0jIEKPZbtJGf",
   "metadata": {},
   "source": [
    "##### [`ggml_gallocr_reserve_n`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L673)\n",
    "\n",
    "> 感觉就是验证PP/TG中最差情况的可运行性，好多没用的赋值。。。。\n",
    "\n",
    "> 而这些复制的结构体中的成员变量是要和 `ggml_gallocr_alloc_graph` 相配合的\n",
    "\n",
    "```c++\n",
    "ggml_gallocr_reserve_n(sched->galloc, &sched->graph, sched->node_backend_ids, sched->leaf_backend_ids);\n",
    "----------\n",
    "bool ggml_gallocr_reserve_n(ggml_gallocr_t galloc, struct ggml_cgraph * graph, const int * node_buffer_ids, const int * leaf_buffer_ids) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lftemkBo9bDY",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct hash_node</summary>\n",
    "\n",
    "```c++\n",
    "struct hash_node {\n",
    "    int n_children;\n",
    "    int n_views;\n",
    "    int buffer_id;\n",
    "    size_t offset; // offset within the buffer\n",
    "    bool allocated;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "> galloc->hash_set 的初始化\n",
    "\n",
    "```c++\n",
    "size_t min_hash_size = graph->n_nodes + graph->n_leafs;\n",
    "// add 25% margin to avoid hash collisions\n",
    "min_hash_size += min_hash_size / 4;\n",
    "// initialize hash table\n",
    "// 这里第二次有可能就不执行了\n",
    "if (galloc->hash_set.size < min_hash_size) {\n",
    "    ggml_hash_set_free(&galloc->hash_set);\n",
    "    galloc->hash_set = ggml_hash_set_new(min_hash_size);\n",
    "    free(galloc->hash_values);\n",
    "    galloc->hash_values = malloc(sizeof(struct hash_node) * galloc->hash_set.size);\n",
    "}\n",
    "// reset allocators\n",
    "for (int i = 0; i < galloc->n_buffers; i++) ggml_dyn_tallocr_reset(galloc->buf_tallocs[i]);\n",
    "// allocate in hash table\n",
    "--> ggml_gallocr_alloc_graph_impl --> sched->galloc->hash_set, sched->galloc->hash_values, sched->galloc->buf_tallocs;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yw2vC_LvConF",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct tensor_alloc</summary>\n",
    "\n",
    "```c++\n",
    "struct tensor_alloc {\n",
    "    int buffer_id;\n",
    "    size_t offset;\n",
    "    size_t size_max; // 0 = pre-allocated, unused, or view\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>struct node_alloc</summary>\n",
    "\n",
    "```c++\n",
    "struct node_alloc {\n",
    "    struct tensor_alloc dst;\n",
    "    struct tensor_alloc src[GGML_MAX_SRC];\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "> galloc->node_allocs的设置，但感觉并没有用到啊！！！\n",
    "\n",
    "```c++\n",
    "// set the node_allocs from the hash table\n",
    "//这里第二次有可能就不执行了\n",
    "if (galloc->n_nodes < graph->n_nodes) {\n",
    "    free(galloc->node_allocs);\n",
    "    galloc->node_allocs = calloc(graph->n_nodes, sizeof(struct node_alloc));\n",
    "}\n",
    "galloc->n_nodes = graph->n_nodes;\n",
    "for (int i = 0; i < graph->n_nodes; i++) {\n",
    "    struct ggml_tensor * node = graph->nodes[i];\n",
    "    struct node_alloc * node_alloc = &galloc->node_allocs[i];\n",
    "    if (node->view_src || node->data) {\n",
    "        node_alloc->dst.buffer_id = -1;\n",
    "        node_alloc->dst.offset = SIZE_MAX;\n",
    "        node_alloc->dst.size_max = 0;\n",
    "    } else {\n",
    "        struct hash_node * hn = ggml_gallocr_hash_get(galloc, node);\n",
    "        node_alloc->dst.buffer_id = hn->buffer_id;\n",
    "        node_alloc->dst.offset    = hn->offset;\n",
    "        node_alloc->dst.size_max  = ggml_backend_buft_get_alloc_size(galloc->bufts[hn->buffer_id], node);\n",
    "    }\n",
    "    for (int j = 0; j < GGML_MAX_SRC; j++) {\n",
    "        struct ggml_tensor * src = node->src[j];\n",
    "        if (!src || src->view_src || src->data) {\n",
    "            node_alloc->src[j].buffer_id = -1;\n",
    "            node_alloc->src[j].offset = SIZE_MAX;\n",
    "            node_alloc->src[j].size_max = 0;\n",
    "        } else {\n",
    "            struct hash_node * hn = ggml_gallocr_hash_get(galloc, src);\n",
    "            node_alloc->src[j].buffer_id = hn->buffer_id;\n",
    "            node_alloc->src[j].offset   = hn->offset;\n",
    "            node_alloc->src[j].size_max = ggml_backend_buft_get_alloc_size(galloc->bufts[hn->buffer_id], src);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ACLCWoE1CvCt",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct leaf_alloc</summary>\n",
    "\n",
    "```c++\n",
    "struct leaf_alloc {\n",
    "    struct tensor_alloc leaf;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "> galloc->leaf_allocs的设置，但感觉并没有用到啊！！！\n",
    "\n",
    "```c++\n",
    "//这里第二次有可能就不执行了\n",
    "if (galloc->n_leafs < graph->n_leafs) {\n",
    "    free(galloc->leaf_allocs);\n",
    "    galloc->leaf_allocs = calloc(graph->n_leafs, sizeof(galloc->leaf_allocs[0]));\n",
    "}\n",
    "galloc->n_leafs = graph->n_leafs;\n",
    "for (int i = 0; i < graph->n_leafs; i++) {\n",
    "    struct ggml_tensor * leaf = graph->leafs[i];\n",
    "    struct hash_node * hn = ggml_gallocr_hash_get(galloc, leaf);\n",
    "    if (leaf->view_src || leaf->data) {\n",
    "        galloc->leaf_allocs[i].leaf.buffer_id = -1;\n",
    "        galloc->leaf_allocs[i].leaf.offset = SIZE_MAX;\n",
    "        galloc->leaf_allocs[i].leaf.size_max = 0;\n",
    "    } else {\n",
    "        galloc->leaf_allocs[i].leaf.buffer_id = hn->buffer_id;\n",
    "        galloc->leaf_allocs[i].leaf.offset = hn->offset;\n",
    "        galloc->leaf_allocs[i].leaf.size_max = ggml_backend_buft_get_alloc_size(galloc->bufts[hn->buffer_id], leaf);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oS-ZUSrfC4xv",
   "metadata": {},
   "source": [
    "> 这里每次重新跑会不一样\n",
    "\n",
    "```c++\n",
    "// reallocate buffers if needed\n",
    "for (int i = 0; i < galloc->n_buffers; i++) {\n",
    "    // if the buffer type is used multiple times, we reuse the same buffer\n",
    "    //...\n",
    "    size_t cur_size = galloc->buffers[i] ? ggml_backend_buffer_get_size(galloc->buffers[i]) : 0;\n",
    "    size_t new_size = ggml_dyn_tallocr_max_size(galloc->buf_tallocs[i]);\n",
    "    //这里第二次有可能就不执行了\n",
    "    // 第一次：cuda=189.31MB, cuda_host=0.532MB\n",
    "    // 第二次：cuda=0.172MB,  cuda_host=0.529MB\n",
    "    // even if there are no tensors allocated in this buffer, we still need to allocate it to initialize views\n",
    "    if (new_size > cur_size || galloc->buffers[i] == NULL) {\n",
    "        ggml_backend_buffer_free(galloc->buffers[i]);\n",
    "        // galloc->buffers[i] = ggml_backend_buft_alloc_buffer(galloc->bufts[i], new_size);\n",
    "        --> ggml_backend_buft_alloc_buffer --> galloc->buffers[i];\n",
    "            --> ggml_backend_cuda_buffer_type_alloc_buffer;\n",
    "            --> ggml_backend_cuda_host_buffer_type_alloc_buffer;\n",
    "        ggml_backend_buffer_set_usage(galloc->buffers[i], GGML_BACKEND_BUFFER_USAGE_COMPUTE);\n",
    "    }\n",
    "}\n",
    "return true;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YEldWDMpKTxe",
   "metadata": {},
   "source": [
    "###### [`ggml_gallocr_alloc_graph_impl`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L566)\n",
    "\n",
    "\n",
    "```c++\n",
    "// allocate in hash table\n",
    "ggml_gallocr_alloc_graph_impl(galloc, graph, node_buffer_ids, leaf_buffer_ids);\n",
    "----------\n",
    "static void ggml_gallocr_alloc_graph_impl(ggml_gallocr_t galloc, struct ggml_cgraph * graph, const int * node_buffer_ids, const int * leaf_buffer_ids) {}\n",
    "```\n",
    "\n",
    "```log\n",
    ">>> p *leaf_buffer_ids@graph->n_leafs\n",
    "$1094 = {[0] = 1 <repeats 11 times>, [11] = 0, [12] = 1, [13] = 1, [14] = 1, [15] = 1, [16] = 1, [17] = 0 <repeats 336 times>, [353] = 1, [354] = 0, [355] = 0, [356] = 0, [357] = 1, [358] = 0, [359] = 0, [360] = 1}\n",
    ">>> p *node_buffer_ids@graph->n_nodes\n",
    "$1095 = {[0] = 1, [1] = 1, [2] = 0, [3] = 0, [4] = 1, [5] = 0, [6] = 0, [7] = 1, [8] = 0, [9] = 0, [10] = 0, [11] = 1, [12] = 0, [13] = 1, [14] = 0, [15] = 0, [16] = 1, [17] = 0, [18] = 0, [19] = 0, [20] = 1, [21] = 0, [22] = 0, [23] = 0, [24] = 0, [25] = 1, [26] = 0, [27] = 1, [28] = 0, [29] = 1, [30] = 1, [31] = 0, [32] = 0, [33] = 1, [34] = 1, [35] = 1, [36] = 1, [37] = 1, [38] = 1, [39] = 0, [40] = 1, [41] = 0, [42] = 1, [43] = 0, [44] = 1, [45] = 0, [46] = 0, [47] = 0, [48] = 0, [49] = 0, [50] = 0, [51] = 1, [52] = 0, [53] = 0, [54] = 0, [55] = 0, [56] = 1, [57] = 0, [58] = 0, [59] = 1, [60] = 0, [61] = 0, [62] = 0, [63] = 1, [64] = 0, [65] = 0, [66] = 0, [67] = 1, [68] = 0, [69] = 1, [70] = 0 <repeats 1122 times>, [1192] = 1, [1193] = 0, [1194] = 0, [1195] = 1, [1196] = 0, [1197] = 0}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1WDXi_KrtB",
   "metadata": {},
   "source": [
    "```c++\n",
    "// clear hash tables\n",
    "ggml_hash_set_reset(&galloc->hash_set);\n",
    "    --> memset(hash_set->used, 0, sizeof(ggml_bitset_t) * ggml_bitset_size(hash_set->size));\n",
    "memset(galloc->hash_values, 0, sizeof(struct hash_node) * galloc->hash_set.size);\n",
    "// allocate leafs\n",
    "// these may be tensors that the application is not using in the graph, but may still want to allocate for other purposes\n",
    "for (int i = 0; i < graph->n_leafs; i++) {\n",
    "    struct ggml_tensor * leaf = graph->leafs[i];\n",
    "    --> ggml_gallocr_allocate_node(galloc, leaf, leaf_buffer_ids[i]);\n",
    "    // 没有 ggml_op_can_inplace 的情况\n",
    "}\n",
    "// count number of children and views\n",
    "// allocate other graph inputs and leafs first to avoid overwriting them\n",
    "for (int i = 0; i < graph->n_nodes; i++) {\n",
    "    struct ggml_tensor * node = graph->nodes[i];\n",
    "    // TODO: better way to add external dependencies\n",
    "    // GGML_OP_NONE does not appear normally in the graph nodes, but is used by ggml-backend to add dependencies to\n",
    "    // control when some tensors are allocated and freed. in this case, the dependencies are in `src`, but the node\n",
    "    // itself is never used and should not be considered a dependency\n",
    "    if (ggml_is_view(node) && node->op != GGML_OP_NONE)\n",
    "        ggml_gallocr_hash_get(galloc, node->view_src)->n_views += 1;\n",
    "    // if (node->flags & GGML_TENSOR_FLAG_INPUT) // 这里应该是不会走到的，因为既然是node了，怎么会是input呢\n",
    "    for (int j = 0; j < GGML_MAX_SRC; j++) {\n",
    "        // 这一步还挺关键的，对于buffer的回收 *key*\n",
    "        ggml_gallocr_hash_get(galloc, node->src[j])->n_children += 1;\n",
    "        // allocate explicit inputs\n",
    "        if (node->src[j]->flags & GGML_TENSOR_FLAG_INPUT) {\n",
    "            //第一次：i = [0-leaf_2,11-leaf_4,42-leaf_10,69-leaf_357]\n",
    "            //第二次：i = [0-leaf_2,5-leaf_4,8-leaf_4,23-leaf_10,40-leaf_4,42-leaf_10,44-leaf_357]\n",
    "            --> ggml_gallocr_allocate_node(galloc, node->src[j], node_buffer_ids[i]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "// allocate tensors\n",
    "for (int i = 0; i < graph->n_nodes; i++) {\n",
    "    struct ggml_tensor * node = graph->nodes[i];\n",
    "    int buffer_id = get_node_buffer_id(node_buffer_ids, i);\n",
    "    // allocate parents (only leafs need to be allocated at this point)\n",
    "    // 感觉没什么作用？应该就只是确保src tensor没有不alloc的\n",
    "    //// for (int j = 0; j < GGML_MAX_SRC; j++) ggml_gallocr_allocate_node(galloc, node->src[j], node_buffer_ids[i]); \n",
    "    // allocate node\n",
    "    --> ggml_gallocr_allocate_node(galloc, node, node_buffer_ids[i]);\n",
    "    // update parents\n",
    "    for (int j = 0; j < GGML_MAX_SRC; j++) {\n",
    "        struct ggml_tensor * parent = node->src[j];\n",
    "        struct hash_node * p_hn = ggml_gallocr_hash_get(galloc, parent);\n",
    "        p_hn->n_children -= 1;\n",
    "        if (p_hn->n_children == 0 && p_hn->n_views == 0) {\n",
    "            if (ggml_is_view(parent)) {\n",
    "                struct ggml_tensor * view_src = parent->view_src;\n",
    "                struct hash_node * view_src_hn = ggml_gallocr_hash_get(galloc, view_src);\n",
    "                view_src_hn->n_views -= 1;\n",
    "                if (view_src_hn->n_views == 0 && view_src_hn->n_children == 0 && view_src_hn->allocated) {\n",
    "                    --> ggml_gallocr_free_node(galloc, view_src);\n",
    "                }\n",
    "            }\n",
    "            else if (p_hn->allocated) {\n",
    "                --> ggml_gallocr_free_node(galloc, parent);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lKigIvKeNVR6",
   "metadata": {},
   "source": [
    "[`ggml_gallocr_allocate_node`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L478)\n",
    "\n",
    "```c++\n",
    "static void ggml_gallocr_allocate_node(ggml_gallocr_t galloc, struct ggml_tensor * node, int buffer_id) {\n",
    "    GGML_ASSERT(buffer_id >= 0);\n",
    "    struct hash_node * hn = ggml_gallocr_hash_get(galloc, node);\n",
    "    if (!ggml_gallocr_is_allocated(galloc, node) && !ggml_is_view(node)) {\n",
    "        --> return t->data != NULL || ggml_gallocr_hash_get(galloc, t)->allocated;\n",
    "        hn->allocated = true;\n",
    "        // try to reuse a parent's buffer (inplace)\n",
    "        if (ggml_op_can_inplace(node->op))\n",
    "        for (int i = 0; i < GGML_MAX_SRC; i++) {\n",
    "            struct ggml_tensor * parent = node->src[i];\n",
    "            // if the node's data is external, then we cannot re-use it\n",
    "            if (!ggml_gallocr_is_own(galloc, parent)) continue;\n",
    "            // outputs cannot be reused\n",
    "            if (parent->flags & GGML_TENSOR_FLAG_OUTPUT || \\\n",
    "               (parent->view_src != NULL && parent->view_src->flags & GGML_TENSOR_FLAG_OUTPUT)) continue;\n",
    "            if (!ggml_are_same_layout(node, parent)) continue;\n",
    "            struct hash_node * p_hn = ggml_gallocr_hash_get(galloc, parent);\n",
    "            if (p_hn->n_children == 1 && p_hn->n_views == 0) {\n",
    "                if (ggml_is_view(parent)) {\n",
    "                    struct ggml_tensor * view_src = parent->view_src;\n",
    "                    struct hash_node * view_src_hn = ggml_gallocr_hash_get(galloc, view_src);\n",
    "                    if (view_src_hn->n_views == 1 && view_src_hn->n_children == 0 && view_src->data == parent->data) {\n",
    "                        assert(view_src_hn->offset == p_hn->offset);\n",
    "                        hn->buffer_id = p_hn->buffer_id;\n",
    "                        hn->offset = p_hn->offset;\n",
    "                        p_hn->allocated = false; // avoid freeing the parent\n",
    "                        view_src_hn->allocated = false;\n",
    "                        return;\n",
    "                    }\n",
    "                } else {\n",
    "                    hn->buffer_id = p_hn->buffer_id;\n",
    "                    hn->offset = p_hn->offset;\n",
    "                    p_hn->allocated = false; // avoid freeing the parent\n",
    "                    return;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        // allocate tensor from the buffer\n",
    "        struct ggml_dyn_tallocr * alloc = galloc->buf_tallocs[buffer_id];\n",
    "        ggml_backend_buffer_type_t buft = galloc->bufts[buffer_id];\n",
    "        size_t size = ggml_backend_buft_get_alloc_size(buft, node);\n",
    "        --> ggml_dyn_tallocr_alloc --> size_t offset;\n",
    "        hn->buffer_id = buffer_id;\n",
    "        hn->offset = offset;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BzqW21nqNVdp",
   "metadata": {},
   "source": [
    "[`ggml_dyn_tallocr_alloc`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L153)\n",
    "\n",
    "```c++\n",
    "size_t offset = ggml_dyn_tallocr_alloc(alloc, size, node);\n",
    "----------\n",
    "static size_t ggml_dyn_tallocr_alloc(struct ggml_dyn_tallocr * alloc, size_t size, const struct ggml_tensor * tensor) {\n",
    "    size = aligned_offset(NULL, size, alloc->alignment);\n",
    "    // find the best fitting free block besides the last block\n",
    "    int best_fit_block = -1;\n",
    "    size_t best_fit_size = SIZE_MAX;\n",
    "    for (int i = 0; i < alloc->n_free_blocks - 1; i++) {\n",
    "        struct free_block * block = &alloc->free_blocks[i];\n",
    "        if (block->size >= size && block->size <= best_fit_size) {\n",
    "            best_fit_block = i;\n",
    "            best_fit_size = block->size;\n",
    "        }\n",
    "    }\n",
    "    if (best_fit_block == -1) {\n",
    "        // the last block is our last resort\n",
    "        // if (block->size >= size) {\n",
    "        best_fit_block = alloc->n_free_blocks - 1;\n",
    "    }\n",
    "    struct free_block * block = &alloc->free_blocks[best_fit_block];\n",
    "    size_t offset = block->offset;\n",
    "    block->offset = offset + size;\n",
    "    block->size -= size;\n",
    "    alloc->max_size = MAX(alloc->max_size, offset + size);\n",
    "    return offset;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mSlLVEbyQFNn",
   "metadata": {},
   "source": [
    "[`ggml_gallocr_free_node`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L545)\n",
    "\n",
    "```c++\n",
    "static void ggml_gallocr_free_node(ggml_gallocr_t galloc, struct ggml_tensor * node) {\n",
    "    // graph outputs are never freed\n",
    "    if (node->flags & GGML_TENSOR_FLAG_OUTPUT) return;\n",
    "    struct hash_node * hn = ggml_gallocr_hash_get(galloc, node);\n",
    "    size_t offset = hn->offset;\n",
    "    int buffer_id = hn->buffer_id;\n",
    "    struct ggml_dyn_tallocr * alloc = galloc->buf_tallocs[buffer_id];\n",
    "    ggml_backend_buffer_type_t buft = galloc->bufts[buffer_id];\n",
    "    size_t size = ggml_backend_buft_get_alloc_size(buft, node);\n",
    "    --> ggml_dyn_tallocr_free_tensor;\n",
    "    hn->allocated = false;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cmUXXd9TQUD4",
   "metadata": {},
   "source": [
    "[`ggml_dyn_tallocr_free_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L238)\n",
    "\n",
    "```c++\n",
    "ggml_dyn_tallocr_free_tensor(alloc, offset, size, node);\n",
    "----------\n",
    "// this is a very naive implementation, but for our case the number of free blocks should be very small\n",
    "static void ggml_dyn_tallocr_free_tensor(struct ggml_dyn_tallocr * alloc, size_t offset, size_t size, const struct ggml_tensor * tensor) {\n",
    "    size = aligned_offset(NULL, size, alloc->alignment);\n",
    "\n",
    "    // see if we can merge with an existing block\n",
    "    for (int i = 0; i < alloc->n_free_blocks; i++) {\n",
    "        struct free_block * block = &alloc->free_blocks[i];\n",
    "        // check if ptr is at the end of the block\n",
    "        if (block->offset + block->size == offset) {\n",
    "            block->size += size;\n",
    "            // check if we can merge with the next block\n",
    "            if (i < alloc->n_free_blocks - 1 && block->offset + block->size == alloc->free_blocks[i+1].offset) {\n",
    "                block->size += alloc->free_blocks[i+1].size;\n",
    "                alloc->n_free_blocks--;\n",
    "                for (int j = i+1; j < alloc->n_free_blocks; j++) {\n",
    "                    alloc->free_blocks[j] = alloc->free_blocks[j+1];\n",
    "                }\n",
    "            }\n",
    "            return;\n",
    "        }\n",
    "        // check if ptr is at the beginning of the block\n",
    "        if (offset + size == block->offset) {\n",
    "            block->offset = offset;\n",
    "            block->size += size;\n",
    "            // check if we can merge with the previous block\n",
    "            if (i > 0 && alloc->free_blocks[i-1].offset + alloc->free_blocks[i-1].size == block->offset) {\n",
    "                alloc->free_blocks[i-1].size += block->size;\n",
    "                alloc->n_free_blocks--;\n",
    "                for (int j = i; j < alloc->n_free_blocks; j++) {\n",
    "                    alloc->free_blocks[j] = alloc->free_blocks[j+1];\n",
    "                }\n",
    "            }\n",
    "            return;\n",
    "        }\n",
    "    }\n",
    "    // otherwise, add a new block\n",
    "    // insert the new block in the correct position to keep the array sorted by address (to make merging blocks faster)\n",
    "    int insert_pos = 0;\n",
    "    while (insert_pos < alloc->n_free_blocks && alloc->free_blocks[insert_pos].offset < offset) {\n",
    "        insert_pos++;\n",
    "    }\n",
    "    // shift all blocks from insert_pos onward to make room for the new block\n",
    "    for (int i = alloc->n_free_blocks; i > insert_pos; i--) {\n",
    "        alloc->free_blocks[i] = alloc->free_blocks[i-1];\n",
    "    }\n",
    "    // insert the new block\n",
    "    alloc->free_blocks[insert_pos].offset = offset;\n",
    "    alloc->free_blocks[insert_pos].size = size;\n",
    "    alloc->n_free_blocks++;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7034405-372b-4f82-803f-87dca0e81d41",
   "metadata": {},
   "source": [
    "## *[`llama_context::decode`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L880)\n",
    "\n",
    "<details>\n",
    "<summary>struct llama_batch</summary>\n",
    "\n",
    "```c++\n",
    "    // Input data for llama_decode\n",
    "    // A llama_batch object can contain input about one or many sequences\n",
    "    // The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens\n",
    "    //\n",
    "    // - token  : the token ids of the input (used when embd is NULL)\n",
    "    // - embd   : token embeddings (i.e. float vector of size n_embd) (used when token is NULL)\n",
    "    // - pos    : the positions of the respective token in the sequence\n",
    "    //            (if set to NULL, the token position will be tracked automatically by llama_decode)\n",
    "    // - seq_id : the sequence to which the respective token belongs\n",
    "    //            (if set to NULL, the sequence ID will be assumed to be 0)\n",
    "    // - logits : if zero, the logits (and/or the embeddings) for the respective token will not be output\n",
    "    //            (if set to NULL, only the logits for last token will be returned)\n",
    "    //\n",
    "    typedef struct llama_batch {\n",
    "        int32_t n_tokens;\n",
    "\n",
    "        llama_token  *  token;\n",
    "        float        *  embd;\n",
    "        llama_pos    *  pos;\n",
    "        int32_t      *  n_seq_id; // TODO: remove, should belong to only 1 sequence\n",
    "        llama_seq_id ** seq_id;   // TODO: become llama_seq_id * seq_id;\n",
    "        int8_t       *  logits;   // TODO: rename this to \"output\"\n",
    "    } llama_batch;\n",
    "\n",
    "\n",
    "struct llama_batch llama_batch_get_one(\n",
    "             llama_token * tokens,\n",
    "                 int32_t   n_tokens) {\n",
    "    return {\n",
    "        /*n_tokens       =*/ n_tokens,\n",
    "        /*tokens         =*/ tokens,\n",
    "        /*embd           =*/ nullptr,\n",
    "        /*pos            =*/ nullptr,\n",
    "        /*n_seq_id       =*/ nullptr,\n",
    "        /*seq_id         =*/ nullptr,\n",
    "        /*logits         =*/ nullptr,\n",
    "    };\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "llama_decode(lctx, llama_batch_get_one(tmp.data(), std::min(tmp.size(), (size_t) params.n_batch)));\n",
    "// or the simpler one\n",
    "llama_token token = llama_vocab_get_add_bos(vocab) ? llama_vocab_bos(vocab) : std::rand() % n_vocab;\n",
    "int res = llama_decode(ctx, llama_batch_get_one(&token, 1));\n",
    "----------\n",
    "int32_t llama_decode(\n",
    "        llama_context * ctx,\n",
    "          llama_batch   batch) {\n",
    "    --> const int ret = ctx->decode(batch);\n",
    "    return ret;\n",
    "}\n",
    "\n",
    "int llama_context::decode(llama_batch & batch_inp) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nG2IP_LzbYQS",
   "metadata": {},
   "source": [
    "```c++\n",
    "--> llama_batch_allocr::init;\n",
    "const llama_batch & batch = batch_allocr->get_batch();\n",
    "const uint32_t n_tokens_all  = balloc->get_n_tokens();\n",
    "const uint32_t n_outputs_all = balloc->get_n_outputs();\n",
    "n_queued_tokens += n_tokens_all;\n",
    "bool did_optimize = false;\n",
    "// handle any pending defrags/shifts\n",
    "--> llama_context::kv_self_update;\n",
    "--> llama_kv_cache_unified::init_batch --> llama_memory_context_ptr mctx;\n",
    "// reserve output buffer\n",
    "// 第二次执行可能会被跳过内存alloc的部分\n",
    "--> output_reserve(n_outputs_all) --> llama_context:: float * logits, ggml_backend_buffer_ptr buf_output;;\n",
    "int64_t n_outputs_prev = 0;\n",
    "do {\n",
    "    const auto & ubatch = mctx->get_ubatch();\n",
    "    // count the outputs in this u_batch\n",
    "    int32_t n_outputs_new = 0;\n",
    "    if (n_outputs_all == n_tokens_all) n_outputs_new = ubatch.n_tokens;\n",
    "    else for (uint32_t i = 0; i < ubatch.n_tokens; i++) n_outputs_new += (int32_t) (ubatch.output[i] != 0);\n",
    "    // needs to happen before the graph is built\n",
    "    n_outputs = n_outputs_new;\n",
    "    ggml_backend_sched_reset(sched.get());\n",
    "    --> ggml_backend_sched_set_eval_callback(sched.get(), cparams.cb_eval, cparams.cb_eval_user_data);\n",
    "        sched->callback_eval = callback;\n",
    "        sched->callback_eval_user_data = user_data;\n",
    "    --> llama_context::process_ubatch --> const auto res;\n",
    "    // plot the computation graph in dot format (for debugging purposes)\n",
    "    //if (n_past%100 == 0) ggml_graph_dump_dot(gf, NULL, \"llama.dot\");\n",
    "    auto * t_logits = res->get_logits();\n",
    "    // extract logits\n",
    "    ggml_backend_t backend_res = ggml_backend_sched_get_tensor_backend(sched.get(), t_logits);\n",
    "    float * logits_out = logits + n_outputs_prev*n_vocab;\n",
    "    --> ggml_backend_tensor_get_async --> logits_out\n",
    "    n_outputs_prev += n_outputs;\n",
    "} while (mctx->next());\n",
    "// set to total number of outputs in the batch, for use in llama_get_logits_ith\n",
    "n_outputs = n_outputs_all;\n",
    "// set output mappings\n",
    "// if (n_outputs > 0)\n",
    "auto & out_ids = balloc->get_out_ids();\n",
    "//llama_context:: std::vector<int32_t> output_ids; // map batch token positions to ids of the logits and embd buffers\n",
    "for (int64_t i = 0; i < n_outputs_all; ++i) output_ids[out_ids[i]] = i;\n",
    "// wait for the computation to finish (automatically done when obtaining the model output)\n",
    "//synchronize();\n",
    "// Reset state for the next token before backend sync, to allow the CPU activities in the reset to overlap with device computation.\n",
    "ggml_backend_sched_reset(sched.get());\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rcWYjLxghV9z",
   "metadata": {},
   "source": [
    "### [`llama_batch_allocr::init`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L25)\n",
    "\n",
    "<details>\n",
    "<summary>class llama_batch_allocr</summary>\n",
    "\n",
    "```c++\n",
    "typedef int32_t llama_pos;\n",
    "typedef int32_t llama_seq_id;\n",
    "\n",
    "class llama_batch_allocr {\n",
    "    llama_batch batch;\n",
    "\n",
    "    uint32_t n_outputs;\n",
    "\n",
    "    std::array<llama_seq_id, 1> seq_id_0 = { 0 }; // default sequence id\n",
    "\n",
    "    std::vector<llama_pos>      pos;\n",
    "    std::vector<int32_t>        n_seq_id;\n",
    "    std::vector<llama_seq_id *> seq_id;\n",
    "    std::vector<int8_t>         output;\n",
    "\n",
    "    std::vector<std::set<llama_pos>> seq_pos; // seq_pos[s]: the set of positions in sequence s\n",
    "    std::vector<std::vector<bool>>   seq_cpl; // seq_cpl[s0][s1]: if sequence s0 is coupled to sequence s1\n",
    "\n",
    "    // batch indices of the output\n",
    "    std::vector<int32_t> out_ids;\n",
    "\n",
    "    // used[i] indicates if token i has already been used in a previous ubatch\n",
    "    std::vector<bool> used;\n",
    "\n",
    "    // llama_ubatch points to this data:\n",
    "    struct ubatch {\n",
    "        std::vector<llama_token>    token;\n",
    "        std::vector<float>          embd;\n",
    "        std::vector<llama_pos>      pos;\n",
    "        std::vector<int32_t>        n_seq_id;\n",
    "        std::vector<llama_seq_id *> seq_id;\n",
    "        std::vector<llama_seq_id>   seq_id_unq;\n",
    "        std::vector<int32_t>        seq_idx;\n",
    "        std::vector<int8_t>         output;\n",
    "    };\n",
    "\n",
    "    // current splitting state:\n",
    "    std::vector<ubatch> ubatches;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "balloc->init(batch_inp, vocab, memory.get(), n_embd, output_all);\n",
    "----------\n",
    "bool llama_batch_allocr::init(\n",
    "        const llama_batch & batch_inp,\n",
    "        const llama_vocab & vocab,\n",
    "        const llama_memory_i * memory,\n",
    "        uint32_t n_embd,\n",
    "        bool output_all) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wVbufKl8hw-D",
   "metadata": {},
   "source": [
    "```c++\n",
    "--> clear();\n",
    "    n_outputs = 0;\n",
    "    batch = {};\n",
    "    pos       .clear();\n",
    "    n_seq_id  .clear();\n",
    "    seq_id    .clear();\n",
    "    seq_id_unq.clear();\n",
    "    output    .clear();\n",
    "    for (auto & cur : seq_pos) cur.clear();\n",
    "    for (auto & cur : seq_cpl) std::fill(cur.begin(), cur.end(), false);\n",
    "    seq_set.clear();\n",
    "    seq_set_map.clear();\n",
    "    std::fill(seq_idx.begin(), seq_idx.end(), -1);\n",
    "batch = batch_inp;\n",
    "// validate input batch\n",
    "// auto-generate missing fields\n",
    "if (!batch.n_seq_id) {\n",
    "    n_seq_id.resize(batch.n_tokens);\n",
    "    for (int32_t i = 0; i < batch.n_tokens; i++) n_seq_id[i] = seq_id_0.size();\n",
    "    batch.n_seq_id = n_seq_id.data();\n",
    "}\n",
    "if (!batch.seq_id) {\n",
    "    seq_id.resize(batch.n_tokens + 1);\n",
    "    seq_id[batch.n_tokens] = NULL;\n",
    "    for (int32_t i = 0; i < batch.n_tokens; i++) seq_id[i] = seq_id_0.data();\n",
    "    batch.seq_id = seq_id.data();\n",
    "}\n",
    "if (!batch.pos) {\n",
    "    pos.resize(batch.n_tokens);\n",
    "    // initialize the starting position for each sequence based on the positions in the memory\n",
    "    llama_pos p0[LLAMA_MAX_SEQ];\n",
    "    for (int32_t s = 0; s < LLAMA_MAX_SEQ; ++s) p0[s] = memory? memory->seq_pos_max(s) + 1 : 0;\n",
    "    for (int32_t i = 0; i < batch.n_tokens; i++) {\n",
    "        const llama_seq_id seq_id = batch.seq_id[i][0];\n",
    "        pos[i] = p0[seq_id];\n",
    "        for (int32_t s = 0; s < batch.n_seq_id[i]; ++s) p0[batch.seq_id[i][s]] = pos[i] + 1;\n",
    "    }\n",
    "    batch.pos = pos.data();\n",
    "}\n",
    "if (!batch.logits) {\n",
    "    // return the output only for the last token\n",
    "    output.resize(batch.n_tokens, false);\n",
    "    output[output.size() - 1] = true;\n",
    "    batch.logits = output.data();\n",
    "}\n",
    "// compute stats\n",
    "for (int32_t i = 0; i < batch.n_tokens; ++i) n_outputs += batch.logits[i] != 0;\n",
    "// determine coupled sequences these are pairs of sequences that have at least one token in the input batch that is assigned to both of them\n",
    "for (int32_t i = 0; i < batch.n_tokens; ++i)\n",
    "    for (int32_t s = 0; s < batch.n_seq_id[i]; ++s)\n",
    "        seq_pos[batch.seq_id[i][s]].insert(batch.pos[i]);\n",
    "        // mark that sequence s1 is coupled to s0\n",
    "        if (s > 0) seq_cpl[batch.seq_id[i][s]][batch.seq_id[i][0]] = true;\n",
    "// precompute the sequence sets for each token and determine the unique sequence ids that participate in the batch\n",
    "seq_set_t seq_set_unq;\n",
    "for (int32_t i = 0; i < batch.n_tokens; ++i)\n",
    "    seq_set_t cur;\n",
    "    for (int32_t s = 0; s < batch.n_seq_id[i]; ++s)\n",
    "        const llama_seq_id seq_id = batch.seq_id[i][s];\n",
    "        cur        .set(seq_id);\n",
    "        seq_set_unq.set(seq_id);\n",
    "    seq_set.push_back(cur);\n",
    "    seq_set_map[cur].push_back(i);\n",
    "for (int32_t s = 0; s < LLAMA_MAX_SEQ; ++s)\n",
    "    if (seq_set_unq.test(s))\n",
    "        seq_idx[s] = seq_id_unq.size();\n",
    "        seq_id_unq.push_back(s);\n",
    "// consistency checks\n",
    "// disallow partial sequence sub-sets:\n",
    "--> split_reset();\n",
    "    out_ids.clear();\n",
    "    used.clear();\n",
    "    used.resize(get_n_tokens(), false);\n",
    "    ubatches.clear();\n",
    "return true;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jIqtIvyzqb8m",
   "metadata": {},
   "source": [
    "### [`llama_context::kv_self_update`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L437)\n",
    "\n",
    "\n",
    "```c++\n",
    "kv_self_update(false);\n",
    "----------\n",
    "// deprecated\n",
    "bool llama_context::kv_self_update(bool optimize) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8kJxA9OQhpxV",
   "metadata": {},
   "source": [
    "```c++\n",
    "// TODO: remove in the future\n",
    "optimize |= memory_force_optimize; //false\n",
    "memory_force_optimize = false;\n",
    "\n",
    "--> memory --> llama_kv_cache_unified::init_update --> const auto mctx;\n",
    "switch (mctx->get_status())\n",
    "case LLAMA_MEMORY_STATUS_NO_UPDATE: return false; // no updates need to be performed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEJTh4tsv5YU",
   "metadata": {},
   "source": [
    "[`lama_kv_cache_unified::init_update`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L340)\n",
    "\n",
    "```c++\n",
    "const auto mstate = memory->init_update(this, optimize);\n",
    "----------\n",
    "llama_memory_state_ptr llama_kv_cache_unified::init_update(llama_context * lctx, bool optimize) {\n",
    "    bool do_shift = get_has_shift();\n",
    "    defrag_info dinfo;\n",
    "    --> return std::make_unique<llama_kv_cache_unified_state>(this, lctx, do_shift, std::move(dinfo));\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ECEOUK5oxRFn",
   "metadata": {},
   "source": [
    "[`llama_kv_cache_unified_state::llama_kv_cache_unified_state`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L1737)\n",
    "\n",
    "```c++\n",
    "llama_kv_cache_unified_state::llama_kv_cache_unified_state(\n",
    "        llama_kv_cache_unified * kv,\n",
    "        llama_context * lctx,\n",
    "        bool do_shift,\n",
    "        defrag_info dinfo) : status(LLAMA_MEMORY_STATUS_SUCCESS), kv(kv), lctx(lctx), do_shift(do_shift), dinfo(std::move(dinfo)) {\n",
    "    if (!do_shift && this->dinfo.empty()) {\n",
    "        status = LLAMA_MEMORY_STATUS_NO_UPDATE;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fp94VaBRs6lv",
   "metadata": {},
   "source": [
    "### [`llama_kv_cache_unified::init_batch`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L310)\n",
    "\n",
    "```c++\n",
    "mctx = memory->init_batch(*balloc, cparams.n_ubatch, output_all);\n",
    "----------\n",
    "llama_memory_context_ptr llama_kv_cache_unified::init_batch(\n",
    "            llama_batch_allocr & balloc,\n",
    "            uint32_t n_ubatch,\n",
    "            bool embd_all) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q4PW8HnO6lsL",
   "metadata": {},
   "source": [
    "```c++\n",
    "balloc.split_reset();\n",
    "std::vector<llama_ubatch> ubatches;\n",
    "while (true) {\n",
    "    -->llama_batch_allocr::split_simple --> llama_ubatch ubatch;\n",
    "    if (ubatch.n_tokens == 0) break;\n",
    "    std::vector<llama_ubatch> ubatches.push_back(std::move(ubatch)); // NOLINT\n",
    "}\n",
    "--> llama_kv_cache_unified::prepare --> llama_kv_cache_unified::ubatch_heads heads;\n",
    "return std::make_unique<llama_kv_cache_unified_context>(\n",
    "        this, std::move(heads), std::move(ubatches));\n",
    "--> llama_kv_cache_unified_context::llama_kv_cache_unified_context(\n",
    "    llama_kv_cache_unified * kv,\n",
    "    llama_kv_cache_unified::ubatch_heads heads,\n",
    "    std::vector<llama_ubatch> ubatches) : status(LLAMA_MEMORY_STATUS_SUCCESS), kv(kv), heads(std::move(heads)), ubatches(std::move(ubatches)) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dZBqXhpJ8Cep",
   "metadata": {},
   "source": [
    "#### [`llama_sbatch::split_simple`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L149)\n",
    "\n",
    "```c++\n",
    "ubatches.push_back(sbatch.split_simple(n_ubatch));\n",
    "----------\n",
    "llama_ubatch llama_sbatch::split_simple(size_t n_ubatch) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u8jAu9qYEA1p",
   "metadata": {},
   "source": [
    "```c++\n",
    "// find the first unused token\n",
    "uint32_t cur_idx = 0;\n",
    "while (cur_idx < used.size() && used[cur_idx]) ++cur_idx;\n",
    "// we are done\n",
    "if (cur_idx >= used.size()) return {};\n",
    "std::vector<int32_t> idxs;\n",
    "while (true)\n",
    "    idxs.push_back(cur_idx);\n",
    "    used[cur_idx] = true;\n",
    "    ++cur_idx;\n",
    "    if (cur_idx >= used.size()) break;\n",
    "    if (idxs.size() >= n_ubatch) break;\n",
    "--> return ubatch_add(idxs, idxs.size(), false);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213285c2",
   "metadata": {},
   "source": [
    "[`llama_batch_allocr::ubatch_add`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L605)\n",
    "\n",
    "```c++\n",
    "llama_ubatch llama_batch_allocr::ubatch_add(const std::vector<int32_t> & idxs, uint32_t n_seqs, bool equal_seqs) {\n",
    "    const uint32_t n_tokens = idxs.size();\n",
    "    ubatches.emplace_back();\n",
    "    auto & ubatch = ubatches.back();\n",
    "    const int32_t n_pos_cur = 1;\n",
    "    const int64_t n_pos_all  = (int64_t) n_tokens*n_pos_cur;\n",
    "    ubatch.token     .resize(n_tokens);\n",
    "    ubatch.embd      .resize(n_embd_all);\n",
    "    ubatch.pos       .resize(n_pos_all);\n",
    "    ubatch.n_seq_id  .resize(n_tokens);\n",
    "    ubatch.seq_id    .resize(n_tokens);\n",
    "    ubatch.seq_id_unq.resize(0);\n",
    "    ubatch.seq_idx   .resize(LLAMA_MAX_SEQ, -1);\n",
    "    ubatch.output    .resize(n_tokens);\n",
    "    seq_set_t seq_set_unq;\n",
    "    for (size_t i = 0; i < idxs.size(); ++i) {\n",
    "        if (batch.token) ubatch.token[i] = batch.token[idxs[i]];\n",
    "        for (int j = 0; j < n_pos_cur; ++j) ubatch.pos[j*n_tokens + i] = batch.pos[j*batch.n_tokens + idxs[i]];\n",
    "        ubatch.n_seq_id[i] = batch.n_seq_id[idxs[i]];\n",
    "        ubatch.seq_id[i]   = batch.seq_id[idxs[i]];\n",
    "        ubatch.output[i]   = batch.logits[idxs[i]];\n",
    "        for (int s = 0; s < ubatch.n_seq_id[i]; ++s) seq_set_unq.set(ubatch.seq_id[i][s]);\n",
    "        if (ubatch.output[i]) out_ids.push_back(idxs[i]);\n",
    "    }\n",
    "    for (int32_t s = 0; s < LLAMA_MAX_SEQ; ++s) {\n",
    "        if (seq_set_unq.test(s)) {\n",
    "            ubatch.seq_idx[s] = ubatch.seq_id_unq.size();\n",
    "            ubatch.seq_id_unq.push_back(s);\n",
    "        }\n",
    "    }\n",
    "    llama_ubatch res {\n",
    "        /*.equal_seqs   =*/ equal_seqs,\n",
    "        /*.n_tokens     =*/ n_tokens,\n",
    "        /*.n_seq_tokens =*/ n_tokens/n_seqs,\n",
    "        /*.n_seqs       =*/ n_seqs,\n",
    "        /*.n_seqs_unq   =*/ (uint32_t) ubatch.seq_id_unq.size(),\n",
    "        /*.token        =*/ batch.token ? ubatch.token.data() : nullptr,\n",
    "        /*.embd         =*/ batch.embd ? ubatch.embd.data() : nullptr,\n",
    "        /*.pos          =*/ ubatch.pos.data(),\n",
    "        /*.n_seq_id     =*/ ubatch.n_seq_id.data(),\n",
    "        /*.seq_id       =*/ ubatch.seq_id.data(),\n",
    "        /*.seq_id_unq   =*/ ubatch.seq_id_unq.data(),\n",
    "        /*.seq_idx      =*/ ubatch.seq_idx.data(),\n",
    "        /*.output       =*/ ubatch.output.data(),\n",
    "    };\n",
    "    return res;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PJtrQGoo8V-S",
   "metadata": {},
   "source": [
    "#### [`llama_kv_cache_unified::prepare`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L373)\n",
    "\n",
    "```c++\n",
    "auto heads = prepare(ubatches);\n",
    "----------\n",
    "llama_kv_cache_unified::ubatch_heads llama_kv_cache_unified::prepare(const std::vector<llama_ubatch> & ubatches) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uL0224yoMgHW",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>llama_kv_cache_unified::ubatch_heads</summary>\n",
    "\n",
    "```c++\n",
    "using llama_kv_cache_unified::ubatch_heads = std::vector<uint32_t>;\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>struct state</summary>\n",
    "\n",
    "```c++\n",
    "struct state {\n",
    "    uint32_t head_old; // old position of the head, before placing the ubatch\n",
    "    uint32_t head_new; // new position of the head, after placing the ubatch\n",
    "\n",
    "    llama_kv_cells_unified cells; // copy of the old cells, before placing the ubatch\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "llama_kv_cache_unified::ubatch_heads res;\n",
    "\n",
    "// remember the old state of the cells so we can restore it in the end\n",
    "std::vector<state> states;\n",
    "//for (const auto & ubatch : ubatches)\n",
    "// only find a suitable slot for the ubatch. don't modify the cells yet\n",
    "--> llama_kv_cache_unified::find_slot ---> const int32_t head_new;\n",
    "// remeber the position that we found\n",
    "res.push_back(head_new);\n",
    "// store the old state of the cells in the recovery stack\n",
    "--> states.push_back({head, (uint32_t) head_new, cells.cp(head_new, ubatch.n_tokens)});\n",
    "// 意思就是打上标记，此地被占用了，for循环的下一次find_slot不会找上这一块\n",
    "// now emplace the ubatch\n",
    "--> llama_kv_cache_unified::apply_ubatch;\n",
    "// iterate backwards and restore the cells to their original state\n",
    "for (auto it = states.rbegin(); it != states.rend(); ++it)\n",
    "    --> cells.set(it->head_new, it->cells);\n",
    "    head = it->head_old;\n",
    "return res;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iZH5uaEhRdpd",
   "metadata": {},
   "source": [
    "##### [`llama_kv_cache_unified::find_slot`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L510)\n",
    "\n",
    "```c++\n",
    "const int32_t head_new = find_slot(ubatch);\n",
    "----------\n",
    "int32_t llama_kv_cache_unified::find_slot(const llama_ubatch & ubatch) const {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GwxYfFD7yHOZ",
   "metadata": {},
   "source": [
    "```c++\n",
    "const uint32_t n_tokens = ubatch.n_tokens;\n",
    "uint32_t head_cur = this->head;\n",
    "// if we have enough unused cells before the current head ->\n",
    "// better to start searching from the beginning of the cache, hoping to fill it\n",
    "if (head_cur > cells.get_used() + 2*n_tokens) head_cur = 0;\n",
    "while (true) {\n",
    "    if (head_cur + n_tokens > cells.size())\n",
    "        head_cur = 0;\n",
    "        continue;\n",
    "    bool found = true;\n",
    "    for (uint32_t i = 0; i < n_tokens; i++) {\n",
    "        // can we use this cell? either:\n",
    "        //- the cell is empty\n",
    "        //- the cell is occupied only by one sequence: <-why?:/\n",
    "        // - (disabled) mask causally, if the sequence is the same as the one we are inserting\n",
    "        // - mask SWA, using current max pos for that sequence in the cache, always insert in the cell with minimum pos\n",
    "        bool can_use = cells.is_empty(head_cur + i);\n",
    "        if (!can_use && cells.seq_count(head_cur + i) == 1)\n",
    "            const llama_pos pos_cell = cells.pos_get(head_cur + i);\n",
    "            const llama_seq_id seq_id_cell = cells.seq_get(head_cur + i);\n",
    "            // (disabled) causal mask\n",
    "            // note: it's better to purge any \"future\" tokens beforehand\n",
    "            //if (cells.seq_has(head_cur + i, seq_id)) {\n",
    "            //    can_use = pos_cell >= pos;\n",
    "            //}\n",
    "            // SWA mask\n",
    "            if (!can_use && is_masked_swa(pos_cell, cells.seq_pos_max(seq_id_cell) + 1)) can_use = true;\n",
    "        if (!can_use)\n",
    "            found = false;\n",
    "            head_cur += i + 1;\n",
    "            break;\n",
    "    }\n",
    "    if (found) break;\n",
    "}\n",
    "return head_cur;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IOFSsjIuRdv3",
   "metadata": {},
   "source": [
    "##### [`llama_kv_cache_unified::apply_ubatch`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L646)\n",
    "\n",
    "```c++\n",
    "apply_ubatch(head_new, ubatch);\n",
    "----------\n",
    "void llama_kv_cache_unified::apply_ubatch(uint32_t head_cur, const llama_ubatch & ubatch) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uWlt7VDg114O",
   "metadata": {},
   "source": [
    "```c++\n",
    "// keep track of the max sequence position that we would overwrite with this ubatch for non-SWA cache, this would be always empty\n",
    "llama_seq_id seq_pos_max_rm[LLAMA_MAX_SEQ];\n",
    "for (int s = 0; s < LLAMA_MAX_SEQ; ++s) seq_pos_max_rm[s] = -1;\n",
    "for (uint32_t i = 0; i < ubatch.n_tokens; ++i)\n",
    "    if (!cells.is_empty(head_cur + i)); // 一般不会执行到\n",
    "    cells.pos_set(i=head_cur + i, p=ubatch.pos[i]);\n",
    "    --> pos[i] = p;\n",
    "        used.insert(i);\n",
    "    for (int32_t s = 0; s < ubatch.n_seq_id[i]; s++)\n",
    "        cells.seq_add(i=head_cur + i, seq_id=ubatch.seq_id[i][s]);\n",
    "        --> seq[i].set(seq_id);\n",
    "            seq_pos[seq_id].insert(pos[i]);\n",
    "// note: we want to preserve the invariant that all positions between [pos_min, pos_max] for each sequence\n",
    "//       will be present in the cache. so we have to purge any position which is less than those we would overwrite\n",
    "//       ref: https://github.com/ggml-org/llama.cpp/pull/13746#issuecomment-2916057092\n",
    "for (int s = 0; s < LLAMA_MAX_SEQ; ++s)\n",
    "    if (seq_pos_max_rm[s] == -1) continue;\n",
    "    if (cells.seq_pos_min(s) <= seq_pos_max_rm[s]); // 一般不会执行到\n",
    "\n",
    "// move the head at the end of the slot\n",
    "head = head_cur + ubatch.n_tokens;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aWinbl5DJviV",
   "metadata": {},
   "source": [
    "### **[`llama_context::process_ubatch`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L681)\n",
    "\n",
    "```c++\n",
    "const auto res = process_ubatch(ubatch, LLM_GRAPH_TYPE_DECODER, mctx.get(), status);\n",
    "----------\n",
    "llm_graph_result_ptr llama_context::process_ubatch(const llama_ubatch & ubatch, llm_graph_type gtype, llama_memory_context_i * mctx, ggml_status & ret) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ncESwyLERiq",
   "metadata": {},
   "source": [
    "```c++\n",
    "--> llama_kv_cache_unified_context::apply;\n",
    "--> graph_init --> ggml_cgraph * gf;\n",
    "// auto res = graph_build(ctx_compute.get(), gf, ubatch, gtype, mctx);\n",
    "--> llama_context::graph_build --> llama_model::build_graph --> llm_build_llama::llm_build_llama --> std::unique_ptr<llm_graph_result> res;\n",
    "// LLAMA_LOG_INFO(\"graph build time: %.3f ms (%d nodes, %d leafs)\\n\", (ggml_time_us() - t_start_us)/1000.0, gf->n_nodes, gf->n_leafs);\n",
    "--> ggml_backend_sched_alloc_graph;\n",
    "--> ggml_backend_cpu_buffer_set_tensor;/llama_kv_cache_unified::set_input_kq_mask;\n",
    "--> llama_context::graph_compute --> ggml_backend_sched_graph_compute_async --> ggml_status ggml_backend_sched_compute_splits --> ggml_backend_graph_compute_async --> ggml_backend_cpu_graph_compute;\n",
    "ret = GGML_STATUS_SUCCESS;\n",
    "return res;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f80ad1-f715-4c13-a66f-897b94c2b019",
   "metadata": {},
   "source": [
    "#### [`llama_kv_cache_unified_context::apply`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L1753)\n",
    "\n",
    "> 不同于llama_kv_cache_unified::prepare, 这回是真的起作用了\n",
    "\n",
    "```c++\n",
    "if (mctx && !mctx->apply()) {}\n",
    "----------\n",
    "llm_graph_result_ptr llama_context::process_ubatch(const llama_ubatch & ubatch, llm_graph_type gtype, llama_memory_context_i * mctx, ggml_status & ret) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e863cc-77df-46de-927d-9c416dfac21f",
   "metadata": {},
   "source": [
    "```c++\n",
    "// no ubatches -> this is a KV cache update\n",
    "if (ubatches.empty()) {\n",
    "    kv->update(lctx, do_shift, dinfo);\n",
    "    return true;\n",
    "}\n",
    "--> kv->apply_ubatch(heads[i_next], ubatches[i_next]);\n",
    "n_kv = kv->get_n_kv(); // 更新n_kv的值，非常关键！ *key*\n",
    "head = heads[i_next];\n",
    "return true;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcsgUWocJbi2",
   "metadata": {},
   "source": [
    "#### [`ggml_gallocr_init_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L787)\n",
    "\n",
    "[`ggml_backend_sched_alloc_graph`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1565)\n",
    "\n",
    "```c++\n",
    "ggml_backend_sched_alloc_graph(sched.get(), gf);\n",
    "----------\n",
    "bool ggml_backend_sched_alloc_graph(ggml_backend_sched_t sched, struct ggml_cgraph * graph) {\n",
    "    --> ggml_backend_sched_split_graph(sched, graph);\n",
    "    --> ggml_backend_sched_alloc_splits(sched);\n",
    "    sched->is_alloc = true;\n",
    "    return true;\n",
    "}\n",
    "```\n",
    "\n",
    "[`ggml_backend_sched_alloc_splits`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1321)\n",
    "\n",
    "```c++\n",
    "static bool ggml_backend_sched_alloc_splits(ggml_backend_sched_t sched) {\n",
    "    bool backend_ids_changed = false;\n",
    "    for (int i = 0; i < sched->graph.n_nodes; i++)\n",
    "        if (sched->node_backend_ids[i] != sched->prev_node_backend_ids[i] &&\n",
    "            sched->bufts[sched->node_backend_ids[i]] != sched->bufts[sched->prev_node_backend_ids[i]])\n",
    "            backend_ids_changed = true;\n",
    "            break;\n",
    "    if (!backend_ids_changed)\n",
    "    for (int i = 0; i < sched->graph.n_leafs; i++)\n",
    "        if (sched->leaf_backend_ids[i] != sched->prev_leaf_backend_ids[i] &&\n",
    "            sched->bufts[sched->leaf_backend_ids[i]] != sched->bufts[sched->prev_leaf_backend_ids[i]])\n",
    "            backend_ids_changed = true;\n",
    "            break;\n",
    "    // allocate graph\n",
    "    // 一般情况下是不会进入if里面的语句的\n",
    "    --> if (backend_ids_changed || !ggml_gallocr_alloc_graph(sched->galloc, &sched->graph))\n",
    "        // the re-allocation may cause the split inputs to be moved to a different address\n",
    "        // synchronize without ggml_backend_sched_synchronize to avoid changing cur_copy\n",
    "        --> for (int i = 0; i < sched->n_backends; i++) ggml_backend_synchronize(sched->backends[i]);\n",
    "        --> ggml_gallocr_reserve_n(sched->galloc, &sched->graph, sched->node_backend_ids, sched->leaf_backend_ids);\n",
    "        --> ggml_gallocr_alloc_graph(sched->galloc, &sched->graph)\n",
    "    return true;\n",
    "}\n",
    "```\n",
    "\n",
    "[`ggml_gallocr_alloc_graph`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L871)\n",
    "\n",
    "> 真正开始给每个tensor的 buffer 和 data 域\n",
    "\n",
    "```c++\n",
    "bool ggml_gallocr_alloc_graph(ggml_gallocr_t galloc, struct ggml_cgraph * graph) {\n",
    "    // 一般不会进入\n",
    "    if (ggml_gallocr_needs_realloc(galloc, graph));\n",
    "    // reset buffers // 一般都是满足的, 但是CPU和GPU的reset函数指针都为空\n",
    "    for (int i = 0; i < galloc->n_buffers; i++);\n",
    "    // allocate the graph tensors from the previous assignments\n",
    "    // leafs\n",
    "    for (int i = 0; i < graph->n_leafs; i++) {\n",
    "        struct ggml_tensor * leaf = graph->leafs[i];\n",
    "        struct leaf_alloc * leaf_alloc = &galloc->leaf_allocs[i];\n",
    "        --> ggml_gallocr_init_tensor(galloc, leaf, &leaf_alloc->leaf);\n",
    "    }\n",
    "    // nodes\n",
    "    for (int i = 0; i < graph->n_nodes; i++) {\n",
    "        struct ggml_tensor * node = graph->nodes[i];\n",
    "        struct node_alloc * node_alloc = &galloc->node_allocs[i];\n",
    "        for (int j = 0; j < GGML_MAX_SRC; j++) {\n",
    "            struct ggml_tensor * src = node->src[j];\n",
    "            if (src == NULL) continue;\n",
    "            --> ggml_gallocr_init_tensor(galloc, src, &node_alloc->src[j]);\n",
    "        }\n",
    "        --> ggml_gallocr_init_tensor(galloc, node, &node_alloc->dst);\n",
    "    }\n",
    "    return true;\n",
    "}\n",
    "```\n",
    "\n",
    "```c++\n",
    "static void ggml_gallocr_init_tensor(ggml_gallocr_t galloc, struct ggml_tensor * tensor, struct tensor_alloc * tensor_alloc) {\n",
    "    int buffer_id = tensor_alloc->buffer_id;\n",
    "    if (tensor->view_src != NULL && tensor->buffer == NULL && tensor->view_src->buffer != NULL)\n",
    "        --> ggml_backend_view_init(tensor);\n",
    "    if (tensor->view_src == NULL && tensor->data == NULL)\n",
    "        void * base = ggml_backend_buffer_get_base(galloc->buffers[buffer_id]);\n",
    "        void * addr = (char *)base + tensor_alloc->offset;\n",
    "        // 分配地址空间 *key*\n",
    "        --> ggml_backend_tensor_alloc(galloc->buffers[buffer_id], tensor, addr);\n",
    "            --> tensor->buffer = buffer; tensor->data = addr;\n",
    "                // 最后的 ggml_backend_buffer_init_tensor 在 CPU, CUDA_HOST上没有对应的函数\n",
    "                // CUDA上上有，但是因为这个buffer的usaga是GGML_BACKEND_BUFFER_USAGE_COMPUTE，所以不起作用。\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dtmaghYQRaC",
   "metadata": {},
   "source": [
    "#### [`ggml_backend_cpu_buffer_set_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1890)\n",
    "\n",
    "```c++\n",
    "res->set_inputs(&ubatch);\n",
    "llm_graph_result::set_inputs -->\n",
    "    for (auto & input : inputs) input->set_input(ubatch);\n",
    "    llm_graph_input_embd::set_input;\n",
    "    llm_graph_input_pos::set_input;\n",
    "\n",
    "ggml_backend_tensor_set(llm_graph_input_embd:: ggml_tensor * tokens, ubatch->token, 0, n_tokens*ggml_element_size(tokens));\n",
    "ggml_backend_tensor_set(llm_graph_input_pos:: ggml_tensor * pos, ubatch->pos, 0, n_tokens*n_pos_per_embd*ggml_element_size(pos));\n",
    "----------\n",
    "void ggml_backend_tensor_set(struct ggml_tensor * tensor, const void * data, size_t offset, size_t size) {\n",
    "    ggml_backend_buffer_t buf = tensor->view_src ? tensor->view_src->buffer : tensor->buffer;\n",
    "    if (size == 0) {\n",
    "        return;\n",
    "    }\n",
    "    --> buf->iface.set_tensor(buf, tensor, data, offset, size);\n",
    "}\n",
    "\n",
    "static void ggml_backend_cpu_buffer_set_tensor(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor, const void * data, size_t offset, size_t size)\n",
    "    memcpy((char *)tensor->data + offset, data, size);\n",
    "```\n",
    "\n",
    "```log\n",
    ">>> p *ubatch->pos\n",
    "$1817 = 0\n",
    ">>> p *ubatch->token \n",
    "$1818 = 128000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_S6sI2iMU6g5",
   "metadata": {},
   "source": [
    "#### [`llama_kv_cache_unified::set_input_kq_mask`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L794)\n",
    "\n",
    "```c++\n",
    "res->set_inputs(&ubatch);\n",
    "llm_graph_result::set_inputs -->\n",
    "    for (auto & input : inputs) input->set_input(ubatch);\n",
    "----------\n",
    "void llm_graph_input_attn_kv_unified::set_input(const llama_ubatch * ubatch) {\n",
    "    --> kv_state->set_input_kq_mask(llama_kv_cache_unified:: ggml_tensor * self_kq_mask, ubatch, cparams.causal_attn);\n",
    "}\n",
    "void llama_kv_cache_unified_state::set_input_kq_mask(ggml_tensor * dst, const llama_ubatch * ubatch, bool causal_attn) const {\n",
    "    --> kv->set_input_kq_mask(dst, ubatch, causal_attn);\n",
    "}\n",
    "void llama_kv_cache_unified::set_input_kq_mask(ggml_tensor * dst, const llama_ubatch * ubatch, bool causal_attn) const {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DK8bnYH2WHlq",
   "metadata": {},
   "source": [
    "```c++\n",
    "const uint32_t n_tokens     = ubatch->n_tokens;\n",
    "float * data = (float *) dst->data;\n",
    "GGML_ASSERT(ggml_backend_buffer_is_host(dst->buffer));\n",
    "const int64_t n_kv = dst->ne[0];\n",
    "// Use only the previous KV cells of the correct sequence for each token of the ubatch.\n",
    "// It's assumed that if a token in the batch has multiple sequences, they are equivalent.\n",
    "// Example with a cache of 10 tokens, 2 tokens populated in cache and 3 tokens in batch:\n",
    "//   Causal mask:\n",
    "//      xxx-------\n",
    "//      xxxx------\n",
    "//      xxxxx-----\n",
    "//   Non-causal mask:\n",
    "//      xxxxx-----\n",
    "//      xxxxx-----\n",
    "//      xxxxx-----\n",
    "// To visualize the mask, see https://github.com/ggml-org/llama.cpp/pull/12615\n",
    "for (uint32_t s = 0; s < n_seqs; ++s) {\n",
    "    const llama_seq_id seq_id = ubatch->seq_id[s][0];\n",
    "    // for (uint32_t j = 0; j < n_seq_tokens; ++j)\n",
    "        // const uint32_t idx = s*n_seq_tokens + j;\n",
    "    const llama_pos p1 = ubatch->pos[s];\n",
    "    for (uint32_t i = 0; i < n_kv; ++i) {\n",
    "        float f = 0.0f;\n",
    "        bool masked = false;\n",
    "        if (cells.is_empty(i)) {\n",
    "            masked = true;\n",
    "        } else {\n",
    "            const llama_pos p0 = cells.pos_get(i);\n",
    "            // mask the token if not the same sequence\n",
    "            masked = masked || (!cells.seq_has(i, seq_id));\n",
    "            // mask future tokens\n",
    "            masked = masked || (causal_attn && p0 > p1);\n",
    "            // apply SWA if any\n",
    "            masked = masked || (is_masked_swa(p0, p1));\n",
    "            if (!masked && hparams.use_alibi) f = -std::abs(p0 - p1);\n",
    "        }\n",
    "        if (masked) f = -INFINITY;\n",
    "        data[n_kv*n_tokens + s*n_kv + i] = f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// mask padded tokens\n",
    "if (data) {\n",
    "    for (uint32_t j = n_tokens; j < GGML_PAD(n_tokens, GGML_KQ_MASK_PAD); ++j) {\n",
    "        for (uint32_t i = 0; i < n_kv; ++i) {\n",
    "            data[n_kv*n_tokens + j*n_kv + i] = -INFINITY;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177aa7a",
   "metadata": {},
   "source": [
    "#### [`llm_graph_input_out_ids::set_input`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-graph.cpp#L94)\n",
    "\n",
    "\n",
    "```c++\n",
    "void llm_graph_input_out_ids::set_input(const llama_ubatch * ubatch) {\n",
    "    const int64_t n_tokens = ubatch->n_tokens;\n",
    "    ggml_backend_buffer_is_host(out_ids->buffer);\n",
    "    int32_t * data = (int32_t *) out_ids->data;\n",
    "    if (n_outputs == n_tokens)\n",
    "        for (int i = 0; i < n_tokens; ++i) data[i] = i;\n",
    "        return;\n",
    "    int n_outputs = 0;\n",
    "    for (int i = 0; i < n_tokens; ++i)\n",
    "        if (ubatch->output[i]) data[n_outputs++] = i;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EwLiKExAOzNZ",
   "metadata": {},
   "source": [
    "#### ***[`ggml_backend_sched_compute_splits`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1360)\n",
    "\n",
    "> 除了少数特殊情况，一般都是只靠的sched split完之后生成的动态的计算图\n",
    "\n",
    "- [`llama_context::graph_compute`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L1369)\n",
    "\n",
    "```c++\n",
    "const auto status = graph_compute(gf, ubatch.n_tokens > 1);\n",
    "----------\n",
    "ggml_status llama_context::graph_compute(\n",
    "            ggml_cgraph * gf,\n",
    "                   bool   batched) {\n",
    "    int n_threads        = batched ? cparams.n_threads_batch : cparams.n_threads;\n",
    "    \n",
    "    ggml_threadpool_t tp = batched ? threadpool_batch        : threadpool;\n",
    "    auto * reg = ggml_backend_dev_backend_reg(ggml_backend_get_device(backend_cpu));\n",
    "    auto * set_threadpool_fn = (decltype(ggml_backend_cpu_set_threadpool) *) ggml_backend_reg_get_proc_address(reg, \"ggml_backend_cpu_set_threadpool\");\n",
    "    set_threadpool_fn(backend_cpu, tp);\n",
    "        void ggml_backend_cpu_set_threadpool(ggml_backend_t backend_cpu, ggml_threadpool_t threadpool) {}\n",
    "        --> ctx->threadpool = threadpool;\n",
    "    // set the number of threads for all the backends\n",
    "    for (const auto & set_n_threads_fn : set_n_threads_fns)\n",
    "        set_n_threads_fn.second(set_n_threads_fn.first, n_threads);\n",
    "            void ggml_backend_cpu_set_n_threads(ggml_backend_t backend_cpu, int n_threads) {}\n",
    "    \n",
    "    --> auto status = ggml_backend_sched_graph_compute_async(sched.get(), gf);\n",
    "    return status;\n",
    "}\n",
    "\n",
    "enum ggml_status ggml_backend_sched_graph_compute_async(ggml_backend_sched_t sched, struct ggml_cgraph * graph) {\n",
    "    // 这两个条件在decode里都不会被满足，也就是说看是靠的sched split完之后生成的动态的计算图\n",
    "    // if (!sched->is_reset && !sched->is_alloc) ggml_backend_sched_reset(sched);\n",
    "    // if (!sched->is_alloc) if (!ggml_backend_sched_alloc_graph(sched, graph)) return GGML_STATUS_ALLOC_FAILED;\n",
    "    --> return ggml_backend_sched_compute_splits(sched);\n",
    "}\n",
    "\n",
    "static enum ggml_status ggml_backend_sched_compute_splits(ggml_backend_sched_t sched) {\n",
    "    struct ggml_backend_sched_split * splits = sched->splits;\n",
    "    for (int i = 0; i < sched->n_splits; i++)\n",
    "        struct ggml_backend_sched_split * split = &splits[i];\n",
    "        int split_backend_id = split->backend_id;\n",
    "        ggml_backend_t split_backend = sched->backends[split_backend_id];\n",
    "        // copy the input tensors to the split backend\n",
    "        for (int j = 0; j < split->n_inputs; j++) {\n",
    "            ggml_backend_t input_backend = ggml_backend_sched_get_tensor_backend(sched, split->inputs[j]);\n",
    "            struct ggml_tensor * input = split->inputs[j];\n",
    "            struct ggml_tensor * input_cpy = tensor_copy(input, split_backend_id, sched->cur_copy);\n",
    "\n",
    "            if (input->flags & GGML_TENSOR_FLAG_INPUT)\n",
    "                // inputs from the user must be copied immediately to prevent the user overwriting the data before the copy is done\n",
    "                ggml_backend_synchronize(split_backend);\n",
    "                --> ggml_backend_tensor_copy(input, input_cpy);\n",
    "            else\n",
    "                // wait for the split backend to finish using the input before overwriting it\n",
    "                ggml_backend_synchronize(split_backend);\n",
    "                // try async copy, but if not possible, we can still use a sync copy without synchronizing the dst backend, since we handle the synchronization here with multiple copies and events\n",
    "                // TODO: add public function to facilitate this, since applications do not have direct access to the backend interface\n",
    "                if (!split_backend->iface.cpy_tensor_async ||\n",
    "                // 貌似不work！\n",
    "                --> !split_backend->iface.cpy_tensor_async(input_backend, split_backend, input, input_cpy))\n",
    "                    ggml_backend_synchronize(input_backend);\n",
    "                    ggml_backend_synchronize(split_backend);\n",
    "                    --> ggml_backend_tensor_copy(input, input_cpy);\n",
    "        }\n",
    "\n",
    "        if (!sched->callback_eval)\n",
    "            --> ggml_backend_graph_compute_async(split_backend, &split->graph);\n",
    "        // similar to ggml_backend_compare_graph_backend\n",
    "        else for (int j0 = 0; j0 < split->graph.n_nodes; j0++) {\n",
    "            struct ggml_tensor * t = split->graph.nodes[j0];\n",
    "            // check if the user needs data from this node\n",
    "            bool need = sched->callback_eval(t, true, sched->callback_eval_user_data);\n",
    "            int j1 = j0;\n",
    "            // determine the range [j0, j1] of nodes that can be computed together\n",
    "            while (!need && j1 < split->graph.n_nodes - 1) need = sched->callback_eval(split->graph.nodes[++j1], true, sched->callback_eval_user_data);\n",
    "            struct ggml_cgraph gv = ggml_graph_view(&split->graph, j0, j1 + 1);\n",
    "            --> ggml_backend_graph_compute_async(split_backend, &gv);\n",
    "            // TODO: pass backend to the callback, then the user can decide if they want to synchronize\n",
    "            ggml_backend_synchronize(split_backend);\n",
    "            if (need) sched->callback_eval(t, false, sched->callback_eval_user_data);\n",
    "            j0 = j1;\n",
    "        }\n",
    "        // record the event of this copy\n",
    "        // 我们不会用到 events 变量\n",
    "        // if (split->n_inputs > 0 && sched->events[split_backend_id][sched->cur_copy] != NULL) {}\n",
    "    // 我们并不关心，不起作用\n",
    "    // sched->cur_copy = (sched->cur_copy + 1) % sched->n_copies;\n",
    "    return GGML_STATUS_SUCCESS;\n",
    "}\n",
    "\n",
    "enum ggml_status ggml_backend_graph_compute_async(ggml_backend_t backend, struct ggml_cgraph * cgraph) {\n",
    "    --> return backend->iface.graph_compute(backend, cgraph);\n",
    "        --> ggml_backend_cpu_graph_compute;\n",
    "        --> ggml_backend_cuda_graph_compute;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22fef3f",
   "metadata": {},
   "source": [
    "```log\n",
    ">>> p sched->backends[0]->iface\n",
    "$1883 = {\n",
    "  get_name = 0x7fffe6d38049 <ggml_backend_cuda_get_name(ggml_backend_t)>,\n",
    "  free = 0x7fffe6d38077 <ggml_backend_cuda_free(ggml_backend_t)>,\n",
    "set_tensor_async = <ggml_backend_cuda_set_tensor_async(ggml_backend_t, ggml_tensor*, void const*, size_t, size_t)>,\n",
    "get_tensor_async = <ggml_backend_cuda_get_tensor_async(ggml_backend_t, ggml_tensor const*, void*, size_t, size_t)>,\n",
    "cpy_tensor_async = <ggml_backend_cuda_cpy_tensor_async(ggml_backend_t, ggml_backend_t, ggml_tensor const*, ggml_tensor*)>,\n",
    "  synchronize = 0x7fffe6d3879f <ggml_backend_cuda_synchronize(ggml_backend_t)>,\n",
    "  graph_plan_create = 0x0,\n",
    "  graph_plan_free = 0x0,\n",
    "  graph_plan_update = 0x0,\n",
    "  graph_plan_compute = 0x0,\n",
    "  graph_compute = <ggml_backend_cuda_graph_compute(ggml_backend_t, ggml_cgraph*)>,\n",
    "  event_record = <ggml_backend_cuda_event_record(ggml_backend_t, ggml_backend_event_t)>,\n",
    "  event_wait = <ggml_backend_cuda_event_wait(ggml_backend_t, ggml_backend_event_t)>\n",
    "}\n",
    "\n",
    ">>> p sched->splits[2]->inputs[0]->buffer->iface\n",
    "$1886 = {\n",
    "  free_buffer = 0x7fffe6d2f0f2 <ggml_backend_cuda_buffer_free_buffer(ggml_backend_buffer_t)>,\n",
    "  get_base = 0x7fffe6d2f156 <ggml_backend_cuda_buffer_get_base(ggml_backend_buffer_t)>,\n",
    "  init_tensor = 0x7fffe6d2f178 <ggml_backend_cuda_buffer_init_tensor(ggml_backend_buffer_t, ggml_tensor*)>,\n",
    "  memset_tensor = 0x7fffe6d2f2f1 <ggml_backend_cuda_buffer_memset_tensor(ggml_backend_buffer_t, ggml_tensor*, uint8_t, size_t, size_t)>,\n",
    "set_tensor = <ggml_backend_cuda_buffer_set_tensor(ggml_backend_buffer_t, ggml_tensor*, void const*, size_t, size_t)>,\n",
    "get_tensor = <ggml_backend_cuda_buffer_get_tensor(ggml_backend_buffer_t, ggml_tensor const*, void*, size_t, size_t)>,\n",
    "cpy_tensor = <ggml_backend_cuda_buffer_cpy_tensor(ggml_backend_buffer_t, ggml_tensor const*, ggml_tensor*)>,\n",
    "  clear = 0x7fffe6d2f76b <ggml_backend_cuda_buffer_clear(ggml_backend_buffer_t, uint8_t)>,\n",
    "  reset = 0x0\n",
    "}\n",
    "\n",
    ">>> p sched->backends[1]->iface\n",
    "$1882 = {\n",
    "  get_name = 0x7ffff7743276 <ggml_backend_cpu_get_name(ggml_backend_t)>,\n",
    "  free = 0x7ffff774328b <ggml_backend_cpu_free(ggml_backend_t)>,\n",
    "  set_tensor_async = 0x0,\n",
    "  get_tensor_async = 0x0,\n",
    "  cpy_tensor_async = 0x0,\n",
    "  synchronize = 0x0,\n",
    "  graph_plan_create = 0x7ffff77432f3 <ggml_backend_cpu_graph_plan_create(ggml_backend_t, ggml_cgraph const*)>,\n",
    "  graph_plan_free = 0x7ffff7743486 <ggml_backend_cpu_graph_plan_free(ggml_backend_t, ggml_backend_graph_plan_t)>,\n",
    "  graph_plan_update = 0x0,\n",
    "  graph_plan_compute = 0x7ffff77434d8 <ggml_backend_cpu_graph_plan_compute(ggml_backend_t, ggml_backend_graph_plan_t)>,\n",
    "  graph_compute = 0x7ffff774350d <ggml_backend_cpu_graph_compute(ggml_backend_t, ggml_cgraph*)>,\n",
    "  event_record = 0x0,\n",
    "  event_wait = 0x0\n",
    "}\n",
    "\n",
    ">>> p sched->splits[1]->inputs[0]->buffer->iface\n",
    "$1885 = {\n",
    "  free_buffer = 0x7fffe6d312c0 <ggml_backend_cuda_host_buffer_free_buffer(ggml_backend_buffer_t)>,\n",
    "  get_base = 0x7fffe6b714da <ggml_backend_cpu_buffer_get_base(ggml_backend_buffer_t)>,\n",
    "  init_tensor = 0x0,\n",
    "  memset_tensor = 0x7fffe6b71542 <ggml_backend_cpu_buffer_memset_tensor(ggml_backend_buffer_t, ggml_tensor*, uint8_t, size_t, size_t)>,\n",
    "set_tensor = <ggml_backend_cpu_buffer_set_tensor(ggml_backend_buffer_t, ggml_tensor*, void const*, size_t, size_t)>,\n",
    "get_tensor = <ggml_backend_cpu_buffer_get_tensor(ggml_backend_buffer_t, ggml_tensor const*, void*, size_t, size_t)>,\n",
    "cpy_tensor = <ggml_backend_cpu_buffer_cpy_tensor(ggml_backend_buffer_t, ggml_tensor const*, ggml_tensor*)>,\n",
    "  clear = 0x7fffe6b71686 <ggml_backend_cpu_buffer_clear(ggml_backend_buffer_t, uint8_t)>,\n",
    "  reset = 0x0\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261b2d1",
   "metadata": {},
   "source": [
    "##### [`ggml_backend_cuda_cpy_tensor_async`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L2442)\n",
    "\n",
    "> 注：CPU没有\n",
    "\n",
    "```c++\n",
    "static bool ggml_backend_cuda_cpy_tensor_async(ggml_backend_t backend_src, ggml_backend_t backend_dst, const ggml_tensor * src, ggml_tensor * dst) {\n",
    "    ggml_backend_buffer_t buf_src = src->view_src ? src->view_src->buffer : src->buffer;\n",
    "    ggml_backend_buffer_t buf_dst = dst->view_src ? dst->view_src->buffer : dst->buffer;\n",
    "    --> if (!ggml_backend_is_cuda(backend_src) || !ggml_backend_is_cuda(backend_dst)) return false;\n",
    "    // 后面的貌似都用不到\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976dad85",
   "metadata": {},
   "source": [
    "##### [`ggml_backend_tensor_copy`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L370)/`ggml_backend_tensor_set`\n",
    "\n",
    "```c++\n",
    "void ggml_backend_tensor_copy(struct ggml_tensor * src, struct ggml_tensor * dst) {\n",
    "    GGML_ASSERT(ggml_are_same_layout(src, dst) && \"cannot copy tensors with different layouts\");\n",
    "    // if (src == dst) return;\n",
    "    if (ggml_backend_buffer_is_host(src->buffer))\n",
    "        --> ggml_backend_tensor_set(dst, src->data, 0, ggml_nbytes(src));\n",
    "    else if (ggml_backend_buffer_is_host(dst->buffer))\n",
    "        --> ggml_backend_tensor_get(src, dst->data, 0, ggml_nbytes(src));\n",
    "    //最后一个分支我们不会走到\n",
    "    // } else if (!ggml_backend_buffer_copy_tensor(src, dst));\n",
    "}\n",
    "```\n",
    "\n",
    "[`ggml_backend_cuda_buffer_set_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L607)\n",
    "\n",
    "```c++\n",
    "void ggml_backend_tensor_set(struct ggml_tensor * tensor, const void * data, size_t offset, size_t size) {\n",
    "    ggml_backend_buffer_t buf = tensor->view_src ? tensor->view_src->buffer : tensor->buffer;\n",
    "    --> buf->iface.set_tensor(buf, tensor, data, offset, size);\n",
    "}\n",
    "static void ggml_backend_cuda_buffer_set_tensor(ggml_backend_buffer_t buffer, ggml_tensor * tensor, const void * data, size_t offset, size_t size) {\n",
    "    ggml_backend_cuda_buffer_context * ctx = (ggml_backend_cuda_buffer_context *)buffer->context;\n",
    "    ggml_cuda_set_device(ctx->device);\n",
    "    cudaMemcpyAsync((char *)tensor->data + offset, data, size, cudaMemcpyHostToDevice, cudaStreamPerThread);\n",
    "    cudaStreamSynchronize(cudaStreamPerThread);\n",
    "}\n",
    "```\n",
    "\n",
    "[`ggml_backend_cuda_buffer_get_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L615)\n",
    "\n",
    "```c++\n",
    "void ggml_backend_tensor_get(const struct ggml_tensor * tensor, void * data, size_t offset, size_t size) {\n",
    "    GGML_ASSERT(tensor);\n",
    "    ggml_backend_buffer_t buf = tensor->view_src ? tensor->view_src->buffer : tensor->buffer;\n",
    "    --> buf->iface.get_tensor(buf, tensor, data, offset, size);\n",
    "}\n",
    "static void ggml_backend_cuda_buffer_get_tensor(ggml_backend_buffer_t buffer, const ggml_tensor * tensor, void * data, size_t offset, size_t size) {\n",
    "    ggml_backend_cuda_buffer_context * ctx = (ggml_backend_cuda_buffer_context *)buffer->context;\n",
    "    ggml_cuda_set_device(ctx->device);\n",
    "    cudaMemcpyAsync(data, (const char *)tensor->data + offset, size, cudaMemcpyDeviceToHost, cudaStreamPerThread);\n",
    "    cudaStreamSynchronize(cudaStreamPerThread);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jCA4R77KBi6q",
   "metadata": {},
   "source": [
    "##### [`ggml_backend_cpu_graph_compute`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.cpp#L153)\n",
    "\n",
    "\n",
    "```c++\n",
    "static enum ggml_status ggml_backend_cpu_graph_compute(ggml_backend_t backend, struct ggml_cgraph * cgraph) {\n",
    "    struct ggml_backend_cpu_context * cpu_ctx = (struct ggml_backend_cpu_context *)backend->context;\n",
    "    --> ggml_graph_plan --> struct ggml_cplan cplan;\n",
    "    if (cpu_ctx->work_size < cplan.work_size) {\n",
    "        delete[] cpu_ctx->work_data;\n",
    "        cpu_ctx->work_data = new uint8_t[cplan.work_size];\n",
    "        cpu_ctx->work_size = cplan.work_size;\n",
    "    }\n",
    "    cplan.work_data = (uint8_t *)cpu_ctx->work_data;\n",
    "    --> return ggml_graph_compute;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HD0RdhZYCsRz",
   "metadata": {},
   "source": [
    "###### [`ggml_graph_plan`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.c#L2597)\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_cplan</summary>\n",
    "\n",
    "```c++\n",
    "// the compute plan that needs to be prepared for ggml_graph_compute()\n",
    "// since https://github.com/ggml-org/ggml/issues/287\n",
    "struct ggml_cplan {\n",
    "    size_t    work_size; // size of work buffer, calculated by `ggml_graph_plan()`\n",
    "    uint8_t * work_data; // work buffer, to be allocated by caller before calling to `ggml_graph_compute()`\n",
    "\n",
    "    int n_threads;\n",
    "    struct ggml_threadpool * threadpool;\n",
    "\n",
    "    // abort ggml_graph_compute when true\n",
    "    ggml_abort_callback abort_callback;\n",
    "    void *              abort_callback_data;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "struct ggml_cplan cplan = ggml_graph_plan(cgraph, cpu_ctx->n_threads, cpu_ctx->threadpool);\n",
    "----------\n",
    "struct ggml_cplan ggml_graph_plan(\n",
    "          const struct ggml_cgraph * cgraph,\n",
    "                               int   n_threads,\n",
    "            struct ggml_threadpool * threadpool) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EtHDLo8gJEj4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_type_traits_cpu</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_type_traits_cpu {\n",
    "    ggml_from_float_t        from_float;\n",
    "    ggml_vec_dot_t           vec_dot;\n",
    "    enum ggml_type           vec_dot_type;\n",
    "    int64_t                  nrows; // number of rows to process simultaneously\n",
    "};\n",
    "\n",
    "[GGML_TYPE_Q4_0] = {\n",
    "    .from_float               = quantize_row_q4_0,\n",
    "    .vec_dot                  = ggml_vec_dot_q4_0_q8_0,\n",
    "    .vec_dot_type             = GGML_TYPE_Q8_0,\n",
    "    .nrows                    = 1,\n",
    "},\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "size_t work_size = 0;\n",
    "struct ggml_cplan cplan;\n",
    "memset(&cplan, 0, sizeof(struct ggml_cplan));\n",
    "int max_tasks = 1;\n",
    "// thread scheduling for the different operations + work buffer size estimation\n",
    "for (int i = 0; i < cgraph->n_nodes; i++)\n",
    "    struct ggml_tensor * node = cgraph->nodes[i];\n",
    "    const int n_tasks = ggml_get_n_tasks(node, n_threads);\n",
    "    max_tasks = MAX(max_tasks, n_tasks);\n",
    "    size_t cur = 0;\n",
    "    // 暂时将extra功能给禁了\n",
    "    // if (!ggml_cpu_extra_work_size(n_threads, node, &cur));\n",
    "    switch (node->op)\n",
    "        case GGML_OP_MUL_MAT:\n",
    "            {\n",
    "                const enum ggml_type vec_dot_type = type_traits_cpu[node->src[0]->type].vec_dot_type;\n",
    "\n",
    "                if (node->src[1]->type != vec_dot_type) {\n",
    "                    cur = ggml_row_size(vec_dot_type, ggml_nelements(node->src[1]));\n",
    "                }\n",
    "            } break;\n",
    "        case GGML_OP_SOFT_MAX:\n",
    "        case GGML_OP_ROPE:\n",
    "        case GGML_OP_ROPE_BACK:\n",
    "            {\n",
    "                cur = ggml_type_size(GGML_TYPE_F32) * node->ne[0] * n_tasks;\n",
    "            } break;\n",
    "        case GGML_OP_CPY:\n",
    "        case GGML_OP_DUP:\n",
    "            {\n",
    "                if (ggml_is_quantized(node->type) ||\n",
    "                    // F16 -> BF16 and BF16 -> F16 copies go through intermediate F32\n",
    "                    (node->src[0]->type == GGML_TYPE_F16  && node->src[1] && node->src[1]->type == GGML_TYPE_BF16) ||\n",
    "                    (node->src[0]->type == GGML_TYPE_BF16 && node->src[1] && node->src[1]->type == GGML_TYPE_F16)) {\n",
    "                    cur = ggml_type_size(GGML_TYPE_F32) * node->ne[0] * n_tasks;\n",
    "                }\n",
    "            } break;\n",
    "        case GGML_OP_ADD:\n",
    "        case GGML_OP_ADD1:\n",
    "            {\n",
    "                if (ggml_is_quantized(node->src[0]->type)) {\n",
    "                    cur = ggml_type_size(GGML_TYPE_F32) * node->src[0]->ne[0] * n_tasks;\n",
    "                }\n",
    "            } break;\n",
    "    work_size = MAX(work_size, cur);\n",
    "\n",
    "work_size += CACHE_LINE_SIZE*(n_threads);\n",
    "cplan.threadpool = threadpool;\n",
    "cplan.n_threads  = MIN(max_tasks, n_threads);\n",
    "cplan.work_size  = work_size;\n",
    "cplan.work_data  = NULL;\n",
    "\n",
    "return cplan;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZeasLYKSC9_s",
   "metadata": {},
   "source": [
    "###### [`ggml_graph_compute`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.c#L3041)\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_threadpool</summary>\n",
    "\n",
    "```c++\n",
    "// Threadpool def\n",
    "struct ggml_threadpool {\n",
    "    ggml_mutex_t mutex;       // mutex for cond.var\n",
    "    ggml_cond_t  cond;        // cond.var for waiting for new work\n",
    "\n",
    "    struct ggml_cgraph * cgraph;\n",
    "    struct ggml_cplan  * cplan;\n",
    "\n",
    "    // synchronization primitives\n",
    "    atomic_int n_graph;       // incremented when there is work to be done (i.e each graph)\n",
    "    atomic_int GGML_CACHE_ALIGN n_barrier;\n",
    "    atomic_int GGML_CACHE_ALIGN n_barrier_passed;\n",
    "    atomic_int GGML_CACHE_ALIGN current_chunk; // currently processing chunk during Mat_Mul, shared between all the threads.\n",
    "\n",
    "    // these are atomic as an annotation for thread-sanitizer\n",
    "    atomic_bool stop;         // Used for stopping the threadpool altogether\n",
    "    atomic_bool pause;        // Used for pausing the threadpool or individual threads\n",
    "    atomic_int abort;         // Used for aborting processing of a graph\n",
    "\n",
    "    struct ggml_compute_state * workers;   // per thread state\n",
    "    int          n_threads_max; // number of threads in the pool\n",
    "    atomic_int   n_threads_cur; // number of threads used in the current graph\n",
    "\n",
    "    int32_t      prio;        // Scheduling priority\n",
    "    uint32_t     poll;        // Polling level (0 - no polling)\n",
    "\n",
    "    enum ggml_status ec;\n",
    "};\n",
    "\n",
    "static struct ggml_threadpool * ggml_threadpool_new_impl(\n",
    "    struct ggml_threadpool_params * tpp,\n",
    "               struct ggml_cgraph * cgraph,\n",
    "                struct ggml_cplan * cplan) {\n",
    "\n",
    "    struct ggml_threadpool * threadpool =\n",
    "        ggml_aligned_malloc(sizeof(struct ggml_threadpool));\n",
    "    {\n",
    "        threadpool->cgraph           = cgraph;\n",
    "        threadpool->cplan            = cplan;\n",
    "        threadpool->n_graph          = 0;\n",
    "        threadpool->n_barrier        = 0;\n",
    "        threadpool->n_barrier_passed = 0;\n",
    "        threadpool->current_chunk    = 0;\n",
    "        threadpool->stop             = false;\n",
    "        threadpool->pause            = tpp->paused;\n",
    "        threadpool->abort            = -1;\n",
    "        threadpool->workers          = NULL;\n",
    "        threadpool->n_threads_max    = tpp->n_threads;\n",
    "        threadpool->n_threads_cur    = tpp->n_threads;\n",
    "        threadpool->poll             = tpp->poll;\n",
    "        threadpool->prio             = tpp->prio;\n",
    "        threadpool->ec               = GGML_STATUS_SUCCESS;\n",
    "    }\n",
    "\n",
    "    // Allocate and init workers state\n",
    "    const size_t workers_size = sizeof(struct ggml_compute_state) * tpp->n_threads;\n",
    "    struct ggml_compute_state * workers = ggml_aligned_malloc(workers_size);\n",
    "\n",
    "    memset(workers, 0, workers_size);\n",
    "    for (int j = 0; j < tpp->n_threads; j++) {\n",
    "        workers[j].threadpool = threadpool;\n",
    "        workers[j].ith        = j;\n",
    "    }\n",
    "\n",
    "    threadpool->workers = workers;\n",
    "\n",
    "    return threadpool;\n",
    "}\n",
    "\n",
    "void ggml_threadpool_free(struct ggml_threadpool* threadpool) {\n",
    "    if (!threadpool) return;\n",
    "    const int n_threads = threadpool->n_threads_max;\n",
    "    const size_t workers_size = sizeof(struct ggml_compute_state) * n_threads;\n",
    "    ggml_aligned_free(threadpool->workers, workers_size);\n",
    "    ggml_aligned_free(threadpool, sizeof(struct ggml_threadpool));\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_threadpool_params</summary>\n",
    "\n",
    "```c++\n",
    "// threadpool params\n",
    "// Use ggml_threadpool_params_default() or ggml_threadpool_params_init() to populate the defaults\n",
    "struct ggml_threadpool_params {\n",
    "    bool                cpumask[GGML_MAX_N_THREADS]; // mask of cpu cores (all-zeros means use default affinity settings)\n",
    "    int                 n_threads;                   // number of threads\n",
    "    enum ggml_sched_priority prio;                   // thread priority\n",
    "    uint32_t            poll;                        // polling level (0 - no polling, 100 - aggressive polling)\n",
    "    bool                strict_cpu;                  // strict cpu placement\n",
    "    bool                paused;                      // start in paused state\n",
    "};\n",
    "\n",
    "void ggml_threadpool_params_init(struct ggml_threadpool_params * p, int n_threads) {\n",
    "    p->n_threads  = n_threads;\n",
    "    p->prio       = 0;     // default priority (usually means normal or inherited)\n",
    "    p->poll       = 50;    // hybrid-polling enabled\n",
    "    p->strict_cpu = false; // no strict placement (all threads share same cpumask)\n",
    "    p->paused     = false; // threads are ready to go\n",
    "    memset(p->cpumask, 0, GGML_MAX_N_THREADS); // all-zero means use the default affinity (usually inherited)\n",
    "}\n",
    "\n",
    "struct ggml_threadpool_params ggml_threadpool_params_default(int n_threads) {\n",
    "    struct ggml_threadpool_params p;\n",
    "    ggml_threadpool_params_init(&p, n_threads);\n",
    "    return p;\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "ggml_graph_compute(cgraph, &cplan);\n",
    "----------\n",
    "enum ggml_status ggml_graph_compute(struct ggml_cgraph * cgraph, struct ggml_cplan * cplan) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NIeRnANuUDao",
   "metadata": {},
   "source": [
    "```c++\n",
    "ggml_cpu_init();\n",
    "int n_threads = cplan->n_threads;\n",
    "struct ggml_threadpool * threadpool = cplan->threadpool;\n",
    "bool disposable_threadpool = false;\n",
    "if (threadpool == NULL) {\n",
    "    //GGML_PRINT_DEBUG(\"Threadpool is not specified. Will create a disposable threadpool : n_threads %d\\n\", n_threads);\n",
    "    disposable_threadpool = true;\n",
    "    struct ggml_threadpool_params ttp = ggml_threadpool_params_default(n_threads);\n",
    "    threadpool = ggml_threadpool_new_impl(&ttp, cgraph, cplan);\n",
    "}\n",
    "if (n_threads > 1) {\n",
    "    #pragma omp parallel num_threads(n_threads)\n",
    "    {\n",
    "        #pragma omp single\n",
    "        {\n",
    "            // update the number of threads from the actual number of threads that we got from OpenMP\n",
    "            n_threads = omp_get_num_threads();\n",
    "            atomic_store_explicit(&threadpool->n_threads_cur, n_threads, memory_order_relaxed);\n",
    "        }\n",
    "        --> ggml_graph_compute_thread(&threadpool->workers[omp_get_thread_num()]);\n",
    "    }\n",
    "} else {\n",
    "    atomic_store_explicit(&threadpool->n_threads_cur, 1, memory_order_relaxed);\n",
    "    --> ggml_graph_compute_thread(&threadpool->workers[0]);\n",
    "}\n",
    "enum ggml_status ret = threadpool->ec;\n",
    "if (disposable_threadpool) ggml_threadpool_free(threadpool);\n",
    "return ret;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VmDmN7blcbEA",
   "metadata": {},
   "source": [
    "[`ggml_compute_forward`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.c#L1648)\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_compute_state</summary>\n",
    "\n",
    "```c++\n",
    "// Per-thread state\n",
    "struct ggml_compute_state {\n",
    "    struct ggml_threadpool * threadpool;\n",
    "    int ith;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_compute_params</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_compute_params {\n",
    "    // ith = thread index, nth = number of threads\n",
    "    int ith, nth;\n",
    "\n",
    "    // work buffer for all threads\n",
    "    size_t wsize;\n",
    "    void * wdata;\n",
    "\n",
    "    struct ggml_threadpool * threadpool;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "> [`ggml_graph_compute_thread`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.c#L2785) 开始遍历当前split子图的每个node\n",
    "\n",
    "```c++\n",
    "static thread_ret_t ggml_graph_compute_thread(void * data) {\n",
    "    struct ggml_compute_state * state = (struct ggml_compute_state *) data;\n",
    "    struct ggml_threadpool    * tp    = state->threadpool;\n",
    "\n",
    "    const struct ggml_cgraph * cgraph = tp->cgraph;\n",
    "    const struct ggml_cplan  * cplan  = tp->cplan;\n",
    "\n",
    "    struct ggml_compute_params params = {\n",
    "        /*.ith       =*/ state->ith,\n",
    "        /*.nth       =*/ atomic_load_explicit(&tp->n_threads_cur, memory_order_relaxed),\n",
    "        /*.wsize     =*/ cplan->work_size,\n",
    "        /*.wdata     =*/ cplan->work_data,\n",
    "        /*.threadpool=*/ tp,\n",
    "    };\n",
    "\n",
    "    for (int node_n = 0; node_n < cgraph->n_nodes && atomic_load_explicit(&tp->abort, memory_order_relaxed) != node_n; node_n++) {\n",
    "        struct ggml_tensor * node = cgraph->nodes[node_n];\n",
    "        --> ggml_compute_forward(&params, node);\n",
    "        if (node_n + 1 < cgraph->n_nodes) {\n",
    "            --> ggml_barrier(state->threadpool);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    --> ggml_barrier(state->threadpool);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "static void ggml_compute_forward(struct ggml_compute_params * params, struct ggml_tensor * tensor) {\n",
    "    if (tensor->op == GGML_OP_NONE || ggml_is_empty(tensor)) return;\n",
    "\n",
    "    // extra_buffer op? 暂时将extra功能给禁了\n",
    "    if (ggml_cpu_extra_compute_forward(params, tensor)) return;\n",
    "    --> switch (tensor->op) {}\n",
    "}\n",
    "\n",
    "void ggml_barrier(struct ggml_threadpool * tp) {\n",
    "    int n_threads = atomic_load_explicit(&tp->n_threads_cur, memory_order_relaxed);\n",
    "    if (n_threads == 1) return;\n",
    "\n",
    "    #pragma omp barrier\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wLP5FfwA7VDU",
   "metadata": {},
   "source": [
    "[`ggml_compute_forward_get_rows_q`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ops.cpp#L4236)\n",
    "\n",
    "```c++\n",
    "ggml_compute_forward_get_rows_q(params, dst);\n",
    "----------\n",
    "static void ggml_compute_forward_get_rows_q(\n",
    "        const ggml_compute_params * params,\n",
    "              ggml_tensor * dst) {\n",
    "    const ggml_tensor * src0 = dst->src[0];\n",
    "    const ggml_tensor * src1 = dst->src[1];\n",
    "\n",
    "    GGML_TENSOR_BINARY_OP_LOCALS\n",
    "\n",
    "    const int64_t nc = ne00;\n",
    "    const int64_t nr = ggml_nelements(src1);\n",
    "\n",
    "    const ggml_type type = src0->type;\n",
    "    ggml_to_float_t const dequantize_row_q = ggml_get_type_traits(type)->to_float;\n",
    "\n",
    "    const int ith = params->ith;\n",
    "    const int nth = params->nth;\n",
    "\n",
    "    // rows per thread\n",
    "    const int dr = (nr + nth - 1)/nth;\n",
    "\n",
    "    // row range for this thread\n",
    "    const int ir0 = dr*ith;\n",
    "    const int ir1 = MIN(ir0 + dr, nr);\n",
    "\n",
    "    for (int64_t i = ir0; i < ir1; ++i) {\n",
    "        const int64_t i12 = i/(ne11*ne10);\n",
    "        const int64_t i11 = (i - i12*ne11*ne10)/ne10;\n",
    "        const int64_t i10 = (i - i12*ne11*ne10 - i11*ne10);\n",
    "        const int64_t i01 = *(int32_t *) ((char *) src1->data + i10*nb10 + i11*nb11 + i12*nb12);\n",
    "        dequantize_row_q(\n",
    "                (const void *) ((char *) src0->data + i01*nb01 + i11*nb02 + i12*nb03),\n",
    "                     (float *) ((char *)  dst->data + i10*nb1  + i11*nb2  + i12*nb3), nc);\n",
    "    }\n",
    "}\n",
    "\n",
    "void dequantize_row_q8_0(const block_q8_0 * GGML_RESTRICT x, float * GGML_RESTRICT y, int64_t k) {\n",
    "    static const int qk = QK8_0;\n",
    "    const int nb = k / qk;\n",
    "    for (int i = 0; i < nb; i++) {\n",
    "        const float d = GGML_FP16_TO_FP32(x[i].d);\n",
    "        for (int j = 0; j < qk; ++j) {\n",
    "            y[i*qk + j] = x[i].qs[j]*d;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FWeTlzsrcgbK",
   "metadata": {},
   "source": [
    "[`ggml_compute_forward_rms_norm_f32`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ops.cpp#L3270)\n",
    "\n",
    "```c++\n",
    "ggml_compute_forward_rms_norm(params, tensor);\n",
    "----------\n",
    "static void ggml_compute_forward_rms_norm_f32(\n",
    "        const ggml_compute_params * params,\n",
    "        ggml_tensor * dst) {\n",
    "    const ggml_tensor * src0 = dst->src[0];\n",
    "    const int ith = params->ith;\n",
    "    const int nth = params->nth;\n",
    "    GGML_TENSOR_UNARY_OP_LOCALS\n",
    "    float eps;\n",
    "    memcpy(&eps, dst->op_params, sizeof(float));\n",
    "    // TODO: optimize\n",
    "    for (int64_t i03 = 0; i03 < ne03; i03++) {\n",
    "        for (int64_t i02 = 0; i02 < ne02; i02++) {\n",
    "            for (int64_t i01 = ith; i01 < ne01; i01 += nth) {\n",
    "                const float * x = (float *) ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03);\n",
    "                ggml_float sum = 0.0;\n",
    "                for (int64_t i00 = 0; i00 < ne00; i00++) sum += (ggml_float)(x[i00] * x[i00]);\n",
    "                const float mean = sum/ne00;\n",
    "                float * y = (float *) ((char *) dst->data + i01*nb1 + i02*nb2 + i03*nb3);\n",
    "                memcpy(y, x, ne00 * sizeof(float));\n",
    "                // for (int i00 = 0; i00 < ne00; i00++) {\n",
    "                //     y[i00] = x[i00];\n",
    "                // }\n",
    "                const float scale = 1.0f/sqrtf(mean + eps);\n",
    "                ggml_vec_scale_f32(ne00, y, scale);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ajj30peBdIsN",
   "metadata": {},
   "source": [
    "[`apply_binary_op`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/binary-ops.cpp#L50)\n",
    "\n",
    "```c++\n",
    "ggml_compute_forward_mul(params, tensor);\n",
    "----------\n",
    "void ggml_compute_forward_mul(const ggml_compute_params * params, ggml_tensor * dst) {\n",
    "    binary_op<op_mul>(params, dst);\n",
    "}\n",
    "static void binary_op(const ggml_compute_params * params, ggml_tensor * dst)\n",
    "    const ggml_tensor * src0 = dst->src[0];\n",
    "    const ggml_tensor * src1 = dst->src[1];\n",
    "    /*  */ if (src0->type == GGML_TYPE_F32  && src1->type == GGML_TYPE_F32  && dst->type == GGML_TYPE_F32) // all f32\n",
    "        apply_binary_op<op, float, float, float>(params, dst);\n",
    "\n",
    "template <float (*op)(float, float), typename src0_t, typename src1_t, typename dst_t>\n",
    "static void apply_binary_op(const ggml_compute_params * params, ggml_tensor * dst) {\n",
    "    const ggml_tensor * src0 = dst->src[0];\n",
    "    const ggml_tensor * src1 = dst->src[1];\n",
    "\n",
    "    GGML_ASSERT(ggml_can_repeat(src1, src0) && ggml_are_same_shape(src0, dst));\n",
    "\n",
    "    GGML_TENSOR_BINARY_OP_LOCALS\n",
    "\n",
    "    GGML_ASSERT( nb0 == sizeof(dst_t));\n",
    "    GGML_ASSERT(nb00 == sizeof(src0_t));\n",
    "\n",
    "    const auto [ir0, ir1] = get_thread_range(params, src0);\n",
    "    const bool is_src1_contiguous = (nb10 == sizeof(src1_t)); // why:-?\n",
    "\n",
    "    if (!is_src1_contiguous) { // broadcast not implemented yet for non-contiguous\n",
    "        GGML_ASSERT(ggml_are_same_shape(src0, src1));\n",
    "    }\n",
    "\n",
    "    for (int64_t ir = ir0; ir < ir1; ++ir) {\n",
    "        const int64_t i03 = ir/(ne02*ne01);\n",
    "        const int64_t i02 = (ir - i03*ne02*ne01)/ne01;\n",
    "        const int64_t i01 = (ir - i03*ne02*ne01 - i02*ne01);\n",
    "\n",
    "        const int64_t i13 = i03 % ne13;\n",
    "        const int64_t i12 = i02 % ne12;\n",
    "        const int64_t i11 = i01 % ne11;\n",
    "\n",
    "        dst_t        * dst_ptr  = (dst_t  *)       ((char *)       dst->data  + i03*nb3  + i02*nb2  + i01*nb1 );\n",
    "        const src0_t * src0_ptr = (const src0_t *) ((const char *) src0->data + i03*nb03 + i02*nb02 + i01*nb01);\n",
    "        const src1_t * src1_ptr = (const src1_t *) ((const char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11);\n",
    "\n",
    "        if (is_src1_contiguous) {\n",
    "            // src1 is broadcastable across src0 and dst in i1, i2, i3\n",
    "            const int64_t nr0 = ne00 / ne10;\n",
    "\n",
    "            for (int64_t r = 0; r < nr0; ++r) {\n",
    "                --> vec_binary_op_contiguous<op>(ne10, dst_ptr + r*ne10, src0_ptr + r*ne10, src1_ptr);\n",
    "            }\n",
    "        } else {\n",
    "            vec_binary_op_non_contiguous<op>(ne0, ne10, nb10, dst_ptr, src0_ptr, src1_ptr);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "template <float (*op)(float, float), typename src0_t, typename src1_t, typename dst_t>\n",
    "static inline void vec_binary_op_contiguous(const int64_t n, dst_t * z, const src0_t * x, const src1_t * y) {\n",
    "    constexpr auto src0_to_f32 = type_conversion_table<src0_t>::to_f32;\n",
    "    constexpr auto src1_to_f32 = type_conversion_table<src1_t>::to_f32;\n",
    "    constexpr auto f32_to_dst  = type_conversion_table<dst_t >::from_f32;\n",
    "\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        z[i] = f32_to_dst(op(src0_to_f32(x[i]), src1_to_f32(y[i])));\n",
    "    }\n",
    "}\n",
    "\n",
    "static inline float f32_to_f32(float x) {\n",
    "    return x;\n",
    "}\n",
    "\n",
    "template <float (*op)(float, float), typename src0_t, typename src1_t, typename dst_t>\n",
    "static inline void vec_binary_op_non_contiguous(const int64_t n, const int64_t ne10, const int64_t nb10, dst_t * z, const src0_t * x, const src1_t * y) {\n",
    "    constexpr auto src0_to_f32 = type_conversion_table<src0_t>::to_f32;\n",
    "    constexpr auto src1_to_f32 = type_conversion_table<src1_t>::to_f32;\n",
    "    constexpr auto f32_to_dst  = type_conversion_table<dst_t >::from_f32;\n",
    "\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        int i10 = i % ne10;\n",
    "        const src1_t * y_ptr = (const src1_t *)((const char *)y + i10*nb10);\n",
    "        z[i] = f32_to_dst(op(src0_to_f32(x[i]), src1_to_f32(*y_ptr)));\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nmZj1wEu6KVQ",
   "metadata": {},
   "source": [
    "[`ggml_compute_forward_mul_mat`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.c#L1192\n",
    ")\n",
    "\n",
    "\n",
    "```c++\n",
    "static void ggml_compute_forward_mul_mat(\n",
    "        const struct ggml_compute_params * params,\n",
    "              struct ggml_tensor * dst) {\n",
    "\n",
    "    const struct ggml_tensor * src0 = dst->src[0];\n",
    "    const struct ggml_tensor * src1 = dst->src[1];\n",
    "\n",
    "    GGML_TENSOR_BINARY_OP_LOCALS\n",
    "\n",
    "    const int ith = params->ith;\n",
    "    const int nth = params->nth;\n",
    "\n",
    "    enum ggml_type           const vec_dot_type         = type_traits_cpu[src0->type].vec_dot_type;\n",
    "    ggml_from_float_t        const from_float           = type_traits_cpu[vec_dot_type].from_float;\n",
    "    int64_t                  const vec_dot_num_rows     = type_traits_cpu[src0->type].nrows;\n",
    "\n",
    "    // nb01 >= nb00 - src0 is not transposed\n",
    "    //   compute by src0 rows\n",
    "\n",
    "    // TODO: extract to \"extra_op\"\n",
    "\n",
    "    // if (src1->type != vec_dot_type) {\n",
    "    char * wdata = params->wdata;\n",
    "    const size_t nbw0 = ggml_type_size(vec_dot_type);\n",
    "    const size_t nbw1 = ggml_row_size(vec_dot_type, ne10);\n",
    "    const size_t nbw2 = nbw1*ne11;\n",
    "    const size_t nbw3 = nbw2*ne12;\n",
    "\n",
    "    for (int64_t i11 = 0; i11 < ne11; ++i11) {\n",
    "        size_t bs = ggml_blck_size(vec_dot_type);\n",
    "        int64_t ne10_block_start = (ith * ne10/bs) / nth;\n",
    "        int64_t ne10_block_end   = ((ith + 1) * ne10/bs) / nth;\n",
    "        --> from_float(\n",
    "          (float *)((char *) src1->data + i11*nb11 + ne10_block_start*bs*nb10),\n",
    "          (void *) (wdata + i11*nbw1 + ne10_block_start*nbw0),\n",
    "          (ne10_block_end - ne10_block_start) * bs);\n",
    "    }\n",
    "\n",
    "    if (ith == 0) {\n",
    "        // Every thread starts at ith, so the first unprocessed chunk is nth.  This save a bit of coordination right at the start.\n",
    "        // 没看懂这个是什么意思啊\n",
    "        // 结合着后面的atomic_fetch_add_explicit原子操作，这下看懂了\n",
    "        atomic_store_explicit(&params->threadpool->current_chunk, nth, memory_order_relaxed);\n",
    "    }\n",
    "\n",
    "    ggml_barrier(params->threadpool);\n",
    "\n",
    "    // This is the size of the first dimension of the result, so we can iterate that way. (see the ASSERT above, these are the same numbers)\n",
    "    const int64_t nr0 = ne0;\n",
    "\n",
    "    // This is the size of the rest of the dimensions of the result\n",
    "    const int64_t nr1 = ne1 * ne2 * ne3;\n",
    "\n",
    "    // Now select a reasonable chunk size.\n",
    "    int chunk_size = 16;\n",
    "\n",
    "    // We need to step up the size if it's small\n",
    "    if (nr0 == 1 || nr1 == 1) {\n",
    "        chunk_size = 64;\n",
    "    }\n",
    "\n",
    "    // distribute the work across the inner or outer loop based on which one is larger\n",
    "    // The number of chunks in the 0/1 dim.\n",
    "    // CEIL(nr0/chunk_size)\n",
    "    int64_t nchunk0 = (nr0 + chunk_size - 1) / chunk_size;\n",
    "    int64_t nchunk1 = (nr1 + chunk_size - 1) / chunk_size;\n",
    "\n",
    "    // If the chunking is poor for the number of threads on this setup, scrap the whole plan.  Re-chunk it by thread.\n",
    "    //   Also, chunking by thread was measured to have perform better on NUMA systems.  See https://github.com/ggml-org/llama.cpp/pull/6915\n",
    "    //   In theory, chunking should be just as useful on NUMA and non NUMA systems, but testing disagreed with that.\n",
    "    if (nchunk0 * nchunk1 < nth * 4 || ggml_is_numa()) {\n",
    "        // distribute the thread work across the inner or outer loop based on which one is larger\n",
    "        nchunk0 = nr0 > nr1 ? nth : 1; // parallelize by src0 rows\n",
    "        nchunk1 = nr0 > nr1 ? 1 : nth; // parallelize by src1 rows\n",
    "    }\n",
    "\n",
    "    // The number of elements in each chunk\n",
    "    const int64_t dr0 = (nr0 + nchunk0 - 1) / nchunk0;\n",
    "    const int64_t dr1 = (nr1 + nchunk1 - 1) / nchunk1;\n",
    "\n",
    "    // The first chunk comes from our thread_id, the rest will get auto-assigned.\n",
    "    int current_chunk = ith;\n",
    "\n",
    "    while (current_chunk < nchunk0 * nchunk1) {\n",
    "        const int64_t ith0 = current_chunk % nchunk0;\n",
    "        const int64_t ith1 = current_chunk / nchunk0;\n",
    "\n",
    "        const int64_t ir0_start = dr0 * ith0;\n",
    "        const int64_t ir0_end = MIN(ir0_start + dr0, nr0);\n",
    "\n",
    "        const int64_t ir1_start = dr1 * ith1;\n",
    "        const int64_t ir1_end = MIN(ir1_start + dr1, nr1);\n",
    "\n",
    "        // dot kernels can handle 1 row and col at a time, but mmla kernels can process 2 rows and cols\n",
    "        int64_t num_rows_per_vec_dot = vec_dot_num_rows;\n",
    "\n",
    "        // these checks are needed to avoid crossing dim1 boundaries\n",
    "        // can be optimized, but the logic would become more complicated, so keeping it like this for simplicity\n",
    "        if ((nr0 % 2 != 0) || (ne11 % 2 != 0) || ((ir0_end - ir0_start) % 2 != 0) || ((ir1_end - ir1_start) % 2 != 0)) {\n",
    "            num_rows_per_vec_dot = 1;\n",
    "        }\n",
    "        --> ggml_compute_forward_mul_mat_one_chunk(params, dst, src0->type, num_rows_per_vec_dot, ir0_start, ir0_end, ir1_start, ir1_end);\n",
    "\n",
    "        if (nth >= nchunk0 * nchunk1) {\n",
    "            break;\n",
    "        }\n",
    "\n",
    "        current_chunk = atomic_fetch_add_explicit(&params->threadpool->current_chunk, 1, memory_order_relaxed);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ty17RxOAFe5N",
   "metadata": {},
   "source": [
    "\n",
    "```c++\n",
    "static void ggml_compute_forward_mul_mat_one_chunk(\n",
    "    const struct ggml_compute_params * params,\n",
    "    struct ggml_tensor * dst,\n",
    "    const enum ggml_type type,\n",
    "    const int64_t num_rows_per_vec_dot,\n",
    "    const int64_t ir0_start,\n",
    "    const int64_t ir0_end,\n",
    "    const int64_t ir1_start,\n",
    "    const int64_t ir1_end) {\n",
    "\n",
    "    const struct ggml_tensor * src0 = dst->src[0];\n",
    "    const struct ggml_tensor * src1 = dst->src[1];\n",
    "\n",
    "    GGML_TENSOR_BINARY_OP_LOCALS\n",
    "\n",
    "    const bool src1_cont = ggml_is_contiguous(src1);\n",
    "\n",
    "    ggml_vec_dot_t const vec_dot      = type_traits_cpu[type].vec_dot;\n",
    "    enum ggml_type const vec_dot_type = type_traits_cpu[type].vec_dot_type;\n",
    "\n",
    "    // broadcast factors 我们的case貌似不需要考虑？\n",
    "    // threads with no work simply yield (not sure if it helps)\n",
    "    if (ir0_start >= ir0_end || ir1_start >= ir1_end) {\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    const void * wdata = params->wdata;\n",
    "    const size_t row_size = ggml_row_size(vec_dot_type, ne10);\n",
    "\n",
    "    assert(ne12 % ne02 == 0);\n",
    "    assert(ne13 % ne03 == 0);\n",
    "\n",
    "    // block-tiling attempt\n",
    "    const int64_t blck_0 = 16;\n",
    "    const int64_t blck_1 = 16;\n",
    "\n",
    "    // const size_t src1_col_stride = src1_cont || src1->type != vec_dot_type ? row_size : nb11; 貌似我们不用考虑src1不cont的情况\n",
    "    const size_t src1_col_stride = row_size;\n",
    "\n",
    "    // attempt to reduce false-sharing (does not seem to make a difference)\n",
    "    // 16 * 2, accounting for mmla kernels\n",
    "    // float tmp[32];\n",
    "    for (int64_t iir1 = ir1_start; iir1 < ir1_end; iir1 += blck_1)\n",
    "    for (int64_t iir0 = ir0_start; iir0 < ir0_end; iir0 += blck_0)\n",
    "    // 理想情况下，上面两个循环是没有用的\n",
    "    for (int64_t ir1 = iir1; ir1 < iir1 + blck_1 && ir1 < ir1_end; ++ir1)\n",
    "    for (int64_t ir0 = iir0; ir0 < iir0 + blck_0 && ir0 < ir0_end; ++ir0)\n",
    "        --> vec_dot(\n",
    "              n=ne00,\n",
    "              s=(float*)((char*)dst->data + ir0 + ir1 * nb1), //dst_col\n",
    "              bs=0, // UNUSED\n",
    "              vx=(const char*)src0->data + ir0 * nb01, //src0_row\n",
    "              bx=0, // UNUSED\n",
    "              vy=(const char*)wdata + ir1 * row_size, // src1_col\n",
    "              by=0, // UNUSED\n",
    "              nrc=1 // UNUSED\n",
    "          );\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kT_ll-E_0rWg",
   "metadata": {},
   "source": [
    "[`ggml_vec_dot_q4_0_q8_0`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/arch/x86/quants.c#L531)\n",
    "\n",
    "| 特性                | `_mm256_maddubs_epi16`               | `_mm256_madd_epi16`               |\n",
    "|---------------------|--------------------------------------|-----------------------------------|\n",
    "| **输入数据类型**    | 无符号 8 位 × 有符号 8 位            | 有符号 16 位 × 有符号 16 位       |\n",
    "| **输出数据类型**    | 16 位有符号整数（16 个结果）         | 32 位有符号整数（8 个结果）       |\n",
    "| **运算粒度**        | 8 位 → 16 位                         | 16 位 → 32 位                     |\n",
    "| **相邻加法**        | 每对 8 位乘积相加                    | 每对 16 位乘积相加                |\n",
    "| **适用场景**        | 低精度计算（如图像处理）             | 高精度计算（如数值运算）          |\n",
    "\n",
    "<details>\n",
    "<summary>original implementation</summary>\n",
    "\n",
    "```c++\n",
    "float sumf = 0;\n",
    "for (ib = 0; ib < nb; ++ib) {\n",
    "    int sumi0 = 0;\n",
    "    int sumi1 = 0;\n",
    "\n",
    "    for (int j = 0; j < qk/2; ++j) {\n",
    "        const int v0 = (x[ib].qs[j] & 0x0F) - 8;\n",
    "        const int v1 = (x[ib].qs[j] >>   4) - 8;\n",
    "\n",
    "        sumi0 += (v0 * y[ib].qs[j]);\n",
    "        sumi1 += (v1 * y[ib].qs[j + qk/2]);\n",
    "    }\n",
    "\n",
    "    int sumi = sumi0 + sumi1;\n",
    "    sumf += sumi*GGML_FP16_TO_FP32(x[ib].d)*GGML_FP16_TO_FP32(y[ib].d);\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "void ggml_vec_dot_q4_0_q8_0(int n, float * GGML_RESTRICT s, size_t, const void * GGML_RESTRICT vx, size_t, const void * GGML_RESTRICT vy, size_t, int) {\n",
    "    const int qk = QK8_0;\n",
    "    const int nb = n / qk;\n",
    "    const block_q4_0 * GGML_RESTRICT x = vx;\n",
    "    const block_q8_0 * GGML_RESTRICT y = vy;\n",
    "    int ib = 0;\n",
    "    float sumf = 0;\n",
    "    // Initialize accumulator with zeros\n",
    "    __m256 acc = _mm256_setzero_ps();\n",
    "\n",
    "    // Main loop\n",
    "    for (; ib < nb; ++ib) {\n",
    "        /* Compute combined scale for the block */\n",
    "        const __m256 d = _mm256_set1_ps( GGML_FP16_TO_FP32(x[ib].d) * GGML_FP16_TO_FP32(y[ib].d) );\n",
    "\n",
    "        --> __m256i qx = bytes_from_nibbles_32(x[ib].qs);\n",
    "\n",
    "        // Now we have a vector with bytes in [ 0 .. 15 ] interval. Offset them into [ -8 .. +7 ] interval.\n",
    "        const __m256i off = _mm256_set1_epi8( 8 );\n",
    "        qx = _mm256_sub_epi8( qx, off );\n",
    "\n",
    "        __m256i qy = _mm256_loadu_si256((const __m256i *)y[ib].qs);\n",
    "\n",
    "        --> const __m256 q = mul_sum_i8_pairs_float(qx, qy);\n",
    "\n",
    "        /* Multiply q with scale and accumulate */\n",
    "        acc = _mm256_fmadd_ps( d, q, acc );\n",
    "    }\n",
    "\n",
    "    --> sumf = hsum_float_8(acc);\n",
    "    *s = sumf;\n",
    "}\n",
    "\n",
    "// Unpack 32 4-bit fields into 32 bytes\n",
    "// The output vector contains 32 bytes, each one in [ 0 .. 15 ] interval\n",
    "static inline __m256i bytes_from_nibbles_32(const uint8_t * rsi)\n",
    "{\n",
    "    const __m128i tmp = _mm_loadu_si128((const __m128i *)rsi);\n",
    "    const __m256i bytes = MM256_SET_M128I(_mm_srli_epi16(tmp, 4), tmp);\n",
    "    const __m256i lowMask = _mm256_set1_epi8( 0xF );\n",
    "    return _mm256_and_si256(lowMask, bytes);\n",
    "}\n",
    "\n",
    "// 4->8->16->32\n",
    "\n",
    "// multiply int8_t, add results pairwise twice and return as float vector\n",
    "static inline __m256 mul_sum_i8_pairs_float(const __m256i x, const __m256i y) {\n",
    "#if __AVXVNNIINT8__\n",
    "    const __m256i zero = _mm256_setzero_si256();\n",
    "    const __m256i summed_pairs = _mm256_dpbssd_epi32(zero, x, y);\n",
    "    return _mm256_cvtepi32_ps(summed_pairs);\n",
    "#else\n",
    "    // Get absolute values of x vectors\n",
    "    const __m256i ax = _mm256_sign_epi8(x, x);\n",
    "    // Sign the values of the y vectors\n",
    "    const __m256i sy = _mm256_sign_epi8(y, x);\n",
    "    return mul_sum_us8_pairs_float(ax, sy);\n",
    "#endif\n",
    "}\n",
    "\n",
    "static inline __m256 mul_sum_us8_pairs_float(const __m256i ax, const __m256i sy) {\n",
    "#if defined(__AVXVNNI__)\n",
    "    const __m256i zero = _mm256_setzero_si256();\n",
    "    // const __m256i summed_pairs = _mm256_dpbusd_epi32(zero, ax, sy);\n",
    "    const __m256i summed_pairs = _mm256_dpbusd_avx_epi32(zero, ax, sy);\n",
    "    return _mm256_cvtepi32_ps(summed_pairs);\n",
    "#else\n",
    "    // Perform multiplication and create 16-bit values\n",
    "    const __m256i dot = _mm256_maddubs_epi16(ax, sy);\n",
    "    const __m256i ones = _mm256_set1_epi16(1);\n",
    "    const __m256i summed_pairs = _mm256_madd_epi16(ones, dot);\n",
    "    return _mm256_cvtepi32_ps(summed_pairs);\n",
    "#endif\n",
    "}\n",
    "\n",
    "// horizontally add 8 floats\n",
    "static inline float hsum_float_8(const __m256 x) {\n",
    "    __m128 res = _mm256_extractf128_ps(x, 1);\n",
    "    res = _mm_add_ps(res, _mm256_castps256_ps128(x));\n",
    "    res = _mm_add_ps(res, _mm_movehl_ps(res, res));\n",
    "    res = _mm_add_ss(res, _mm_movehdup_ps(res));\n",
    "    return _mm_cvtss_f32(res);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kJxjlKTh8oyg",
   "metadata": {},
   "source": [
    "[`llamafile_sgemm`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/llamafile/sgemm.cpp#L3310)\n",
    "\n",
    "加速比大概是2倍，但是为了简单起见，我们先禁用了。\n",
    "\n",
    "```c++\n",
    "llamafile_sgemm(params,\n",
    "                m=ne01, n=ne11, k=ne00/ggml_blck_size(src0->type),\n",
    "                A=(const char *)src0->data,\n",
    "                lda=nb01/ggml_type_size(src0->type),\n",
    "                B=(const char *)src1->data,\n",
    "                ldb=nb11/ggml_type_size(src1->type),\n",
    "                C=(char *)dst->data,\n",
    "                ldc=nb1/ggml_type_size(dst->type),\n",
    "                Atype=src0->type,\n",
    "                Btype=src1->type,\n",
    "                Ctype=dst->type);\n",
    "\n",
    "const void* wdata = params->wdata;\n",
    "const size_t row_size = ggml_row_size(vec_dot_type, ne10);\n",
    "llamafile_sgemm(params,\n",
    "                ne01, ne11, ne00/ggml_blck_size(src0->type),\n",
    "                (const char *)src0->data,\n",
    "                nb01/ggml_type_size(src0->type),\n",
    "                (const char *)wdata,\n",
    "                row_size/ggml_type_size(vec_dot_type),\n",
    "                (char *)dst->data,\n",
    "                nb1/ggml_type_size(dst->type),\n",
    "                src0->type,\n",
    "                vec_dot_type,\n",
    "                dst->type);\n",
    "----------\n",
    "/**\n",
    " * Performs optimized matrix multiplication on CPU.\n",
    " *\n",
    " * This subroutine may compute C = Aᵀ * B with column major ordering.\n",
    " * Despite its name, this isn't a generalized implementation. Work is\n",
    " * only performed when a handwritten kernel is written and available.\n",
    " * Otherwise the caller should fall back to a general matmul routine.\n",
    " *\n",
    " * For example, for single-threaded single-precision GEMM you can say\n",
    " *\n",
    " *     llamafile_sgemm(m, n, k, A, lda, B, ldb, C, ldc,\n",
    " *                     0, 1,\n",
    " *                     GGML_TYPE_F32, GGML_TYPE_F32, GGML_TYPE_F32);\n",
    " *\n",
    " * @param m is rows in `A` and `C`\n",
    " * @param n is cols in `B` and `C`\n",
    " * @param k is cols in `A` and rows in `B`\n",
    " * @param A is first input matrix (always transposed)\n",
    " * @param lda is row stride of `A`\n",
    " * @param B is second input matrix (never transposed)\n",
    " * @param ldb is row stride of `B`\n",
    " * @param C is input/output array of output matrices\n",
    " * @param ldc is row stride of `C`\n",
    " * @param ith is thread id (must be less than `nth`)\n",
    " * @param nth is number of threads (must be greater than zero)\n",
    " * @param Atype is GGML data type of `A`\n",
    " * @param Btype is GGML data type of `B`\n",
    " * @param Ctype is GGML data type of `C`\n",
    " * @return true if this function was able to service the matmul request\n",
    " */\n",
    "bool llamafile_sgemm(const struct ggml_compute_params * params, int64_t m, int64_t n, int64_t k,\n",
    "                     const void *A, int64_t lda, const void *B, int64_t ldb, void *C,\n",
    "                     int64_t ldc, int Atype, int Btype, int Ctype) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ogCMyjuGytL",
   "metadata": {},
   "source": [
    "\n",
    "```c++\n",
    "void quantize_row_q8_0(const float * GGML_RESTRICT x, void * GGML_RESTRICT vy, int64_t k) {\n",
    "    assert(QK8_0 == 32);\n",
    "    assert(k % QK8_0 == 0);\n",
    "    const int nb = k / QK8_0;\n",
    "    block_q8_0 * GGML_RESTRICT y = vy;\n",
    "\n",
    "    for (int i = 0; i < nb; i++) {\n",
    "        // Load elements into 4 AVX vectors\n",
    "        __m256 v0 = _mm256_loadu_ps( x );\n",
    "        __m256 v1 = _mm256_loadu_ps( x + 8 );\n",
    "        __m256 v2 = _mm256_loadu_ps( x + 16 );\n",
    "        __m256 v3 = _mm256_loadu_ps( x + 24 );\n",
    "        x += 32;\n",
    "        // Compute max(abs(e)) for the block\n",
    "        const __m256 signBit = _mm256_set1_ps( -0.0f );\n",
    "        __m256 maxAbs = _mm256_andnot_ps( signBit, v0 );\n",
    "        maxAbs = _mm256_max_ps( maxAbs, _mm256_andnot_ps( signBit, v1 ) );\n",
    "        maxAbs = _mm256_max_ps( maxAbs, _mm256_andnot_ps( signBit, v2 ) );\n",
    "        maxAbs = _mm256_max_ps( maxAbs, _mm256_andnot_ps( signBit, v3 ) );\n",
    "        __m128 max4 = _mm_max_ps( _mm256_extractf128_ps( maxAbs, 1 ), _mm256_castps256_ps128( maxAbs ) );\n",
    "        max4 = _mm_max_ps( max4, _mm_movehl_ps( max4, max4 ) );\n",
    "        max4 = _mm_max_ss( max4, _mm_movehdup_ps( max4 ) );\n",
    "        const float maxScalar = _mm_cvtss_f32( max4 );\n",
    "        // Quantize these floats\n",
    "        const float d = maxScalar / 127.f;\n",
    "        y[i].d = GGML_FP32_TO_FP16(d);\n",
    "        const float id = ( maxScalar != 0.0f ) ? 127.f / maxScalar : 0.0f;\n",
    "        const __m256 mul = _mm256_set1_ps( id );\n",
    "        // Apply the multiplier\n",
    "        v0 = _mm256_mul_ps( v0, mul );\n",
    "        v1 = _mm256_mul_ps( v1, mul );\n",
    "        v2 = _mm256_mul_ps( v2, mul );\n",
    "        v3 = _mm256_mul_ps( v3, mul );\n",
    "        // Round to nearest integer\n",
    "        v0 = _mm256_round_ps( v0, _MM_ROUND_NEAREST );\n",
    "        v1 = _mm256_round_ps( v1, _MM_ROUND_NEAREST );\n",
    "        v2 = _mm256_round_ps( v2, _MM_ROUND_NEAREST );\n",
    "        v3 = _mm256_round_ps( v3, _MM_ROUND_NEAREST );\n",
    "        // Convert floats to integers\n",
    "        __m256i i0 = _mm256_cvtps_epi32( v0 );\n",
    "        __m256i i1 = _mm256_cvtps_epi32( v1 );\n",
    "        __m256i i2 = _mm256_cvtps_epi32( v2 );\n",
    "        __m256i i3 = _mm256_cvtps_epi32( v3 );\n",
    "        // Convert int32 to int16\n",
    "        i0 = _mm256_packs_epi32( i0, i1 );\t// 0, 1, 2, 3,  8, 9, 10, 11,  4, 5, 6, 7, 12, 13, 14, 15\n",
    "        i2 = _mm256_packs_epi32( i2, i3 );\t// 16, 17, 18, 19,  24, 25, 26, 27,  20, 21, 22, 23, 28, 29, 30, 31\n",
    "                                            // Convert int16 to int8\n",
    "        i0 = _mm256_packs_epi16( i0, i2 );\t// 0, 1, 2, 3,  8, 9, 10, 11,  16, 17, 18, 19,  24, 25, 26, 27,  4, 5, 6, 7, 12, 13, 14, 15, 20, 21, 22, 23, 28, 29, 30, 31\n",
    "        // We got our precious signed bytes, but the order is now wrong\n",
    "        // These AVX2 pack instructions process 16-byte pieces independently\n",
    "        // The following instruction is fixing the order\n",
    "        const __m256i perm = _mm256_setr_epi32( 0, 4, 1, 5, 2, 6, 3, 7 );\n",
    "        i0 = _mm256_permutevar8x32_epi32( i0, perm );\n",
    "        _mm256_storeu_si256((__m256i *)y[i].qs, i0);\n",
    "    }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df2b3f4",
   "metadata": {},
   "source": [
    "##### [`ggml_backend_cuda_graph_compute`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L2752)\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_cuda_graph</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_cuda_graph {\n",
    "// #ifdef USE_CUDA_GRAPH\n",
    "    ~ggml_cuda_graph() {\n",
    "        if (instance != nullptr) CUDA_CHECK(cudaGraphExecDestroy(instance));\n",
    "        if (graph != nullptr) CUDA_CHECK(cudaGraphDestroy(graph));\n",
    "    }\n",
    "    cudaGraph_t graph = nullptr;\n",
    "    cudaGraphExec_t instance = nullptr;\n",
    "    size_t num_nodes = 0;\n",
    "    std::vector<cudaGraphNode_t> nodes;\n",
    "    std::vector<cudaKernelNodeParams> params;\n",
    "    bool disable_due_to_gpu_arch = false;\n",
    "    bool disable_due_to_too_many_updates = false;\n",
    "    bool disable_due_to_failed_graph_capture = false;\n",
    "    int number_consecutive_updates = 0;\n",
    "    std::vector<ggml_graph_node_properties> ggml_graph_properties;\n",
    "    bool use_cpy_indirection = false;\n",
    "    std::vector<char *> cpy_dest_ptrs;\n",
    "    char ** dest_ptrs_d;\n",
    "    int dest_ptrs_size = 0;\n",
    "    // Index to allow each cpy kernel to be aware of it's position within the graph\n",
    "    // relative to other cpy nodes.\n",
    "    int graph_cpynode_index = -1;\n",
    "// #endif\n",
    "};\n",
    "\n",
    ">>> p *cuda_ctx->cuda_graph\n",
    "$1707 = {\n",
    "  graph = 0x0,\n",
    "  instance = 0x0,\n",
    "  num_nodes = 0,\n",
    "  nodes = std::vector of length 0, capacity 0,\n",
    "  params = std::vector of length 0, capacity 0,\n",
    "  disable_due_to_gpu_arch = false,\n",
    "  disable_due_to_too_many_updates = false,\n",
    "  disable_due_to_failed_graph_capture = false,\n",
    "  number_consecutive_updates = 0,\n",
    "  ggml_graph_properties = std::vector of length 0, capacity 0,\n",
    "  use_cpy_indirection = false,\n",
    "  cpy_dest_ptrs = std::vector of length 0, capacity 0,\n",
    "  dest_ptrs_d = 0x0,\n",
    "  dest_ptrs_size = 0,\n",
    "  graph_cpynode_index = -1\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "> 这个函数说明了，难怪要warmup\n",
    "\n",
    "```c++\n",
    "static enum ggml_status ggml_backend_cuda_graph_compute(ggml_backend_t backend, ggml_cgraph * cgraph) {\n",
    "    ggml_backend_cuda_context * cuda_ctx = (ggml_backend_cuda_context *)backend->context;\n",
    "    ggml_cuda_set_device(cuda_ctx->device);\n",
    "// #ifdef USE_CUDA_GRAPH\n",
    "    static const bool disable_cuda_graphs_due_to_env = (getenv(\"GGML_CUDA_DISABLE_GRAPHS\") != nullptr);\n",
    "    // Objects required for CUDA Graph\n",
    "    // 需要warmup的原因找到了\n",
    "    // 第二次可能就不执行了 *key*\n",
    "    if (cuda_ctx->cuda_graph == nullptr) --> cuda_ctx->cuda_graph.reset(new ggml_cuda_graph());\n",
    "    bool use_cuda_graph = true;\n",
    "    bool cuda_graph_update_required = false;\n",
    "    if (cuda_ctx->cuda_graph->graph == nullptr);\n",
    "        // 我们不会用到AMPERE以下的\n",
    "        // if (ggml_cuda_info().devices[cuda_ctx->device].cc < GGML_CUDA_CC_AMPERE) cuda_ctx->cuda_graph->disable_due_to_gpu_arch = true; // GGML_LOG_DEBUG(\"%s: disabling CUDA graphs due to GPU architecture\\n\", __func__);\n",
    "    // Disable CUDA graphs in presence of env var, old GPU, use-case which is changing too rapidly,\n",
    "    // or previous graph capture failure.\n",
    "    // Also disable for multi-gpu for now. TO DO investigate\n",
    "    // 不会被执行到\n",
    "    if (disable_cuda_graphs_due_to_env\n",
    "        || cuda_ctx->cuda_graph->disable_due_to_gpu_arch\n",
    "        || cuda_ctx->cuda_graph->disable_due_to_too_many_updates\n",
    "        || cuda_ctx->cuda_graph->disable_due_to_failed_graph_capture)\n",
    "        use_cuda_graph = false;\n",
    "    \n",
    "    // 默认会被执行到\n",
    "    // if (use_cuda_graph) {\n",
    "    --> is_cuda_graph_update_required(cuda_ctx, cgraph) --> cuda_graph_update_required, cuda_ctx->cuda_graph->ggml_graph_properties;\n",
    "    --> check_node_graph_compatibility_and_refresh_copy_ops(cuda_ctx, cgraph, use_cuda_graph) --> use_cuda_graph, cuda_ctx->cuda_graph|->cpy_dest_ptrs/dest_ptrs_d/dest_ptrs_size/use_cpy_indirection/graph_cpynode_index;\n",
    "    // Disable CUDA graphs (from the next token) if the use-case is demanding too many consecutive graph updates.\n",
    "    if (use_cuda_graph && cuda_graph_update_required) cuda_ctx->cuda_graph->number_consecutive_updates++;\n",
    "    else cuda_ctx->cuda_graph->number_consecutive_updates = 0;\n",
    "    // GGML_LOG_DEBUG(\"%s: disabling CUDA graphs due to too many consecutive updates\\n\", __func__);\n",
    "    if (cuda_ctx->cuda_graph->number_consecutive_updates >= 4) cuda_ctx->cuda_graph->disable_due_to_too_many_updates = true;\n",
    "    // }\n",
    "    // 第二次可能不是执行到\n",
    "    if (use_cuda_graph && cuda_graph_update_required)\n",
    "        // Start CUDA graph capture\n",
    "        std::lock_guard<std::mutex> lock(ggml_cuda_lock);\n",
    "        ggml_cuda_lock_counter.fetch_add(1, std::memory_order_relaxed);\n",
    "        cudaStreamBeginCapture(cuda_ctx->stream(), cudaStreamCaptureModeRelaxed);\n",
    "    // if (!use_cuda_graph) cuda_ctx->cuda_graph->use_cpy_indirection = false; //大概率不会执行到\n",
    "// #endif // USE_CUDA_GRAPH\n",
    "    --> evaluate_and_capture_cuda_graph(cuda_ctx, cgraph, graph_evaluated_or_captured=false, use_cuda_graph, cuda_graph_update_required) --> cuda_ctx->cuda_graph->graph;\n",
    "    return GGML_STATUS_SUCCESS;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c8f50",
   "metadata": {},
   "source": [
    "###### [`is_cuda_graph_update_required`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L2623)\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_graph_node_properties</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_graph_node_properties {\n",
    "    void * node_address;\n",
    "    ggml_op node_op;\n",
    "    int64_t ne[GGML_MAX_DIMS];\n",
    "    size_t nb[GGML_MAX_DIMS];\n",
    "    void * src_address[GGML_MAX_SRC];\n",
    "    int32_t op_params[GGML_MAX_OP_PARAMS / sizeof(int32_t)];\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "static bool is_cuda_graph_update_required(ggml_backend_cuda_context * cuda_ctx, ggml_cgraph * cgraph) {\n",
    "    bool cuda_graph_update_required = false;\n",
    "    if (cuda_ctx->cuda_graph->instance == nullptr) cuda_graph_update_required = true;\n",
    "    // Check if the graph size has changed\n",
    "    if (cuda_ctx->cuda_graph->ggml_graph_properties.size() != (size_t)cgraph->n_nodes)\n",
    "        cuda_graph_update_required = true;\n",
    "        cuda_ctx->cuda_graph->ggml_graph_properties.resize(cgraph->n_nodes);\n",
    "    // Loop over nodes in GGML graph to determine if CUDA graph update is required\n",
    "    // and store properties to allow this comparison for the next token\n",
    "    for (int i = 0; i < cgraph->n_nodes; i++)\n",
    "        bool has_matching_properties = true;\n",
    "        if (!cuda_graph_update_required)\n",
    "            ggml_graph_node_has_matching_properties(cgraph->nodes[i], &cuda_ctx->cuda_graph->ggml_graph_properties[i]) --> has_matching_properties;\n",
    "            --->if (node->data != graph_node_properties->node_address &&\n",
    "                    node->op != GGML_OP_CPY &&\n",
    "                    node->op != GGML_OP_VIEW) return false;\n",
    "                if (node->op != graph_node_properties->node_op) return false;\n",
    "                for (int i = 0; i < GGML_MAX_DIMS; i++)\n",
    "                    if (node->ne[i] != graph_node_properties->ne[i]) return false;\n",
    "                    if (node->nb[i] != graph_node_properties->nb[i]) return false;\n",
    "                for (int i = 0; i < GGML_MAX_SRC; i++)\n",
    "                    if (node->src[i] &&\n",
    "                        node->src[i]->data != graph_node_properties->src_address[i] &&\n",
    "                        node->op != GGML_OP_CPY &&\n",
    "                        node->op != GGML_OP_VIEW\n",
    "                    ) return false;\n",
    "                if (node->op == GGML_OP_SCALE &&\n",
    "                    memcmp(graph_node_properties->op_params, node->op_params, GGML_MAX_OP_PARAMS) != 0) return false;\n",
    "                return true;\n",
    "        if (!has_matching_properties)\n",
    "            cuda_graph_update_required = true;\n",
    "            set_ggml_graph_node_properties(node=cgraph->nodes[i], p=&cuda_ctx->cuda_graph->ggml_graph_properties[i]);\n",
    "            --->p->node_address = node->data; p->node_op = node->op;\n",
    "                for (int i = 0; i < GGML_MAX_DIMS; i++) p->ne[i] = node->ne[i]; p->nb[i] = node->nb[i];\n",
    "                for (int i = 0; i < GGML_MAX_SRC; i++) p->src_address[i] = node->src[i] ? node->src[i]->data : nullptr;\n",
    "                memcpy(p->op_params, node->op_params, GGML_MAX_OP_PARAMS);\n",
    "    return cuda_graph_update_required;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66454ec7",
   "metadata": {},
   "source": [
    "###### [`check_node_graph_compatibility_and_refresh_copy_ops`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L2506)\n",
    "\n",
    "```c++\n",
    "static bool check_node_graph_compatibility_and_refresh_copy_ops(ggml_backend_cuda_context * cuda_ctx, ggml_cgraph * cgraph, bool use_cuda_graph) {\n",
    "    // Loop over nodes in GGML graph to obtain info needed for CUDA graph\n",
    "    cuda_ctx->cuda_graph->cpy_dest_ptrs.clear();\n",
    "    for (int i = 0; i < cgraph->n_nodes; i++)\n",
    "        ggml_tensor * node = cgraph->nodes[i];\n",
    "        if (ggml_is_empty(node) || node->op == GGML_OP_RESHAPE || node->op == GGML_OP_TRANSPOSE || \\\n",
    "            node->op == GGML_OP_VIEW || node->op == GGML_OP_PERMUTE || node->op == GGML_OP_NONE) continue;\n",
    "        if (node->src[0] && node->src[0]->buffer && ggml_backend_buft_is_cuda_split(node->src[0]->buffer->buft))\n",
    "            use_cuda_graph = false; // Split buffers are not supported by CUDA graph capture\n",
    "            // GGML_LOG_DEBUG(\"%s: disabling CUDA graphs due to split buffer\\n\", __func__);\n",
    "        if (node->op == GGML_OP_MUL_MAT_ID && node->ne[2] != 1)\n",
    "            use_cuda_graph = false; // This node type is not supported by CUDA graph capture\n",
    "            // GGML_LOG_DEBUG(\"%s: disabling CUDA graphs due to unsupported node type\\n\", __func__);\n",
    "        if (node->op == GGML_OP_ADD && node->src[1] && node->src[1]->ne[1] > 1)\n",
    "            // disable CUDA graphs for batch size > 1 for now.\n",
    "            // Changes in batch size or context size can cause changes to the grid size of some kernels.\n",
    "            use_cuda_graph = false;\n",
    "            // GGML_LOG_DEBUG(\"%s: disabling CUDA graphs due to batch size > 1 [%s] [%ld %ld %ld %ld]\\n\", __func__, node->name, node->ne[0], node->ne[1], node->ne[2], node->ne[3]);\n",
    "        if (node->op == GGML_OP_CPY) // cache_k, cache_v\n",
    "            // Store the pointers which are updated for each token, such that these can be sent\n",
    "            // to the device and accessed using indirection from CUDA graph\n",
    "            cuda_ctx->cuda_graph->cpy_dest_ptrs.push_back((char *) node->src[1]->data);\n",
    "            // store a pointer to each copy op CUDA kernel to identify it later ??? 看起来也没用到啊\n",
    "            void * ptr = ggml_cuda_cpy_fn(node->src[0], node->src[1]); // return (void*) cpy_f32_f16<cpy_1_f32_f16>;\n",
    "            if (!ptr) use_cuda_graph = false;\n",
    "            // GGML_LOG_DEBUG(\"%s: disabling CUDA graphs due to unsupported copy op\\n\", __func__);\n",
    "        if (!use_cuda_graph) break;\n",
    "    if (use_cuda_graph)\n",
    "        cuda_ctx->cuda_graph->use_cpy_indirection = true;\n",
    "        // copy pointers to GPU so they can be accessed via indirection within CUDA graph\n",
    "        ggml_cuda_cpy_dest_ptrs_copy(\n",
    "            cuda_graph=cuda_ctx->cuda_graph.get(),\n",
    "            host_dest_ptrs=cuda_ctx->cuda_graph->cpy_dest_ptrs.data(),\n",
    "            host_dest_ptrs_size=cuda_ctx->cuda_graph->cpy_dest_ptrs.size(),\n",
    "            stream=cuda_ctx->stream());\n",
    "        --->// 下面这个if第二次可能不会执行\n",
    "            if (cuda_graph->dest_ptrs_size < host_dest_ptrs_size) // (re-)allocate GPU memory for \n",
    "                cudaStreamSynchronize(stream);\n",
    "                if (cuda_graph->dest_ptrs_d != nullptr) cudaFree(cuda_graph->dest_ptrs_d);\n",
    "                cudaMalloc(&cuda_graph->dest_ptrs_d, host_dest_ptrs_size*sizeof(char *));\n",
    "                cuda_graph->dest_ptrs_size = host_dest_ptrs_size;\n",
    "            cudaMemcpyAsync(cuda_graph->dest_ptrs_d, host_dest_ptrs, host_dest_ptrs_size*sizeof(char *), cudaMemcpyHostToDevice, stream);\n",
    "            cuda_graph->graph_cpynode_index = 0; // reset index\n",
    "    return use_cuda_graph;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a37ac",
   "metadata": {},
   "source": [
    "###### [`evaluate_and_capture_cuda_graph`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L2681)\n",
    "\n",
    "```c++\n",
    "static void evaluate_and_capture_cuda_graph(ggml_backend_cuda_context * cuda_ctx, ggml_cgraph * cgraph,\n",
    "    bool & graph_evaluated_or_captured, bool & use_cuda_graph, bool & cuda_graph_update_required) {\n",
    "    // flag used to determine whether it is an integrated_gpu\n",
    "    const bool integrated = ggml_cuda_info().devices[cuda_ctx->device].integrated;\n",
    "\n",
    "    while (!graph_evaluated_or_captured) // while 循环只会被执行一次\n",
    "        // Only perform the graph execution if CUDA graphs are not enabled, or we are capturing the graph.\n",
    "        // With the use of CUDA graphs, the execution will be performed by the graph launch.\n",
    "        // 第二次大概率不会被执行到\n",
    "        if (!use_cuda_graph || cuda_graph_update_required)\n",
    "            for (int i = 0; i < cgraph->n_nodes; i++)\n",
    "                ggml_tensor * node = cgraph->nodes[i];\n",
    "                if (ggml_is_empty(node) || node->op == GGML_OP_RESHAPE || node->op == GGML_OP_TRANSPOSE || node->op == GGML_OP_VIEW || node->op == GGML_OP_PERMUTE || node->op == GGML_OP_NONE) continue;\n",
    "                --> ggml_cuda_compute_forward(*cuda_ctx, node) --> bool ok;\n",
    "        // 第二次大概率不会被执行到\n",
    "        if (use_cuda_graph && cuda_graph_update_required) // End CUDA graph capture\n",
    "            // if (cuda_ctx->cuda_graph->graph != nullptr);// 不会为true\n",
    "            cudaStreamEndCapture(cuda_ctx->stream(), &cuda_ctx->cuda_graph->graph);\n",
    "            graph_evaluated_or_captured = true; // CUDA graph has been captured\n",
    "            std::lock_guard<std::mutex> lock(ggml_cuda_lock);\n",
    "            if (ggml_cuda_lock_counter.fetch_sub(1, std::memory_order_relaxed) == 1) ggml_cuda_lock_cv.notify_all();\n",
    "        else //第二次大概率直接会到这里\n",
    "            graph_evaluated_or_captured = true; // ggml graph has been directly evaluated\n",
    "    if (use_cuda_graph)\n",
    "        // 第二次大概率不会被执行到\n",
    "        if (cuda_ctx->cuda_graph->instance == nullptr) // Create executable graph from captured graph.\n",
    "            cudaGraphInstantiate(&cuda_ctx->cuda_graph->instance, cuda_ctx->cuda_graph->graph, NULL, NULL, 0);\n",
    "        // 第二次大概率不会被执行到\n",
    "        if (cuda_graph_update_required) // Update graph executable\n",
    "            update_cuda_graph_executable(cuda_ctx);\n",
    "            --->//#if CUDART_VERSION >= 12000\n",
    "                cudaGraphExecUpdate(cuda_ctx->cuda_graph->instance, cuda_ctx->cuda_graph->graph, cudaGraphExecUpdateResultInfo &result_info); //cudaSuccess\n",
    "        // Launch graph\n",
    "        cudaGraphLaunch(cuda_ctx->cuda_graph->instance, cuda_ctx->stream());\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ICX2FPjOQNu5",
   "metadata": {},
   "source": [
    "### [`ggml_backend_tensor_get_async`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#245)\n",
    "\n",
    "```c++\n",
    "ggml_backend_tensor_get_async(backend_res, t_logits, logits_out, 0, n_outputs*n_vocab*sizeof(float));\n",
    "----------\n",
    "void ggml_backend_tensor_get_async(ggml_backend_t backend, const struct ggml_tensor * tensor, void * data, size_t offset, size_t size) {\n",
    "    if (backend->iface.get_tensor_async == NULL) ggml_backend_tensor_get(tensor, data, offset, size);\n",
    "    else backend->iface.get_tensor_async(backend, tensor, data, offset, size); // output 层也在GPU上\n",
    "}\n",
    "```\n",
    "\n",
    "[`ggml_backend_cpu_buffer_get_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1896)\n",
    "\n",
    "```c++\n",
    "void ggml_backend_tensor_get(const struct ggml_tensor * tensor, void * data, size_t offset, size_t size) {\n",
    "    ggml_backend_buffer_t buf = tensor->view_src ? tensor->view_src->buffer : tensor->buffer;\n",
    "    --> buf->iface.get_tensor(buf, tensor, data, offset, size);\n",
    "        --> static void ggml_backend_cpu_buffer_get_tensor(ggml_backend_buffer_t buffer, const struct ggml_tensor * tensor, void * data, size_t offset, size_t size)\n",
    "            memcpy(data, (const char *)tensor->data + offset, size);\n",
    "}\n",
    "```\n",
    "\n",
    "[`ggml_backend_cuda_get_tensor_async`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/ggml-cuda.cu#L2507)\n",
    "\n",
    "```c++\n",
    "static void ggml_backend_cuda_get_tensor_async(ggml_backend_t backend, const ggml_tensor * tensor, void * data, size_t offset, size_t size) {\n",
    "    cudaMemcpyAsync(data, (const char *)tensor->data + offset, size, cudaMemcpyDeviceToHost, ((ggml_backend_cuda_context *)backend->context;)->stream());\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956178f8-f679-474a-a2e8-dc00bf58cdac",
   "metadata": {},
   "source": [
    "# [`perplexity`](https://github.com/ggml-org/llama.cpp/blob/master/tools/perplexity/perplexity.cpp#L441)\n",
    "\n",
    "```c++\n",
    "results = perplexity(ctx, params, n_ctx);\n",
    "----------\n",
    "static results_perplexity perplexity(llama_context * ctx, const common_params & params, const int32_t n_ctx) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HlvSDPRTaPLh",
   "metadata": {},
   "source": [
    "```c++\n",
    "// Download: https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip\n",
    "// Run `./llama-perplexity -m models/7B/ggml-model-q4_0.bin -f wiki.test.raw`\n",
    "// Output: `perplexity: 13.5106 [114/114]`\n",
    "// BOS tokens will be added for each chunk before eval\n",
    "\n",
    "const llama_model * model = llama_get_model(ctx);\n",
    "const llama_vocab * vocab = llama_model_get_vocab(model);\n",
    "const bool add_bos = llama_vocab_get_add_bos(vocab);\n",
    "--> common_tokenize --> llama_tokenize --> vocab->tokenize --> llama_vocab::tokenize --> llama_vocab::impl::tokenize --> std::vector<llama_token> tokens;\n",
    "\n",
    "std::vector<float> logit_history;\n",
    "logit_history.resize(tokens.size());\n",
    "\n",
    "std::vector<float> prob_history;\n",
    "prob_history.resize(tokens.size());\n",
    "\n",
    "const int n_chunk_max = tokens.size() / n_ctx;\n",
    "const int n_chunk = params.n_chunks < 0 ? n_chunk_max : std::min(params.n_chunks, n_chunk_max);\n",
    "const int n_batch = params.n_batch;\n",
    "const int n_vocab = llama_vocab_n_tokens(vocab);\n",
    "int count = 0;\n",
    "double nll = 0.0;\n",
    "double nll2 = 0.0;\n",
    "const int num_batches = (n_ctx + n_batch - 1) / n_batch;\n",
    "const int n_seq = std::max(1, n_batch / n_ctx);\n",
    "--> llama_batch_init --> llama_batch batch;\n",
    "std::vector<std::thread> workers(std::thread::hardware_concurrency() - 1);\n",
    "\n",
    "// We get the logits for all the tokens in the context window (params.n_ctx)\n",
    "// from llama_eval above.  Now, based on https://huggingface.co/docs/transformers/perplexity,\n",
    "// calculate the perplexity over the last half of the window (so the model always has\n",
    "// some context to predict the token).\n",
    "//\n",
    "// We rely on the fact that attention in the forward pass only looks at previous\n",
    "// tokens here, so the logits returned for each token are an accurate representation\n",
    "// of what the model would have predicted at that point.\n",
    "//\n",
    "// Example, we have a context window of 512, we will compute perplexity for each of the\n",
    "// last 256 tokens.  Then, we split the input up into context window size chunks to\n",
    "// process the entire prompt.\n",
    "const int first = n_ctx/2;\n",
    "for (int i = 0; i < n_chunk; i += n_seq)\n",
    "    const int start =     i * n_ctx;\n",
    "    const int end   = start + n_ctx;\n",
    "    const int n_seq_batch = std::min(n_seq, n_chunk - i);\n",
    "    // clear the KV cache\n",
    "    llama_memory_clear(llama_get_memory(ctx), true);\n",
    "    // for (int j = 0; j < num_batches; ++j)\n",
    "    const int batch_start = start + j * n_batch;\n",
    "    const int batch_size  = std::min(end - batch_start, n_batch);\n",
    "    int n_outputs = 0;\n",
    "    batch.n_tokens = 0;\n",
    "    for (int seq = 0; seq < n_seq_batch; seq++)\n",
    "        int seq_start = batch_start + seq*n_ctx;\n",
    "        // save original token and restore it after eval\n",
    "        const auto token_org = tokens[seq_start];\n",
    "        if (add_bos && j == 0) tokens[seq_start] = llama_vocab_bos(vocab);\n",
    "        for (int k = 0; k < batch_size; ++k) {\n",
    "            const int idx = seq*n_ctx + k;\n",
    "            batch.token   [idx]    = tokens[seq_start + k];\n",
    "            batch.pos     [idx]    = j*n_batch + k;\n",
    "            batch.n_seq_id[idx]    = 1;\n",
    "            batch.seq_id  [idx][0] = seq;\n",
    "            batch.logits  [idx]    = batch.pos[idx] >= first ? 1 : 0;\n",
    "\n",
    "            n_outputs += batch.logits[idx] != 0;\n",
    "        }\n",
    "        batch.n_tokens += batch_size;\n",
    "        // restore the original token in case it was set to BOS\n",
    "        tokens[seq_start] = token_org;\n",
    "\n",
    "    --> llama_decode(ctx, batch);\n",
    "    if (i == 0) llama_synchronize(ctx);\n",
    "\n",
    "    for (int seq = 0; seq < n_seq_batch; seq++)\n",
    "        const float * all_logits = llama_get_logits_ith(ctx, seq*n_ctx + first);\n",
    "            ctx->synchronize();\n",
    "            return ctx:: logits + output_ids[seq*n_ctx + first]*model.vocab.n_tokens();\n",
    "        llama_token * tokens_data = tokens.data() + start + seq*n_ctx + first;\n",
    "        --> process_logits(n_vocab, all_logits,\n",
    "                tokens_data, n_ctx - 1 - first,\n",
    "                workers, nll, nll2,\n",
    "                logit_history.data() + start + seq*n_ctx + first,\n",
    "                prob_history.data()  + start + seq*n_ctx + first);\n",
    "        count += n_ctx - first - 1;\n",
    "\n",
    "nll2 /= count;\n",
    "nll /= count;\n",
    "const double ppl = exp(nll);\n",
    "nll2 -= nll * nll;\n",
    "nll2 = sqrt(nll2/(count-1));\n",
    "\n",
    "--> llama_batch_free;\n",
    "return {tokens, ppl, logit_history, prob_history};\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N5cLIa6uaeTV",
   "metadata": {},
   "source": [
    "## [`llama_vocab::impl::tokenize`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-vocab.cpp#L2390)\n",
    "\n",
    "```c++\n",
    "std::vector<llama_token> tokens = common_tokenize(ctx, params.prompt, true);\n",
    "----------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qXuZOOwqjNTy",
   "metadata": {},
   "source": [
    "```c++\n",
    "// tokenizes a string into a vector of tokens\n",
    "// should work similar to Python's `tokenizer.encode`\n",
    "std::vector<llama_token> common_tokenize(\n",
    "  const struct llama_context * ctx,\n",
    "           const std::string & text,\n",
    "                        bool   add_special,\n",
    "                        bool   parse_special = false) {\n",
    "    const llama_model * model = llama_get_model(ctx);\n",
    "    const llama_vocab * vocab = llama_model_get_vocab(model);\n",
    "    --> return common_tokenize(vocab, text, add_special, parse_special);\n",
    "}\n",
    "\n",
    "std::vector<llama_token> common_tokenize(\n",
    "    const struct llama_vocab * vocab,\n",
    "           const std::string & text,\n",
    "                        bool   add_special,\n",
    "                        bool   parse_special) {\n",
    "    // upper limit for the number of tokens\n",
    "    int n_tokens = text.length() + 2 * add_special;\n",
    "    std::vector<llama_token> result(n_tokens);\n",
    "    --> n_tokens = llama_tokenize(vocab, text.data(), text.length(), result.data(), result.size(), add_special, parse_special);\n",
    "    result.resize(n_tokens);\n",
    "    return result;\n",
    "}\n",
    "\n",
    "int32_t llama_tokenize(\n",
    "    const struct llama_vocab * vocab,\n",
    "                  const char * text,\n",
    "                     int32_t   text_len,\n",
    "                 llama_token * tokens,\n",
    "                     int32_t   n_tokens_max,\n",
    "                        bool   add_special,\n",
    "                        bool   parse_special) {\n",
    "    --> return vocab->tokenize(text, text_len, tokens, n_tokens_max, add_special, parse_special);\n",
    "}\n",
    "\n",
    "int32_t llama_vocab::tokenize(\n",
    "                  const char * text,\n",
    "                     int32_t   text_len,\n",
    "                 llama_token * tokens,\n",
    "                     int32_t   n_tokens_max,\n",
    "                        bool   add_special,\n",
    "                        bool   parse_special) const {\n",
    "    --> auto res = tokenize(std::string(text, text_len), add_special, parse_special);\n",
    "    for (size_t i = 0; i < res.size(); i++) {\n",
    "        tokens[i] = res[i];\n",
    "    }\n",
    "    return res.size();\n",
    "}\n",
    "\n",
    "std::vector<llama_token> llama_vocab::tokenize(\n",
    "        const std::string & raw_text,\n",
    "        bool add_special,\n",
    "        bool parse_special) const {\n",
    "    --> return pimpl->tokenize(raw_text, add_special, parse_special);\n",
    "}\n",
    "\n",
    "std::vector<llama_token> llama_vocab::impl::tokenize(\n",
    "        const std::string & raw_text,\n",
    "        bool add_special,\n",
    "        bool parse_special) const {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obgPbFpXeXQd",
   "metadata": {},
   "source": [
    "```c++\n",
    "std::vector<llama_token> output;\n",
    "std::forward_list<fragment_buffer_variant> fragment_buffer;\n",
    "\n",
    "fragment_buffer.emplace_front(raw_text, 0, raw_text.length());\n",
    "tokenizer_st_partition(fragment_buffer, parse_special);\n",
    "\n",
    "llm_tokenizer_bpe_session session(vocab, *static_cast<const llm_tokenizer_bpe *>(tokenizer.get()));\n",
    "// it calls some other methods that are not exist in llm_tokenizer,\n",
    "// here just cast it to bpe tokenizer object\n",
    "if (add_special) {\n",
    "    session.append_bos(output);\n",
    "}\n",
    "for (const auto & fragment : fragment_buffer) {\n",
    "    std::string text = fragment.raw_text.substr(fragment.offset, fragment.length);\n",
    "    --> llm_tokenizer_bpe_session::tokenize;\n",
    "}\n",
    "\n",
    "if (add_special) {\n",
    "    session.append_eos(output);\n",
    "    session.check_double_bos_eos(output);\n",
    "}\n",
    "\n",
    "return output;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BMJIwOtyiNV0",
   "metadata": {},
   "source": [
    "[`llm_tokenizer_bpe_session::tokenize`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-vocab.cpp#L482)\n",
    "\n",
    "<details>\n",
    "<summary>struct llm_tokenizer_bpe_session</summary>\n",
    "\n",
    "```c++\n",
    "struct llm_tokenizer_bpe_session {\n",
    "    llm_tokenizer_bpe_session(const llama_vocab & vocab, const llm_tokenizer_bpe & tokenizer) : vocab(vocab), tokenizer(tokenizer) {}\n",
    "\n",
    "    static void append(const llama_token token_id, std::vector<llama_token> & output)  {\n",
    "    bool append_bos(std::vector<llama_token> & output) const\n",
    "    bool append_eos(std::vector<llama_token> & output) const\n",
    "    void check_double_bos_eos(const std::vector<llama_token> & output) const\n",
    "    void tokenize(const std::string & text, std::vector<llama_token> & output)\n",
    "\n",
    "private:\n",
    "    void add_new_bigram(int left, int right)\n",
    "    const llama_vocab & vocab;\n",
    "    const llm_tokenizer_bpe & tokenizer;\n",
    "\n",
    "    std::vector<llm_symbol> symbols;\n",
    "    std::vector<llm_symbol> symbols_final;\n",
    "    llm_bigram_bpe::queue work_queue;\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "session.tokenize(text, output);\n",
    "----------\n",
    "void llm_tokenizer_bpe_session::tokenize(const std::string & text, std::vector<llama_token> & output) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_-FyHhnun2uG",
   "metadata": {},
   "source": [
    "## [`llama_batch_init`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L590)\n",
    "\n",
    "```c++\n",
    "llama_batch batch = llama_batch_init(std::min(n_batch, n_ctx*n_seq), 0, 1);\n",
    "----------\n",
    "struct llama_batch llama_batch_init(int32_t n_tokens_alloc, int32_t embd, int32_t n_seq_max) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9_wfg-xcoEHj",
   "metadata": {},
   "source": [
    "```c++\n",
    "llama_batch batch = {\n",
    "    /*n_tokens       =*/ 0,\n",
    "    /*tokens         =*/ nullptr,\n",
    "    /*embd           =*/ nullptr,\n",
    "    /*pos            =*/ nullptr,\n",
    "    /*n_seq_id       =*/ nullptr,\n",
    "    /*seq_id         =*/ nullptr,\n",
    "    /*logits         =*/ nullptr,\n",
    "};\n",
    "\n",
    "if (embd) {\n",
    "    batch.embd = (float *) malloc(sizeof(float) * n_tokens_alloc * embd);\n",
    "} else {\n",
    "    batch.token = (llama_token *) malloc(sizeof(llama_token) * n_tokens_alloc);\n",
    "}\n",
    "\n",
    "batch.pos      = (llama_pos *)     malloc(sizeof(llama_pos)      * n_tokens_alloc);\n",
    "batch.n_seq_id = (int32_t *)       malloc(sizeof(int32_t)        * n_tokens_alloc);\n",
    "batch.seq_id   = (llama_seq_id **) malloc(sizeof(llama_seq_id *) * (n_tokens_alloc + 1));\n",
    "for (int i = 0; i < n_tokens_alloc; ++i) {\n",
    "    batch.seq_id[i] = (llama_seq_id *) malloc(sizeof(llama_seq_id) * n_seq_max);\n",
    "}\n",
    "batch.seq_id[n_tokens_alloc] = nullptr;\n",
    "\n",
    "batch.logits   = (int8_t *)        malloc(sizeof(int8_t)         * n_tokens_alloc);\n",
    "\n",
    "return batch;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KlIl9uGu0oc3",
   "metadata": {},
   "source": [
    "## [`process_logits`](https://github.com/ggml-org/llama.cpp/blob/master/tools/perplexity/perplexity.cpp#L107)\n",
    "\n",
    "```c++\n",
    "process_logits(n_vocab, all_logits,\n",
    "        tokens_data, n_ctx - 1 - first,\n",
    "        workers, nll, nll2,\n",
    "        logit_history.data() + start + seq*n_ctx + first,\n",
    "        prob_history.data()  + start + seq*n_ctx + first);\n",
    "----------\n",
    "static void process_logits(\n",
    "    int n_vocab, const float * logits, const int * tokens, int n_token, std::vector<std::thread> & workers,\n",
    "    double & nll, double & nll2, float * logit_history, float * prob_history) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sQIGDu411eES",
   "metadata": {},
   "source": [
    "```c++\n",
    "std::mutex mutex;\n",
    "int counter = 0;\n",
    "auto compute = [&mutex, &counter, &nll, &nll2, logit_history, prob_history, n_vocab, logits, tokens, n_token] () {\n",
    "    double local_nll  = 0;\n",
    "    double local_nll2 = 0;\n",
    "    while (true) {\n",
    "        std::unique_lock<std::mutex> lock(mutex);\n",
    "        int i = counter++;\n",
    "        if (i >= n_token) {\n",
    "            nll += local_nll; nll2 += local_nll2;\n",
    "            break;\n",
    "        }\n",
    "        lock.unlock();\n",
    "        --> log_softmax --> const results_log_softmax results;\n",
    "        const double v = -results.log_softmax;\n",
    "        local_nll += v;\n",
    "        local_nll2 += v*v;\n",
    "\n",
    "        logit_history[i] = results.logit;\n",
    "        prob_history[i]  = results.prob;\n",
    "    }\n",
    "};\n",
    "for (auto & w : workers) {\n",
    "    w = std::thread(compute);\n",
    "}\n",
    "compute();\n",
    "for (auto & w : workers) {\n",
    "    w.join();\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QkDpQiXv13Nq",
   "metadata": {},
   "source": [
    "[`log_softmax`](https://github.com/ggml-org/llama.cpp/blob/master/tools/perplexity/perplexity.cpp#L58)\n",
    "\n",
    "<details>\n",
    "<summary>struct results_log_softmax</summary>\n",
    "\n",
    "```c++\n",
    "struct results_log_softmax {\n",
    "    double log_softmax;\n",
    "    float  logit;\n",
    "    float  prob;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "const results_log_softmax results = log_softmax(n_vocab, logits + size_t(i)*n_vocab, tokens[i+1]);\n",
    "----------\n",
    "static results_log_softmax log_softmax(int n_vocab, const float * logits, int tok) {\n",
    "    float max_logit = logits[0];\n",
    "    for (int i = 1; i < n_vocab; ++i) {\n",
    "        max_logit = std::max(max_logit, logits[i]);\n",
    "    }\n",
    "    double sum_exp = 0.0;\n",
    "    for (int i = 0; i < n_vocab; ++i) {\n",
    "        sum_exp += expf(logits[i] - max_logit);\n",
    "    }\n",
    "    return {logits[tok] - max_logit - log(sum_exp), logits[tok], expf(logits[tok] - max_logit) / (float) sum_exp};\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nxv91gZNzyqS",
   "metadata": {},
   "source": [
    "## [`llama_batch_free`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L620)\n",
    "\n",
    "```c++\n",
    "llama_batch_free(batch);\n",
    "----------\n",
    "void llama_batch_free(struct llama_batch batch) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "It9cQT67z5SF",
   "metadata": {},
   "source": [
    "```c++\n",
    "if (batch.token)    free(batch.token);\n",
    "if (batch.embd)     free(batch.embd);\n",
    "if (batch.pos)      free(batch.pos);\n",
    "if (batch.n_seq_id) free(batch.n_seq_id);\n",
    "if (batch.seq_id) {\n",
    "    for (int i = 0; batch.seq_id[i] != nullptr; ++i) {\n",
    "        free(batch.seq_id[i]);\n",
    "    }\n",
    "    free(batch.seq_id);\n",
    "}\n",
    "if (batch.logits)   free(batch.logits);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a1c09-8369-40ef-943f-36d6e8a90f15",
   "metadata": {},
   "source": [
    "# [`ggml_quantize_free`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L6451)\n",
    "\n",
    "```c++\n",
    "llama_backend_free();\n",
    "----------\n",
    "void llama_backend_free(void) {\n",
    "    --> ggml_quantize_free();\n",
    "}\n",
    "\n",
    "void ggml_quantize_free(void) {\n",
    "    ggml_critical_section_start();\n",
    "\n",
    "    iq2xs_free_impl(GGML_TYPE_IQ2_XXS);\n",
    "    iq2xs_free_impl(GGML_TYPE_IQ2_XS);\n",
    "    iq2xs_free_impl(GGML_TYPE_IQ1_S);\n",
    "    iq3xs_free_impl(256);\n",
    "\n",
    "    ggml_critical_section_end();\n",
    "}\n",
    "\n",
    "void iq2xs_free_impl(enum ggml_type type)\n",
    "    const int gindex = iq2_data_index(type);\n",
    "    if (iq2_data[gindex].grid) {\n",
    "        free(iq2_data[gindex].grid);       iq2_data[gindex].grid = NULL;\n",
    "        free(iq2_data[gindex].map);        iq2_data[gindex].map  = NULL;\n",
    "        free(iq2_data[gindex].neighbours); iq2_data[gindex].neighbours = NULL;\n",
    "    }\n",
    "\n",
    "void iq3xs_free_impl(int grid_size)\n",
    "    const int gindex = iq3_data_index(grid_size);\n",
    "    if (iq3_data[gindex].grid) {\n",
    "        free(iq3_data[gindex].grid);       iq3_data[gindex].grid = NULL;\n",
    "        free(iq3_data[gindex].map);        iq3_data[gindex].map  = NULL;\n",
    "        free(iq3_data[gindex].neighbours); iq3_data[gindex].neighbours = NULL;\n",
    "    }\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "23b35917-d3da-45b2-996e-cf9b9e26cfb9",
    "9ad88911-3e43-461f-bcb1-8599dccb1fe6",
    "7d4cdb26-bd29-4915-96b6-9573765ce679"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
